TITLE: Extracting Structured Data with LLM using Crawl4AI in Python
DESCRIPTION: This snippet demonstrates how to use the `Crawl4AI` library to extract structured data from a webpage (OpenAI pricing page) using an LLM. It defines a Pydantic schema (`OpenAIModelFee`) for the desired output structure, configures the `LLMExtractionStrategy` with the schema, instructions, and LLM provider details (handling API tokens for providers like OpenAI or using local models like Ollama), and then runs the `AsyncWebCrawler` to fetch and process the page. The extracted data, formatted according to the schema, is printed.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_5

LANGUAGE: python
CODE:
```
import os
import json
import asyncio
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig, BrowserConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from typing import Dict

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(
        ..., description="Fee for output token for the OpenAI model."
    )

async def extract_structured_data_using_llm(
    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None
):
    print(f"\n--- Extracting Structured Data with {provider} ---")

    if api_token is None and provider != "ollama" and not provider.startswith("ollama/") and provider != "no_token": # Adjusted check for ollama variations
        print(f"API token is required for {provider} unless it's 'ollama' or 'no_token'. Skipping this example.")
        return

    browser_config = BrowserConfig(headless=True)

    extra_args = {"temperature": 0, "top_p": 0.9, "max_tokens": 2000}
    if extra_headers:
        extra_args["extra_headers"] = extra_headers

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        word_count_threshold=1,
        page_timeout=80000,
        extraction_strategy=LLMExtractionStrategy(
            llm_config = LLMConfig(provider=provider,api_token=api_token),
            schema=OpenAIModelFee.model_json_schema(),
            extraction_type="schema",
            instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content.""",
            extra_args=extra_args,
        ),
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://openai.com/api/pricing/", config=crawler_config
        )
        print(result.extracted_content)

if __name__ == "__main__":

    asyncio.run(
        extract_structured_data_using_llm(
            provider="openai/gpt-4o", api_token=os.getenv("OPENAI_API_KEY")
        )
    )
    # Example for Ollama (assuming Ollama server is running)
    # asyncio.run(
    #     extract_structured_data_using_llm(
    #         provider="ollama/llama3" 
    #     )
    # )
```

----------------------------------------

TITLE: Basic Crawl4AI Usage Example
DESCRIPTION: Minimal Python script demonstrating a basic web crawl using AsyncWebCrawler to verify installation.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_81

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.example.com",
        )
        print(result.markdown[:300])  # Show the first 300 characters of extracted text

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Running Crawl4AI setup command
DESCRIPTION: Executes the setup command after installation. This installs or updates required Playwright browsers, performs OS-level checks, and confirms the environment is ready for crawling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_1

LANGUAGE: bash
CODE:
```
crawl4ai-setup
```

----------------------------------------

TITLE: Initializing and Running AsyncWebCrawler with Custom Configurations in Python
DESCRIPTION: Complete example of configuring and running an asynchronous web crawler with custom browser settings and run parameters. It demonstrates setting up browser configuration, run configuration with cache mode and selectors, and handling the crawl results including screenshots.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    # Configure the browser
    browser_cfg = BrowserConfig(
        headless=False,
        viewport_width=1280,
        viewport_height=720,
        proxy="http://user:pass@myproxy:8080",
        text_mode=True
    )

    # Configure the run
    run_cfg = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        session_id="my_session",
        css_selector="main.article",
        excluded_tags=["script", "style"],
        exclude_external_links=True,
        wait_for="css:.article-loaded",
        screenshot=True,
        stream=True
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://example.com/news",
            config=run_cfg
        )
        if result.success:
            print("Final cleaned_html length:", len(result.cleaned_html))
            if result.screenshot:
                print("Screenshot captured (base64, length):", len(result.screenshot))
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Simulating Full-Page Scrolling for Dynamic Content
DESCRIPTION: This snippet demonstrates how to enable full-page scanning that simulates scrolling to capture dynamically loaded content. It scrolls incrementally with configurable delay, making it effective for infinite scroll pages.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
await crawler.crawl(
    url="https://example.com",
    scan_full_page=True,   # Enables scrolling
    scroll_delay=0.2       # Waits 200ms between scrolls (optional)
)
```

----------------------------------------

TITLE: Managing Profiles - Crawl4AI - Bash
DESCRIPTION: Executes the `crwl profiles` command to manage browser profiles for Crawl4AI. This is a one-time setup step to log into LinkedIn and create a persistent session, avoiding repeated logins and CAPTCHAs.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/apps/linkdin/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
crwl profiles
```

----------------------------------------

TITLE: Accessing CrawlResult Properties in Crawl4AI
DESCRIPTION: Shows how to access and utilize different properties of the CrawlResult object returned by the crawler, including content formats, status information, and extracted media and links.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
result = await crawler.arun(
    url="https://example.com",
    config=CrawlerRunConfig(fit_markdown=True)
)

# Different content formats
print(result.html)         # Raw HTML
print(result.cleaned_html) # Cleaned HTML
print(result.markdown.raw_markdown) # Raw markdown from cleaned html
print(result.markdown.fit_markdown) # Most relevant content in markdown

# Check success status
print(result.success)      # True if crawl succeeded
print(result.status_code)  # HTTP status code (e.g., 200, 404)

# Access extracted media and links
print(result.media)        # Dictionary of found media (images, videos, audio)
print(result.links)        # Dictionary of internal and external links
```

----------------------------------------

TITLE: Basic Installation of Crawl4AI using pip (Bash)
DESCRIPTION: This command installs the basic asynchronous version of Crawl4AI using pip and runs the setup command to configure the necessary browser components (Playwright by default).
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_4

LANGUAGE: bash
CODE:
```
pip install crawl4ai
crawl4ai-setup # Setup the browser
```

----------------------------------------

TITLE: Multi-Step GitHub Commits Crawling with Crawl4AI
DESCRIPTION: Implements a complete multi-step interaction to crawl GitHub commits across multiple pages. Uses session management, JavaScript execution, and custom wait conditions to navigate through paginated content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def multi_page_commits():
    browser_cfg = BrowserConfig(
        headless=False,  # Visible for demonstration
        verbose=True
    )
    session_id = "github_ts_commits"
    
    base_wait = """js:() => {
        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
        return commits.length > 0;
    }"""

    # Step 1: Load initial commits
    config1 = CrawlerRunConfig(
        wait_for=base_wait,
        session_id=session_id,
        cache_mode=CacheMode.BYPASS,
        # Not using js_only yet since it's our first load
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://github.com/microsoft/TypeScript/commits/main",
            config=config1
        )
        print("Initial commits loaded. Count:", result.cleaned_html.count("commit"))

        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists
        js_next_page = """
        const selector = 'a[data-testid="pagination-next-button"]';
        const button = document.querySelector(selector);
        if (button) button.click();
        """
        
        # Wait until new commits appear
        wait_for_more = """js:() => {
            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
            if (!window.firstCommit && commits.length>0) {
                window.firstCommit = commits[0].textContent;
                return false;
            }
            // If top commit changes, we have new commits
            const topNow = commits[0]?.textContent.trim();
            return topNow && topNow !== window.firstCommit;
        }"""

        for page in range(2):  # let's do 2 more "Next" pages
            config_next = CrawlerRunConfig(
                session_id=session_id,
                js_code=js_next_page,
                wait_for=wait_for_more,
                js_only=True,       # We're continuing from the open tab
                cache_mode=CacheMode.BYPASS
            )
            result2 = await crawler.arun(
                url="https://github.com/microsoft/TypeScript/commits/main",
                config=config_next
            )
            print(f"Page {page+2} commits count:", result2.cleaned_html.count("commit"))

        # Optionally kill session
        await crawler.crawler_strategy.kill_session(session_id)

async def main():
    await multi_page_commits()

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Extracting HTML Tables to Pandas DataFrame with Crawl4AI in Python
DESCRIPTION: This Python script demonstrates extracting HTML tables from a webpage using `AsyncWebCrawler` and converting the first detected table into a pandas DataFrame. It initializes the crawler, sets a `table_score_threshold` in `CrawlerRunConfig` for stricter table detection, crawls a URL (CoinMarketCap), iterates through the results, checks for successfully extracted tables (`result.media["tables"]`), and populates a pandas DataFrame with the headers and rows of the first table found.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_21

LANGUAGE: python
CODE:
```
crawler = AsyncWebCrawler(config=browser_config)
await crawler.start()

try:
    # Set up scraping parameters
    crawl_config = CrawlerRunConfig(
        table_score_threshold=8,  # Strict table detection
    )

    # Execute market data extraction
    results: List[CrawlResult] = await crawler.arun(
        url="https://coinmarketcap.com/?page=1", config=crawl_config
    )

    # Process results
    raw_df = pd.DataFrame()
    for result in results:
        if result.success and result.media["tables"]:
            raw_df = pd.DataFrame(
                result.media["tables"][0]["rows"],
                columns=result.media["tables"][0]["headers"],
            )
            break
    print(raw_df.head())

finally:
    await crawler.stop()
```

----------------------------------------

TITLE: Setting Custom Headers with AsyncWebCrawler
DESCRIPTION: This example shows two different methods for setting custom headers when using AsyncWebCrawler. The first approach updates headers at the crawler strategy level, while the second passes headers directly to the arun() method.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_133

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    # Option 1: Set headers at the crawler strategy level
    crawler1 = AsyncWebCrawler(
        # The underlying strategy can accept headers in its constructor
        crawler_strategy=None  # We'll override below for clarity
    )
    crawler1.crawler_strategy.update_user_agent("MyCustomUA/1.0")
    crawler1.crawler_strategy.set_custom_headers({
        "Accept-Language": "fr-FR,fr;q=0.9"
    })
    result1 = await crawler1.arun("https://www.example.com")
    print("Example 1 result success:", result1.success)

    # Option 2: Pass headers directly to `arun()`
    crawler2 = AsyncWebCrawler()
    result2 = await crawler2.arun(
        url="https://www.example.com",
        headers={"Accept-Language": "es-ES,es;q=0.9"}
    )
    print("Example 2 result success:", result2.success)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Authenticated Proxy Configuration in Python
DESCRIPTION: Shows how to configure an authenticated proxy with username and password using BrowserConfig.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_159

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import BrowserConfig

proxy_config = {
    "server": "http://proxy.example.com:8080",
    "username": "user",
    "password": "pass"
}

browser_config = BrowserConfig(proxy_config=proxy_config)
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")
```

----------------------------------------

TITLE: Building and Running Locally with Docker Compose
DESCRIPTION: This command uses Docker Compose to build the Crawl4AI image locally from the Dockerfile and then run it in detached mode. The `--build` flag triggers the local build process.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_8

LANGUAGE: Bash
CODE:
```
# Builds the image locally using Dockerfile and runs it
# Automatically uses the correct architecture for your machine
docker compose up --build -d
```

----------------------------------------

TITLE: Pulling Crawl4AI Docker Images
DESCRIPTION: These commands pull the official Crawl4AI Docker images from Docker Hub. The first command pulls a specific release candidate version, while the second pulls the latest stable version. Docker automatically selects the correct architecture.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_0

LANGUAGE: Bash
CODE:
```
# Pull the release candidate (recommended for latest features)
docker pull unclecode/crawl4ai:0.6.0-r1

# Or pull the latest stable version
docker pull unclecode/crawl4ai:latest
```

----------------------------------------

TITLE: Using LLMExtractionStrategy with crawl4ai (Python)
DESCRIPTION: Provides an example of using LLMExtractionStrategy to extract structured data from a web page using an LLM. It requires defining a Pydantic schema for the desired data structure, configuring an LLM provider, and providing extraction instructions. The extracted data is returned as a JSON string.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_9

LANGUAGE: python
CODE:
```
from pydantic import BaseModel
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai import LLMConfig

# Define schema
class Article(BaseModel):
    title: str
    content: str
    author: str

# Create strategy
strategy = LLMExtractionStrategy(
    llm_config = LLMConfig(provider="ollama/llama2"),
    schema=Article.schema(),
    instruction="Extract article details"
)

# Use with crawler
result = await crawler.arun(
    url="https://example.com/article",
    extraction_strategy=strategy
)

# Access extracted data
data = json.loads(result.extracted_content)
```

----------------------------------------

TITLE: Multi-Step Page Interaction Example for GitHub Commits in Python
DESCRIPTION: This comprehensive example demonstrates a multi-step interaction that loads multiple pages of GitHub commits by clicking 'Next Page' buttons. It maintains session state between requests and uses JavaScript to interact with the page elements.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_119

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def multi_page_commits():
    browser_cfg = BrowserConfig(
        headless=False,  # Visible for demonstration
        verbose=True
    )
    session_id = "github_ts_commits"
    
    base_wait = """js:() => {
        const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
        return commits.length > 0;
    }"""

    # Step 1: Load initial commits
    config1 = CrawlerRunConfig(
        wait_for=base_wait,
        session_id=session_id,
        cache_mode=CacheMode.BYPASS,
        # Not using js_only yet since it's our first load
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://github.com/microsoft/TypeScript/commits/main",
            config=config1
        )
        print("Initial commits loaded. Count:", result.cleaned_html.count("commit"))

        # Step 2: For subsequent pages, we run JS to click 'Next Page' if it exists
        js_next_page = """
        const selector = 'a[data-testid="pagination-next-button"]';
        const button = document.querySelector(selector);
        if (button) button.click();
        """
        
        # Wait until new commits appear
        wait_for_more = """js:() => {
            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
            if (!window.firstCommit && commits.length>0) {
                window.firstCommit = commits[0].textContent;
                return false;
            }
            // If top commit changes, we have new commits
            const topNow = commits[0]?.textContent.trim();
            return topNow && topNow !== window.firstCommit;
        }"""

        for page in range(2):  # let's do 2 more "Next" pages
            config_next = CrawlerRunConfig(
                session_id=session_id,
                js_code=js_next_page,
                wait_for=wait_for_more,
                js_only=True,       # We're continuing from the open tab
                cache_mode=CacheMode.BYPASS
            )
            result2 = await crawler.arun(
                url="https://github.com/microsoft/TypeScript/commits/main",
                config=config_next
            )
            print(f"Page {page+2} commits count:", result2.cleaned_html.count("commit"))

        # Optionally kill session
        await crawler.crawler_strategy.kill_session(session_id)

async def main():
    await multi_page_commits()

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Basic Usage Example of Crawler4ai
DESCRIPTION: Simple example demonstrating how to import and use Crawler4ai to crawl a website, with basic configuration options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from crawler4ai import Crawler4ai

crawler = Crawler4ai(
    urls=["https://example.com"],
    max_pages=100,
    output_dir="crawl_results"
)

results = crawler.crawl()
```

----------------------------------------

TITLE: Installing Crawl4AI using pip (Bash)
DESCRIPTION: This command uses pip, the Python package installer, to download and install the Crawl4AI library and its required dependencies. This installation is a prerequisite for running any of the provided examples.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/examples.md#_snippet_0

LANGUAGE: bash
CODE:
```
pip install crawl4ai
```

----------------------------------------

TITLE: Basic Setup and Simple Crawl with Crawl4AI
DESCRIPTION: This snippet demonstrates a basic crawl operation using Crawl4AI. It crawls a specific URL and prints the first 500 characters of the raw markdown content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def simple_crawl():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            bypass_cache=True # By default this is False, meaning the cache will be used
        )
        print(result.markdown.raw_markdown[:500])  # Print the first 500 characters
        
asyncio.run(simple_crawl())
```

----------------------------------------

TITLE: Initializing CrawlerRunConfig with Keyword Arguments in Python
DESCRIPTION: This snippet shows the initialization of CrawlerRunConfig using keyword arguments. It sets various parameters for web crawling, including content extraction, image handling, link processing, and debugging options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_14

LANGUAGE: Python
CODE:
```
exclude_all_images=kwargs.get("exclude_all_images", False),
exclude_external_images=kwargs.get("exclude_external_images", False),
# Link and Domain Handling Parameters
exclude_social_media_domains=kwargs.get(
    "exclude_social_media_domains", SOCIAL_MEDIA_DOMAINS
),
exclude_external_links=kwargs.get("exclude_external_links", False),
exclude_social_media_links=kwargs.get("exclude_social_media_links", False),
exclude_domains=kwargs.get("exclude_domains", []),
exclude_internal_links=kwargs.get("exclude_internal_links", False),
# Debugging and Logging Parameters
verbose=kwargs.get("verbose", True),
log_console=kwargs.get("log_console", False),
# Network and Console Capturing Parameters
capture_network_requests=kwargs.get("capture_network_requests", False),
capture_console_messages=kwargs.get("capture_console_messages", False),
# Connection Parameters
method=kwargs.get("method", "GET"),
stream=kwargs.get("stream", False),
check_robots_txt=kwargs.get("check_robots_txt", False),
user_agent=kwargs.get("user_agent"),
user_agent_mode=kwargs.get("user_agent_mode"),
user_agent_generator_config=kwargs.get("user_agent_generator_config", {}),
# Deep Crawl Parameters
deep_crawl_strategy=kwargs.get("deep_crawl_strategy"),
url=kwargs.get("url"),
# Experimental Parameters 
experimental=kwargs.get("experimental"),
```

----------------------------------------

TITLE: Installing Crawl4AI using pip
DESCRIPTION: Installs the core Crawl4AI library with essential dependencies using pip. This basic installation doesn't include advanced features like transformers or PyTorch.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install crawl4ai
```

----------------------------------------

TITLE: Installing Crawl4AI and Dependencies
DESCRIPTION: This snippet shows how to install Crawl4AI and its dependencies using pip. It also includes the installation of nest_asyncio and Playwright.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
# %%capture
!pip install crawl4ai
!pip install nest_asyncio
!playwright install
```

----------------------------------------

TITLE: Running a Simple Asynchronous Web Crawl with Crawl4AI in Python
DESCRIPTION: This Python snippet demonstrates how to perform an asynchronous web crawl using the AsyncWebCrawler class from Crawl4AI. It manages an async browser session context, fetches the specified URL, and prints the Markdown-formatted result. Dependencies include Crawl4AI and asyncio. The main input is the target URL; output is sent to the standard output. Running requires installing Crawl4AI and Playwright with all browser dependencies. The example is suitable for new users seeking a minimal programmable workflow for web extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_2

LANGUAGE: python
CODE:
```
import asyncio\nfrom crawl4ai import *\n\nasync def main():\n    async with AsyncWebCrawler() as crawler:\n        result = await crawler.arun(\n            url=\"https://www.nbcnews.com/business\",\n        )\n        print(result.markdown)\n\nif __name__ == \"__main__\":\n    asyncio.run(main())
```

----------------------------------------

TITLE: Basic Web Crawling with AsyncWebCrawler in Python
DESCRIPTION: A minimal example demonstrating how to use Crawl4AI's AsyncWebCrawler to fetch a webpage and convert it to Markdown. This basic script fetches example.com and prints the first 300 characters of the resulting Markdown content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_121

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com")
        print(result.markdown[:300])  # Print first 300 chars

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Parallel Crawling of Multiple URLs in Python using Crawl4AI
DESCRIPTION: This snippet demonstrates how to crawl multiple URLs concurrently using Crawl4AI's AsyncWebCrawler. It shows both streaming and batch modes for processing results, and includes error handling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_127

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def quick_parallel_example():
    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3"
    ]
    
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=True  # Enable streaming mode
    )

    async with AsyncWebCrawler() as crawler:
        # Stream results as they complete
        async for result in await crawler.arun_many(urls, config=run_conf):
            if result.success:
                print(f"[OK] {result.url}, length: {len(result.markdown.raw_markdown)}")
            else:
                print(f"[ERROR] {result.url} => {result.error_message}")

        # Or get all results at once (default behavior)
        run_conf = run_conf.clone(stream=False)
        results = await crawler.arun_many(urls, config=run_conf)
        for res in results:
            if res.success:
                print(f"[OK] {res.url}, length: {len(res.markdown.raw_markdown)}")
            else:
                print(f"[ERROR] {res.url} => {res.error_message}")

if __name__ == "__main__":
    asyncio.run(quick_parallel_example())
```

----------------------------------------

TITLE: Performing Asynchronous Web Crawling with Crawl4AI in Python
DESCRIPTION: This snippet demonstrates how to perform an asynchronous web crawl using the AsyncWebCrawler class from the crawl4ai Python package. It initializes an AsyncWebCrawler context, runs the crawler asynchronously on a given URL, and prints the Markdown-formatted extraction result. To run this code, install the crawl4ai package (e.g., via pip), ensure Python 3.7+ with asyncio support, and have network access to the target website. It accepts a target URL and outputs the extracted content; limitations may arise if the URL is unreachable or dependencies are unmet.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/index.md#_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    # Create an instance of AsyncWebCrawler
    async with AsyncWebCrawler() as crawler:
        # Run the crawler on a URL
        result = await crawler.arun(url="https://crawl4ai.com")

        # Print the extracted content
        print(result.markdown)

# Run the async main function
asyncio.run(main())
```

----------------------------------------

TITLE: Crawling a Web URL with AsyncWebCrawler in Python
DESCRIPTION: This code demonstrates how to crawl a live website (Wikipedia) using Crawl4AI's AsyncWebCrawler. It creates a configuration object, initializes the crawler, and asynchronously retrieves and processes the web content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import CrawlerRunConfig

async def crawl_web():
    config = CrawlerRunConfig(bypass_cache=True)
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://en.wikipedia.org/wiki/apple", 
            config=config
        )
        if result.success:
            print("Markdown Content:")
            print(result.markdown)
        else:
            print(f"Failed to crawl: {result.error_message}")

asyncio.run(crawl_web())
```

----------------------------------------

TITLE: Complete Example of LLM-based Product Extraction in Python
DESCRIPTION: A full working example showing how to define a Pydantic model, create an LLM extraction strategy, configure the crawler, and process the results. This includes error handling and usage tracking.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
import os
import asyncio
import json
from pydantic import BaseModel, Field
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class Product(BaseModel):
    name: str
    price: str

async def main():
    # 1. Define the LLM extraction strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv('OPENAI_API_KEY')),
        schema=Product.schema_json(), # Or use model_json_schema()
        extraction_type="schema",
        instruction="Extract all product objects with 'name' and 'price' from the content.",
        chunk_token_threshold=1000,
        overlap_rate=0.0,
        apply_chunking=True,
        input_format="markdown",   # or "html", "fit_markdown"
        extra_args={"temperature": 0.0, "max_tokens": 800}
    )

    # 2. Build the crawler config
    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        cache_mode=CacheMode.BYPASS
    )

    # 3. Create a browser config if needed
    browser_cfg = BrowserConfig(headless=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # 4. Let's say we want to crawl a single page
        result = await crawler.arun(
            url="https://example.com/products",
            config=crawl_config
        )

        if result.success:
            # 5. The extracted content is presumably JSON
            data = json.loads(result.extracted_content)
            print("Extracted items:", data)
            
            # 6. Show usage stats
            llm_strategy.show_usage()  # prints token usage
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Implementing Batch Processing in Python with AsyncWebCrawler
DESCRIPTION: This snippet shows how to use AsyncWebCrawler for batch processing of URLs, utilizing MemoryAdaptiveDispatcher and CrawlerMonitor for efficient crawling and monitoring.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_146

LANGUAGE: python
CODE:
```
async def crawl_batch():
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=False  # Default: get all results at once
    )
    
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=10,
        monitor=CrawlerMonitor(
            display_mode=DisplayMode.DETAILED
        )
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # Get all results at once
        results = await crawler.arun_many(
            urls=urls,
            config=run_config,
            dispatcher=dispatcher
        )
        
        # Process all results after completion
        for result in results:
            if result.success:
                await process_result(result)
            else:
                print(f"Failed to crawl {result.url}: {result.error_message}")
```

----------------------------------------

TITLE: Schema-Based Crypto Price Extraction in Python
DESCRIPTION: Shows how to extract cryptocurrency prices using JsonCssExtractionStrategy without LLMs. Defines a schema with CSS selectors to extract structured data from web pages efficiently.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_185

LANGUAGE: python
CODE:
```
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_crypto_prices():
    # 1. Define a simple extraction schema
    schema = {
        "name": "Crypto Prices",
        "baseSelector": "div.crypto-row",    # Repeated elements
        "fields": [
            {
                "name": "coin_name",
                "selector": "h2.coin-name",
                "type": "text"
            },
            {
                "name": "price",
                "selector": "span.coin-price",
                "type": "text"
            }
        ]
    }

    # 2. Create the extraction strategy
    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    # 3. Set up your crawler config (if needed)
    config = CrawlerRunConfig(
        # e.g., pass js_code or wait_for if the page is dynamic
        # wait_for="css:.crypto-row:nth-child(20)"
        cache_mode = CacheMode.BYPASS,
        extraction_strategy=extraction_strategy,
    )

    async with AsyncWebCrawler(verbose=True) as crawler:
        # 4. Run the crawl and extraction
        result = await crawler.arun(
            url="https://example.com/crypto-prices",
            
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return

        # 5. Parse the extracted JSON
        data = json.loads(result.extracted_content)
        print(f"Extracted {len(data)} coin entries")
        print(json.dumps(data[0], indent=2) if data else "No data found")

asyncio.run(extract_crypto_prices())
```

----------------------------------------

TITLE: Running Crawler and Accessing CrawlResult Fields in Python
DESCRIPTION: This snippet demonstrates how to run the crawler asynchronously and access various fields of the CrawlResult object, including status code, response headers, links, markdown content, and extracted structured content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
result = await crawler.arun(url="https://example.com", config=some_config)

if result.success:
    print(result.status_code, result.response_headers)
    print("Links found:", len(result.links.get("internal", [])))
    if result.markdown:
        print("Markdown snippet:", result.markdown.raw_markdown[:200])
    if result.extracted_content:
        print("Structured JSON:", result.extracted_content)
else:
    print("Error:", result.error_message)
```

----------------------------------------

TITLE: Extracting Structured Data with JsonCssExtractionStrategy (Python)
DESCRIPTION: Demonstrates setting up and running an asynchronous web crawl using AsyncWebCrawler with JsonCssExtractionStrategy. It highlights defining an extraction schema, configuring the crawler, executing the crawl, and handling the JSON output.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_3

LANGUAGE: python
CODE:
```
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

ecommerce_schema = {
    # ... the advanced schema from above ...
}

async def extract_ecommerce_data():
    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)
    
    config = CrawlerRunConfig()
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html",
            extraction_strategy=strategy,
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return
        
        # Parse the JSON output
        data = json.loads(result.extracted_content)
        print(json.dumps(data, indent=2) if data else "No data found.")

asyncio.run(extract_ecommerce_data())
```

----------------------------------------

TITLE: Complete Example of Crawl4AI with LLM Extraction
DESCRIPTION: Full implementation example showing how to set up a product extractor using LLM-based extraction with Crawl4AI, including Pydantic schema definition, crawler configuration, and result processing.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_183

LANGUAGE: python
CODE:
```
import os
import asyncio
import json
from pydantic import BaseModel, Field
from typing import List
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class Product(BaseModel):
    name: str
    price: str

async def main():
    # 1. Define the LLM extraction strategy
    llm_strategy = LLMExtractionStrategy(
        llm_config = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv('OPENAI_API_KEY')),
        schema=Product.schema_json(), # Or use model_json_schema()
        extraction_type="schema",
        instruction="Extract all product objects with 'name' and 'price' from the content.",
        chunk_token_threshold=1000,
        overlap_rate=0.0,
        apply_chunking=True,
        input_format="markdown",   # or "html", "fit_markdown"
        extra_args={"temperature": 0.0, "max_tokens": 800}
    )

    # 2. Build the crawler config
    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strategy,
        cache_mode=CacheMode.BYPASS
    )

    # 3. Create a browser config if needed
    browser_cfg = BrowserConfig(headless=True)

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        # 4. Let's say we want to crawl a single page
        result = await crawler.arun(
            url="https://example.com/products",
            config=crawl_config
        )

        if result.success:
            # 5. The extracted content is presumably JSON
            data = json.loads(result.extracted_content)
            print("Extracted items:", data)
            
            # 6. Show usage stats
            llm_strategy.show_usage()  # prints token usage
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Installing Crawl4AI Package
DESCRIPTION: Command to install or upgrade the Crawl4AI package using pip package manager.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_8

LANGUAGE: bash
CODE:
```
pip install -U crawl4ai
```

----------------------------------------

TITLE: Extracting Structured Data using CSS Selectors with Crawl4AI in Python
DESCRIPTION: This snippet demonstrates how to extract structured data (JSON) from HTML using a predefined CSS schema with `JsonCssExtractionStrategy`. A schema dictionary defines the base selector for items and specifies fields to extract (like 'title' and 'link') using CSS selectors. Raw HTML is passed directly to the crawler using the `raw://` prefix. The `JsonCssExtractionStrategy`, initialized with the schema, is provided in the `CrawlerRunConfig`. The extracted data is available as a JSON string in `result.extracted_content`, which is then parsed and printed.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    schema = {
        "name": "Example Items",
        "baseSelector": "div.item",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}
        ]
    }

    raw_html = "<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>"

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="raw://" + raw_html,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema)
            )
        )
        # The JSON output is stored in 'extracted_content'
        data = json.loads(result.extracted_content)
        print(data)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Crawling Dynamic Content with Session Management in Python
DESCRIPTION: Demonstrates how to crawl GitHub commits across multiple pages while preserving session state. It uses JsonCssExtractionStrategy for data extraction, JavaScript execution for pagination, and wait conditions to ensure content is loaded properly.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai.cache_context import CacheMode

async def crawl_dynamic_content():
    async with AsyncWebCrawler() as crawler:
        session_id = "github_commits_session"
        url = "https://github.com/microsoft/TypeScript/commits/main"
        all_commits = []

        # Define extraction schema
        schema = {
            "name": "Commit Extractor",
            "baseSelector": "li.Box-sc-g0xbh4-0",
            "fields": [{
                "name": "title", "selector": "h4.markdown-title", "type": "text"
            }],
        }
        extraction_strategy = JsonCssExtractionStrategy(schema)

        # JavaScript and wait configurations
        js_next_page = """document.querySelector('a[data-testid="pagination-next-button"]').click();"""
        wait_for = """() => document.querySelectorAll('li.Box-sc-g0xbh4-0').length > 0"""

        # Crawl multiple pages
        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                extraction_strategy=extraction_strategy,
                js_code=js_next_page if page > 0 else None,
                wait_for=wait_for if page > 0 else None,
                js_only=page > 0,
                cache_mode=CacheMode.BYPASS
            )

            result = await crawler.arun(config=config)
            if result.success:
                commits = json.loads(result.extracted_content)
                all_commits.extend(commits)
                print(f"Page {page + 1}: Found {len(commits)} commits")

        # Clean up session
        await crawler.crawler_strategy.kill_session(session_id)
        return all_commits
```

----------------------------------------

TITLE: Using css_selector for Content Selection in Crawl4AI
DESCRIPTION: This snippet demonstrates how to use the css_selector parameter in CrawlerRunConfig to limit crawl results to a specific region of a webpage. The example targets the first 30 items from Hacker News.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    config = CrawlerRunConfig(
        # e.g., first 30 items from Hacker News
        css_selector=".athing:nth-child(-n+30)"  
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com/newest", 
            config=config
        )
        print("Partial HTML length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Extracting Structured Data with LLMs using Crawl4AI in Python
DESCRIPTION: This Python script demonstrates using `AsyncWebCrawler` along with an LLM for structured data extraction. It defines a Pydantic schema (`OpenAIModelFee`) for the desired output structure, configures `LLMExtractionStrategy` specifying the LLM provider (e.g., OpenAI), API token, the schema, and instructions for extraction. It then crawls a URL (OpenAI pricing page), extracts data matching the schema using the LLM, and prints the extracted content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_18

LANGUAGE: python
CODE:
```
import os
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(..., description="Fee for output token for the OpenAI model.")

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        word_count_threshold=1,
        extraction_strategy=LLMExtractionStrategy(
            # Here you can use any provider that Litellm library supports, for instance: ollama/qwen2
            # provider="ollama/qwen2", api_token="no-token", 
            llm_config = LLMConfig(provider="openai/gpt-4o", api_token=os.getenv('OPENAI_API_KEY')), 
            schema=OpenAIModelFee.schema(),
            extraction_type="schema",
            instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content. One extracted model JSON format should look like this: 
            {\"model_name\": \"GPT-4\", \"input_fee\": \"US$10.00 / 1M tokens\", \"output_fee\": \"US$30.00 / 1M tokens\"}."""
        ),            
        cache_mode=CacheMode.BYPASS,
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url='https://openai.com/api/pricing/',
            config=run_config
        )
        print(result.extracted_content)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Setting Up a Basic Web Crawler with Crawl4AI
DESCRIPTION: Demonstrates how to initialize and use AsyncWebCrawler with default configurations to crawl a website and retrieve markdown content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig

async def main():
    browser_config = BrowserConfig()  # Default browser configuration
    run_config = CrawlerRunConfig()   # Default crawl run configuration

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        print(result.markdown)  # Print clean markdown content

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Comprehensive Web Crawling Example with Crawl4AI
DESCRIPTION: A complete example showcasing common usage patterns including configuration, content filtering, processing, cache control, and handling the results with content, images, and links.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        # Content filtering
        word_count_threshold=10,
        excluded_tags=['form', 'header'],
        exclude_external_links=True,
        
        # Content processing
        process_iframes=True,
        remove_overlay_elements=True,
        
        # Cache control
        cache_mode=CacheMode.ENABLED  # Use cache if available
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        
        if result.success:
            # Print clean content
            print("Content:", result.markdown[:500])  # First 500 chars
            
            # Process images
            for image in result.media["images"]:
                print(f"Found image: {image['src']}")
            
            # Process links
            for link in result.links["internal"]:
                print(f"Internal link: {link['href']}")
                
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Performing a Basic Web Crawl with Crawl4AI in Python
DESCRIPTION: This snippet demonstrates the minimal setup required to perform a web crawl using Crawl4AI. It imports the necessary `AsyncWebCrawler` class and `asyncio` library. An asynchronous `main` function initializes the crawler within an `async with` block, executes a crawl on 'https://example.com' using `crawler.arun()`, and prints the first 300 characters of the resulting Markdown generated automatically from the page's HTML.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com")
        print(result.markdown[:300])  # Print first 300 chars

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring and Running AsyncWebCrawler with Content Processing
DESCRIPTION: This example demonstrates how to configure and use AsyncWebCrawler with various settings including content filtering, iframe processing, overlay removal, and cache control. It also shows how to process the crawled content like text, images, and links.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_129

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    browser_config = BrowserConfig(verbose=True)
    run_config = CrawlerRunConfig(
        # Content filtering
        word_count_threshold=10,
        excluded_tags=['form', 'header'],
        exclude_external_links=True,
        
        # Content processing
        process_iframes=True,
        remove_overlay_elements=True,
        
        # Cache control
        cache_mode=CacheMode.ENABLED  # Use cache if available
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        
        if result.success:
            # Print clean content
            print("Content:", result.markdown[:500])  # First 500 chars
            
            # Process images
            for image in result.media["images"]:
                print(f"Found image: {image['src']}")
            
            # Process links
            for link in result.links["internal"]:
                print(f"Internal link: {link['href']}")
                
        else:
            print(f"Crawl failed: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Advanced Session Crawling with Custom Execution Hooks in Python
DESCRIPTION: Uses custom execution hooks to handle complex dynamic content scenarios. This example demonstrates tracking the first commit on each page and ensuring new content loads before proceeding with the next action, with proper error handling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
async def advanced_session_crawl_with_hooks():
    first_commit = ""

    async def on_execution_started(page):
        nonlocal first_commit
        try:
            while True:
                await page.wait_for_selector("li.commit-item h4")
                commit = await page.query_selector("li.commit-item h4")
                commit = await commit.evaluate("(element) => element.textContent").strip()
                if commit and commit != first_commit:
                    first_commit = commit
                    break
                await asyncio.sleep(0.5)
        except Exception as e:
            print(f"Warning: New content didn't appear: {e}")

    async with AsyncWebCrawler() as crawler:
        session_id = "commit_session"
        url = "https://github.com/example/repo/commits/main"
        crawler.crawler_strategy.set_hook("on_execution_started", on_execution_started)

        js_next_page = """document.querySelector('a.pagination-next').click();"""

        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                js_code=js_next_page if page > 0 else None,
                css_selector="li.commit-item",
                js_only=page > 0,
                cache_mode=CacheMode.BYPASS
            )

            result = await crawler.arun(config=config)
            print(f"Page {page + 1}: Found {len(result.extracted_content)} commits")

        await crawler.crawler_strategy.kill_session(session_id)

asyncio.run(advanced_session_crawl_with_hooks())
```

----------------------------------------

TITLE: Checking Crawl HTTP Status Code - crawl4ai - Python
DESCRIPTION: This snippet demonstrates how to access and check the `status_code` attribute of a `CrawlResult` object. This field contains the HTTP status code returned by the server for the crawled page, such as 200 for success or 404 for not found.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_3

LANGUAGE: python
CODE:
```
if result.status_code == 404:
    print("Page not found!")
```

----------------------------------------

TITLE: Basic Extraction Complete Example for Crawl4AI
DESCRIPTION: Comprehensive example showing basic extraction with browser and crawler configurations.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_15

LANGUAGE: bash
CODE:
```
crwl https://example.com \
    -B browser.yml \
    -C crawler.yml \
    -o json
```

----------------------------------------

TITLE: Performing a simple crawl with Crawl4AI in Python
DESCRIPTION: A minimal Python script demonstrating a basic web crawl using Crawl4AI. It utilizes BrowserConfig and CrawlerRunConfig to crawl example.com and print the first 300 characters of extracted text in markdown format.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.example.com",
        )
        print(result.markdown[:300])  # Show the first 300 characters of extracted text

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Generating Heuristic Markdown with Crawl4AI in Python
DESCRIPTION: This Python script demonstrates how to use the `AsyncWebCrawler` from the `crawl4ai` library to crawl a website and generate cleaned Markdown content. It configures the browser to run headlessly, enables caching, and uses a `DefaultMarkdownGenerator` with a `PruningContentFilter` to refine the output based on a fixed threshold. The script then prints the length of the raw and fitted Markdown.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_16

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.content_filter_strategy import PruningContentFilter, BM25ContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    browser_config = BrowserConfig(
        headless=True,  
        verbose=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(threshold=0.48, threshold_type="fixed", min_word_threshold=0)
        ),
        # markdown_generator=DefaultMarkdownGenerator(
        #     content_filter=BM25ContentFilter(user_query="WHEN_WE_FOCUS_BASED_ON_A_USER_QUERY", bm25_threshold=1.0)
        # ),
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://docs.micronaut.io/4.7.6/guide/",
            config=run_config
        )
        print(len(result.markdown.raw_markdown))
        print(len(result.markdown.fit_markdown))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Interacting with Crawl4ai API using Python SDK
DESCRIPTION: Asynchronous Python script demonstrating basic usage of the `crawl4ai` Python SDK (`crawl4ai-docker-client`). It shows how to connect to the server, perform both non-streaming and streaming crawl operations with configuration objects (`BrowserConfig`, `CrawlerRunConfig`), and retrieve the API schema.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_27

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai.docker_client import Crawl4aiDockerClient
from crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode # Assuming you have crawl4ai installed

async def main():
    # Point to the correct server port
    async with Crawl4aiDockerClient(base_url="http://localhost:11235", verbose=True) as client:
        # If JWT is enabled on the server, authenticate first:
        # await client.authenticate("user@example.com") # See Server Configuration section

        # Example Non-streaming crawl
        print("--- Running Non-Streaming Crawl ---")
        results = await client.crawl(
            ["https://httpbin.org/html"],
            browser_config=BrowserConfig(headless=True), # Use library classes for config aid
            crawler_config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        )
        if results: # client.crawl returns None on failure
          print(f"Non-streaming results success: {results.success}")
          if results.success:
              for result in results: # Iterate through the CrawlResultContainer
                  print(f"URL: {result.url}, Success: {result.success}")
        else:
            print("Non-streaming crawl failed.")


        # Example Streaming crawl
        print("\n--- Running Streaming Crawl ---")
        stream_config = CrawlerRunConfig(stream=True, cache_mode=CacheMode.BYPASS)
        try:
            async for result in await client.crawl( # client.crawl returns an async generator for streaming
                ["https://httpbin.org/html", "https://httpbin.org/links/5/0"],
                browser_config=BrowserConfig(headless=True),
                crawler_config=stream_config
            ):
                print(f"Streamed result: URL: {result.url}, Success: {result.success}")
        except Exception as e:
            print(f"Streaming crawl failed: {e}")


        # Example Get schema
        print("\n--- Getting Schema ---")
        schema = await client.get_schema()
        print(f"Schema received: {bool(schema)}") # Print whether schema was received

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: CSS-based JSON Extraction from Raw HTML in Crawl4AI
DESCRIPTION: Demonstrates structured data extraction using CSS selectors with Crawl4AI. This example processes raw HTML directly (without network requests) and extracts items according to a defined schema.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    schema = {
        "name": "Example Items",
        "baseSelector": "div.item",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}
        ]
    }
    raw_html = "<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>"

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="raw://" + raw_html,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema)
            )
        )
        data = json.loads(result.extracted_content)
        print(data)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring and Running AsyncWebCrawler with Extraction and LLM Filtering in Python
DESCRIPTION: This snippet demonstrates the complete process of setting up and running an AsyncWebCrawler. It includes configuring the browser, defining an extraction strategy, setting up LLM-based content filtering, and executing the crawl. The code showcases the use of BrowserConfig, CrawlerRunConfig, and LLMConfig to customize the crawling behavior.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode, LLMConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # 1) Browser config: headless, bigger viewport, no proxy
    browser_conf = BrowserConfig(
        headless=True,
        viewport_width=1280,
        viewport_height=720
    )

    # 2) Example extraction strategy
    schema = {
        "name": "Articles",
        "baseSelector": "div.article",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}
        ]
    }
    extraction = JsonCssExtractionStrategy(schema)

    # 3) Example LLM content filtering

    gemini_config = LLMConfig(
        provider="gemini/gemini-1.5-pro" 
        api_token = "env:GEMINI_API_TOKEN"
    )

    # Initialize LLM filter with specific instruction
    filter = LLMContentFilter(
        llm_config=gemini_config,  # or your preferred provider
        instruction="""
        Focus on extracting the core educational content.
        Include:
        - Key concepts and explanations
        - Important code examples
        - Essential technical details
        Exclude:
        - Navigation elements
        - Sidebars
        - Footer content
        Format the output as clean markdown with proper code blocks and headers.
        """,
        chunk_token_threshold=500,  # Adjust based on your needs
        verbose=True
    )

    md_generator = DefaultMarkdownGenerator(
    content_filter=filter,
    options={"ignore_links": True}

    # 4) Crawler run config: skip cache, use extraction
    run_conf = CrawlerRunConfig(
        markdown_generator=md_generator,
        extraction_strategy=extraction,
        cache_mode=CacheMode.BYPASS,
    )

    async with AsyncWebCrawler(config=browser_conf) as crawler:
        # 4) Execute the crawl
        result = await crawler.arun(url="https://example.com/news", config=run_conf)

        if result.success:
            print("Extracted content:", result.extracted_content)
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Generating CSS Extraction Schemas using LLMs with Crawl4AI in Python
DESCRIPTION: This snippet illustrates how to automatically generate a JSON schema for CSS-based extraction using an LLM via `JsonCssExtractionStrategy.generate_schema`. It shows examples using both OpenAI (requiring an API token via `LLMConfig`) and Ollama (which doesn't require a token). The function takes sample HTML and an `LLMConfig` specifying the provider (e.g., 'openai/gpt-4o' or 'ollama/llama3.3') to generate a reusable schema. This schema can then be used with `JsonCssExtractionStrategy` for efficient, LLM-free data extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_3

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai import LLMConfig

# Generate a schema (one-time cost)
html = "<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>"

# Using OpenAI (requires API token)
schema = JsonCssExtractionStrategy.generate_schema(
    html,
    llm_config = LLMConfig(provider="openai/gpt-4o",api_token="your-openai-token")  # Required for OpenAI
)

# Or using Ollama (open source, no token needed)
schema = JsonCssExtractionStrategy.generate_schema(
    html,
    llm_config = LLMConfig(provider="ollama/llama3.3", api_token=None)  # Not needed for Ollama
)

# Use the schema for fast, repeated extractions
strategy = JsonCssExtractionStrategy(schema)
```

----------------------------------------

TITLE: Configuring Crawl4AI Browser and Run Settings in Python
DESCRIPTION: This snippet shows how to configure the Crawl4AI crawler using `BrowserConfig` and `CrawlerRunConfig`. `BrowserConfig` is used to set browser options like running in headless mode (`headless=True`). `CrawlerRunConfig` controls run-specific settings, such as bypassing the cache (`cache_mode=CacheMode.BYPASS`) to ensure fresh content is fetched. The configurations are passed to the `AsyncWebCrawler` constructor and the `arun` method respectively.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_1

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    browser_conf = BrowserConfig(headless=True)  # or False to see the browser
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler(config=browser_conf) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_conf
        )
        print(result.markdown)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Combining Page Interaction with Extraction Strategy in Crawl4AI
DESCRIPTION: This code demonstrates how to set up a JsonCssExtractionStrategy with CrawlerRunConfig to extract content after dynamic page interaction. It defines a schema for extracting commit information from a list and configures the crawler with JavaScript code for pagination, wait conditions, and the extraction strategy.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

schema = {
    "name": "Commits",
    "baseSelector": "li.Box-sc-g0xbh4-0",
    "fields": [
        {"name": "title", "selector": "h4.markdown-title", "type": "text"}
    ]
}
config = CrawlerRunConfig(
    session_id="ts_commits_session",
    js_code=js_next_page,
    wait_for=wait_for_more,
    extraction_strategy=JsonCssExtractionStrategy(schema)
)
```

----------------------------------------

TITLE: Executing JavaScript and Extracting Structured Data without LLMs using Crawl4AI in Python
DESCRIPTION: This Python script showcases using `AsyncWebCrawler` to execute custom JavaScript on a target page and extract structured data based on a predefined JSON schema using CSS selectors. It defines a schema for course details, configures `JsonCssExtractionStrategy`, injects JavaScript to interact with page tabs, runs the browser non-headlessly, bypasses the cache, crawls the URL, parses the extracted JSON data, and prints the results.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_17

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    schema = {
    "name": "KidoCode Courses",
    "baseSelector": "section.charge-methodology .w-tab-content > div",
    "fields": [
        {
            "name": "section_title",
            "selector": "h3.heading-50",
            "type": "text",
        },
        {
            "name": "section_description",
            "selector": ".charge-content",
            "type": "text",
        },
        {
            "name": "course_name",
            "selector": ".text-block-93",
            "type": "text",
        },
        {
            "name": "course_description",
            "selector": ".course-content-text",
            "type": "text",
        },
        {
            "name": "course_icon",
            "selector": ".image-92",
            "type": "attribute",
            "attribute": "src"
        }
    ]
}

    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    browser_config = BrowserConfig(
        headless=False,
        verbose=True
    )
    run_config = CrawlerRunConfig(
        extraction_strategy=extraction_strategy,
        js_code=["""(async () => {const tabs = document.querySelectorAll("section.charge-methodology .tabs-menu-3 > div");for(let tab of tabs) {tab.scrollIntoView();tab.click();await new Promise(r => setTimeout(r, 500));}})();"""],
        cache_mode=CacheMode.BYPASS
    )
        
    async with AsyncWebCrawler(config=browser_config) as crawler:
        
        result = await crawler.arun(
            url="https://www.kidocode.com/degrees/technology",
            config=run_config
        )

        companies = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(companies)} companies")
        print(json.dumps(companies[0], indent=2))


if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring Crawl4AI Server with YAML
DESCRIPTION: Provides a detailed YAML configuration for the Crawl4AI server, including application settings, rate limiting, security options, crawler parameters, and logging configuration.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_64

LANGUAGE: yaml
CODE:
```
# Application Configuration
app:
  title: "Crawl4AI API"           # Server title in OpenAPI docs
  version: "1.0.0"               # API version
  host: "0.0.0.0"               # Listen on all interfaces
  port: 8000                    # Server port
  reload: True                  # Enable hot reloading (development only)
  timeout_keep_alive: 300       # Keep-alive timeout in seconds

# Rate Limiting Configuration
rate_limiting:
  enabled: True                 # Enable/disable rate limiting
  default_limit: "100/minute"   # Rate limit format: "number/timeunit"
  trusted_proxies: []          # List of trusted proxy IPs
  storage_uri: "memory://"     # Use "redis://localhost:6379" for production

# Security Configuration
security:
  enabled: false               # Master toggle for security features
  jwt_enabled: true            # Enable JWT authentication
  https_redirect: True         # Force HTTPS
  trusted_hosts: ["*"]         # Allowed hosts (use specific domains in production)
  headers:                     # Security headers
    x_content_type_options: "nosniff"
    x_frame_options: "DENY"
    content_security_policy: "default-src 'self'"
    strict_transport_security: "max-age=63072000; includeSubDomains"

# Crawler Configuration
crawler:
  memory_threshold_percent: 95.0  # Memory usage threshold
  rate_limiter:
    base_delay: [1.0, 2.0]      # Min and max delay between requests
  timeouts:
    stream_init: 30.0           # Stream initialization timeout
    batch_process: 300.0        # Batch processing timeout

# Logging Configuration
logging:
  level: "INFO"                 # Log level (DEBUG, INFO, WARNING, ERROR)
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"
```

----------------------------------------

TITLE: Crawling Dynamic Multi-Page Content with Crawl4AI
DESCRIPTION: This function demonstrates advanced multi-page crawling of dynamic content with JavaScript execution. It implements a custom execution hook to detect new content, navigates through multiple pages of GitHub commits, and aggregates the results across pages while maintaining session state.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_139

LANGUAGE: python
CODE:
```
async def crawl_dynamic_content_pages_method_1():
    print("\n--- Advanced Multi-Page Crawling with JavaScript Execution ---")
    first_commit = ""

    async def on_execution_started(page, **kwargs):
        nonlocal first_commit
        try:
            while True:
                await page.wait_for_selector("li.Box-sc-g0xbh4-0 h4")
                commit = await page.query_selector("li.Box-sc-g0xbh4-0 h4")
                commit = await commit.evaluate("(element) => element.textContent")
                commit = re.sub(r"\s+", "", commit)
                if commit and commit != first_commit:
                    first_commit = commit
                    break
                await asyncio.sleep(0.5)
        except Exception as e:
            print(f"Warning: New content didn't appear after JavaScript execution: {e}")

    browser_config = BrowserConfig(headless=False, java_script_enabled=True)

    async with AsyncWebCrawler(config=browser_config) as crawler:
        crawler.crawler_strategy.set_hook("on_execution_started", on_execution_started)

        url = "https://github.com/microsoft/TypeScript/commits/main"
        session_id = "typescript_commits_session"
        all_commits = []

        js_next_page = """
        const button = document.querySelector('a[data-testid="pagination-next-button"]');
        if (button) button.click();
        """

        for page in range(3):
            crawler_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                css_selector="li.Box-sc-g0xbh4-0",
                js_code=js_next_page if page > 0 else None,
                js_only=page > 0,
                session_id=session_id,
            )

            result = await crawler.arun(url=url, config=crawler_config)
            assert result.success, f"Failed to crawl page {page + 1}"

            soup = BeautifulSoup(result.cleaned_html, "html.parser")
            commits = soup.select("li")
            all_commits.extend(commits)

            print(f"Page {page + 1}: Found {len(commits)} commits")

        print(f"Successfully crawled {len(all_commits)} commits across 3 pages")
```

----------------------------------------

TITLE: Performing Basic Deep Crawl with BFS Strategy in Python
DESCRIPTION: Demonstrates a minimal asynchronous deep crawl using `AsyncWebCrawler` and `BFSDeepCrawlStrategy`. It configures a 2-level deep crawl limited to the starting domain ('https://example.com'), uses `LXMLWebScrapingStrategy` for content extraction, enables verbose output, and prints the total count and details (URL, depth) of the first few crawled pages. Requires `asyncio`, `AsyncWebCrawler`, `CrawlerRunConfig`, `BFSDeepCrawlStrategy`, and `LXMLWebScrapingStrategy`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy

async def main():
    # Configure a 2-level deep crawl
    config = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(
            max_depth=2, 
            include_external=False
        ),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=True
    )
    
    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun("https://example.com", config=config)
        
        print(f"Crawled {len(results)} pages in total")
        
        # Access individual results
        for result in results[:3]:  # Show first 3 results
            print(f"URL: {result.url}")
            print(f"Depth: {result.metadata.get('depth', 0)}")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: LLM-Based Extraction in Crawl4AI
DESCRIPTION: This example demonstrates how to use Crawl4AI's LLM-based extraction strategy to extract structured data from crawled content. It uses a Pydantic model to define the extraction schema and leverages an LLM (like GPT-4) to parse content into headline and summary fields.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
import asyncio
import json
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class ArticleData(BaseModel):
    headline: str
    summary: str

async def main():
    llm_strategy = LLMExtractionStrategy(
        llm_config = LLMConfig(provider="openai/gpt-4",api_token="sk-YOUR_API_KEY")
        schema=ArticleData.schema(),
        extraction_type="schema",
        instruction="Extract 'headline' and a short 'summary' from the content."
    )

    config = CrawlerRunConfig(
        exclude_external_links=True,
        word_count_threshold=20,
        extraction_strategy=llm_strategy
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        article = json.loads(result.extracted_content)
        print(article)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring LLMExtractionStrategy with Schema-Based Extraction
DESCRIPTION: Demonstrates how to set up an LLM extraction strategy with OpenAI GPT-4, defining schema parameters, chunking settings, and additional configuration options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_182

LANGUAGE: python
CODE:
```
extraction_strategy = LLMExtractionStrategy(
    llm_config = LLMConfig(provider="openai/gpt-4", api_token="YOUR_OPENAI_KEY"),
    schema=MyModel.model_json_schema(),
    extraction_type="schema",
    instruction="Extract a list of items from the text with 'name' and 'price' fields.",
    chunk_token_threshold=1200,
    overlap_rate=0.1,
    apply_chunking=True,
    input_format="html",
    extra_args={"temperature": 0.1, "max_tokens": 1000},
    verbose=True
)
```

----------------------------------------

TITLE: Configuring LLMExtractionStrategy with Full Parameters in Python
DESCRIPTION: Creates a complete LLM extraction strategy configuration with settings for the model, schema, chunking parameters, and input format. This example shows all the main parameters that can be customized.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
extraction_strategy = LLMExtractionStrategy(
    llm_config = LLMConfig(provider="openai/gpt-4", api_token="YOUR_OPENAI_KEY"),
    schema=MyModel.model_json_schema(),
    extraction_type="schema",
    instruction="Extract a list of items from the text with 'name' and 'price' fields.",
    chunk_token_threshold=1200,
    overlap_rate=0.1,
    apply_chunking=True,
    input_format="html",
    extra_args={"temperature": 0.1, "max_tokens": 1000},
    verbose=True
)
```

----------------------------------------

TITLE: Complete AsyncWebCrawler Usage Example in Python
DESCRIPTION: A comprehensive example demonstrating the usage of AsyncWebCrawler with BrowserConfig, CrawlerRunConfig, and JsonCssExtractionStrategy for web scraping.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
import json

async def main():
    # 1. Browser config
    browser_cfg = BrowserConfig(
        browser_type="firefox",
        headless=False,
        verbose=True
    )

    # 2. Run config
    schema = {
        "name": "Articles",
        "baseSelector": "article.post",
        "fields": [
            {
                "name": "title", 
                "selector": "h2", 
                "type": "text"
            },
            {
                "name": "url", 
                "selector": "a", 
                "type": "attribute", 
                "attribute": "href"
            }
        ]
    }

    run_cfg = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema),
        word_count_threshold=15,
        remove_overlay_elements=True,
        wait_for="css:.post"  # Wait for posts to appear
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://example.com/blog",
            config=run_cfg
        )

        if result.success:
            print("Cleaned HTML length:", len(result.cleaned_html))
            if result.extracted_content:
                articles = json.loads(result.extracted_content)
                print("Extracted articles:", articles[:2])
        else:
            print("Error:", result.error_message)

asyncio.run(main())
```

----------------------------------------

TITLE: Executing Web Crawler in Python
DESCRIPTION: Runs the AsyncWebCrawler with the provided configurations. Handles verbose output and error handling. Returns the crawling result.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_38

LANGUAGE: python
CODE:
```
async def run_crawler(url: str, browser_cfg: BrowserConfig, crawler_cfg: CrawlerRunConfig, verbose: bool):
    if verbose:
        click.echo("Starting crawler with configurations:")
        click.echo(f"Browser config: {browser_cfg.dump()}")
        click.echo(f"Crawler config: {crawler_cfg.dump()}")

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        try:
            result = await crawler.arun(url=url, config=crawler_cfg)
            return result
        except Exception as e:
            raise click.ClickException(f"Crawling failed: {str(e)}")
```

----------------------------------------

TITLE: Defining CrawlResult Class Structure - crawl4ai - Python
DESCRIPTION: This snippet shows the definition of the `CrawlResult` class, a Pydantic `BaseModel` used in `crawl4ai` to encapsulate the results of a web crawl operation. It lists the various fields available, including URL, HTML content, success status, links, media, and optional metadata like screenshots or extracted content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_0

LANGUAGE: python
CODE:
```
class CrawlResult(BaseModel):
    url: str
    html: str
    success: bool
    cleaned_html: Optional[str] = None
    fit_html: Optional[str] = None  # Preprocessed HTML optimized for extraction
    media: Dict[str, List[Dict]] = {}
    links: Dict[str, List[Dict]] = {}
    downloaded_files: Optional[List[str]] = None
    screenshot: Optional[str] = None
    pdf : Optional[bytes] = None
    mhtml: Optional[str] = None
    markdown: Optional[Union[str, MarkdownGenerationResult]] = None
    extracted_content: Optional[str] = None
    metadata: Optional[dict] = None
    error_message: Optional[str] = None
    session_id: Optional[str] = None
    response_headers: Optional[dict] = None
    status_code: Optional[int] = None
    ssl_certificate: Optional[SSLCertificate] = None
    dispatch_result: Optional[DispatchResult] = None
    ...
```

----------------------------------------

TITLE: Configuring Geolocation and Locale for Crawl4AI in Python
DESCRIPTION: This Python code snippet shows how to configure the `CrawlerRunConfig` in `crawl4ai` to simulate specific user location and locale settings. It sets the `locale` (for Accept-Language header and UI), `timezone_id` (for JavaScript Date/Intl objects), and `geolocation` (overriding GPS coordinates with latitude, longitude, and accuracy) to fetch locale-specific content from a target URL.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_20

LANGUAGE: python
CODE:
```
crun_cfg = CrawlerRunConfig(
    url="https://browserleaks.com/geo",          # test page that shows your location
    locale="en-US",                              # Accept-Language & UI locale
    timezone_id="America/Los_Angeles",           # JS Date()/Intl timezone
    geolocation=GeolocationConfig(                 # override GPS coords
        latitude=34.0522,
        longitude=-118.2437,
        accuracy=10.0,
    )
)
```

----------------------------------------

TITLE: Building an Advanced Asynchronous Crawler with Multiple Filters (Python)
DESCRIPTION: Defines an end-to-end async crawling workflow using Crawl4AI, showcasing filter chains for domains, URL patterns, and content types, as well as a keyword relevance scorer to prioritize pages. Utilizes BestFirstCrawlingStrategy and LXMLWebScrapingStrategy for efficient crawling and scraping, processes crawling results with score and depth analysis, and prints statistics by crawl depth. Requires Crawl4AI and asyncio, with all imports explicitly managed.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_12

LANGUAGE: python
CODE:
```
import asyncio\nfrom crawl4ai import AsyncWebCrawler, CrawlerRunConfig\nfrom crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy\nfrom crawl4ai.deep_crawling import BestFirstCrawlingStrategy\nfrom crawl4ai.deep_crawling.filters import (\n    FilterChain,\n    DomainFilter,\n    URLPatternFilter,\n    ContentTypeFilter\n)\nfrom crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer\n\nasync def run_advanced_crawler():\n    # Create a sophisticated filter chain\n    filter_chain = FilterChain([\n        # Domain boundaries\n        DomainFilter(\n            allowed_domains=[\"docs.example.com\"],\n            blocked_domains=[\"old.docs.example.com\"]\n        ),\n        \n        # URL patterns to include\n        URLPatternFilter(patterns=[\"*guide*\", \"*tutorial*\", \"*blog*\"]),\n        \n        # Content type filtering\n        ContentTypeFilter(allowed_types=[\"text/html\"])\n    ])\n\n    # Create a relevance scorer\n    keyword_scorer = KeywordRelevanceScorer(\n        keywords=[\"crawl\", \"example\", \"async\", \"configuration\"],\n        weight=0.7\n    )\n\n    # Set up the configuration\n    config = CrawlerRunConfig(\n        deep_crawl_strategy=BestFirstCrawlingStrategy(\n            max_depth=2,\n            include_external=False,\n            filter_chain=filter_chain,\n            url_scorer=keyword_scorer\n        ),\n        scraping_strategy=LXMLWebScrapingStrategy(),\n        stream=True,\n        verbose=True\n    )\n\n    # Execute the crawl\n    results = []\n    async with AsyncWebCrawler() as crawler:\n        async for result in await crawler.arun(\"https://docs.example.com\", config=config):\n            results.append(result)\n            score = result.metadata.get(\"score\", 0)\n            depth = result.metadata.get(\"depth\", 0)\n            print(f\"Depth: {depth} | Score: {score:.2f} | {result.url}\")\n\n    # Analyze the results\n    print(f\"Crawled {len(results)} high-value pages\")\n    print(f\"Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}\")\n\n    # Group by depth\n    depth_counts = {}\n    for result in results:\n        depth = result.metadata.get(\"depth\", 0)\n        depth_counts[depth] = depth_counts.get(depth, 0) + 1\n\n    print(\"Pages crawled by depth:\")\n    for depth, count in sorted(depth_counts.items()):\n        print(f\"  Depth {depth}: {count} pages\")\n\nif __name__ == \"__main__\":\n    asyncio.run(run_advanced_crawler())\n
```

----------------------------------------

TITLE: Default Crawl4AI Server Configuration YAML
DESCRIPTION: Shows the structure and default values of the `config.yml` file used to customize various aspects of the Crawl4AI server, including application settings, LLM provider, Redis connection, rate limiting rules, security features, crawler parameters, logging, and observability endpoints.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_34

LANGUAGE: yaml
CODE:
```
# Application Configuration
app:
  title: "Crawl4AI API"
  version: "1.0.0" # Consider setting this to match library version, e.g., "0.5.1"
  host: "0.0.0.0"
  port: 8020 # NOTE: This port is used ONLY when running server.py directly. Gunicorn overrides this (see supervisord.conf).
  reload: False # Default set to False - suitable for production
  timeout_keep_alive: 300

# Default LLM Configuration
llm:
  provider: "openai/gpt-4o-mini"
  api_key_env: "OPENAI_API_KEY"
  # api_key: sk-...  # If you pass the API key directly then api_key_env will be ignored

# Redis Configuration (Used by internal Redis server managed by supervisord)
redis:
  host: "localhost"
  port: 6379
  db: 0
  password: ""
  # ... other redis options ...

# Rate Limiting Configuration
rate_limiting:
  enabled: True
  default_limit: "1000/minute"
  trusted_proxies: []
  storage_uri: "memory://"  # Use "redis://localhost:6379" if you need persistent/shared limits

# Security Configuration
security:
  enabled: false # Master toggle for security features
  jwt_enabled: false # Enable JWT authentication (requires security.enabled=true)
  https_redirect: false # Force HTTPS (requires security.enabled=true)
  trusted_hosts: ["*"] # Allowed hosts (use specific domains in production)
  headers: # Security headers (applied if security.enabled=true)
    x_content_type_options: "nosniff"
    x_frame_options: "DENY"
    content_security_policy: "default-src 'self'"
    strict_transport_security: "max-age=63072000; includeSubDomains"

# Crawler Configuration
crawler:
  memory_threshold_percent: 95.0
  rate_limiter:
    base_delay: [1.0, 2.0] # Min/max delay between requests in seconds for dispatcher
  timeouts:
    stream_init: 30.0  # Timeout for stream initialization
    batch_process: 300.0 # Timeout for non-streaming /crawl processing

# Logging Configuration
logging:
  level: "INFO"
  format: "% (asctime)s - % (name)s - % (levelname)s - % (message)s"

# Observability Configuration
observability:
  prometheus:
    enabled: True
    endpoint: "/metrics"
  health_check:
    endpoint: "/health"
```

----------------------------------------

TITLE: Accessing Markdown Content (Python)
DESCRIPTION: Demonstrates how to access the raw, fit markdown, and fit HTML content stored in the `markdown` field of a `CrawlResult` object. This field holds a `MarkdownGenerationResult`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_11

LANGUAGE: python
CODE:
```
print(result.markdown.raw_markdown[:200])
print(result.markdown.fit_markdown)
print(result.markdown.fit_html)
```

----------------------------------------

TITLE: Crawling Dynamic Content with JavaScript Execution in Python
DESCRIPTION: This function demonstrates advanced multi-page crawling with JavaScript execution using Crawl4AI. It extracts commit information from GitHub's TypeScript repository across multiple pages by simulating next page clicks.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_140

LANGUAGE: python
CODE:
```
async def crawl_dynamic_content_pages_method_2():
    print("\n--- Advanced Multi-Page Crawling with JavaScript Execution ---")

    browser_config = BrowserConfig(headless=False, java_script_enabled=True)

    js_next_page_and_wait = """
    (async () => {
        const getCurrentCommit = () => {
            const commits = document.querySelectorAll('li.Box-sc-g0xbh4-0 h4');
            return commits.length > 0 ? commits[0].textContent.trim() : null;
        };

        const initialCommit = getCurrentCommit();
        const button = document.querySelector('a[data-testid="pagination-next-button"]');
        if (button) button.click();

        while (true) {
            await new Promise(resolve => setTimeout(resolve, 100));
            const newCommit = getCurrentCommit();
            if (newCommit && newCommit !== initialCommit) {
                break;
            }
        }
    })();
    """

    schema = {
        "name": "Commit Extractor",
        "baseSelector": "li.Box-sc-g0xbh4-0",
        "fields": [
            {
                "name": "title",
                "selector": "h4.markdown-title",
                "type": "text",
                "transform": "strip",
            },
        ],
    }

    async with AsyncWebCrawler(config=browser_config) as crawler:
        url = "https://github.com/microsoft/TypeScript/commits/main"
        session_id = "typescript_commits_session"
        all_commits = []

        extraction_strategy = JsonCssExtractionStrategy(schema)

        for page in range(3):
            crawler_config = CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                css_selector="li.Box-sc-g0xbh4-0",
                extraction_strategy=extraction_strategy,
                js_code=js_next_page_and_wait if page > 0 else None,
                js_only=page > 0,
                session_id=session_id,
            )

            result = await crawler.arun(url=url, config=crawler_config)
            assert result.success, f"Failed to crawl page {page + 1}"

            commits = json.loads(result.extracted_content)
            all_commits.extend(commits)
            print(f"Page {page + 1}: Found {len(commits)} commits")

        print(f"Successfully crawled {len(all_commits)} commits across 3 pages")
```

----------------------------------------

TITLE: Crawling Multiple URLs Concurrently with Crawl4AI in Python
DESCRIPTION: This snippet illustrates how to crawl multiple URLs in parallel using the `arun_many` method of `Crawl4AI`. It sets up a list of target URLs and a `CrawlerRunConfig`. The example shows two approaches: streaming results as they complete using `async for` (`stream=True`) and collecting all results after all crawls are finished (`stream=False`). It prints success or error messages for each URL, including the length of the extracted markdown content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_6

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def quick_parallel_example():
    urls = [
        "https://example.com/page1",
        "https://example.com/page2",
        "https://example.com/page3"
    ]
    
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=True  # Enable streaming mode
    )

    async with AsyncWebCrawler() as crawler:
        # Stream results as they complete
        print("--- Streaming Results ---")
        async for result in await crawler.arun_many(urls, config=run_conf):
            if result.success:
                print(f"[OK] {result.url}, length: {len(result.markdown.raw_markdown) if result.markdown else 'N/A'}")
            else:
                print(f"[ERROR] {result.url} => {result.error_message}")

        # Or get all results at once (default behavior)
        print("\n--- Batch Results ---")
        run_conf = run_conf.clone(stream=False)
        results = await crawler.arun_many(urls, config=run_conf)
        for res in results:
            if res.success:
                print(f"[OK] {res.url}, length: {len(res.markdown.raw_markdown) if res.markdown else 'N/A'}")
            else:
                print(f"[ERROR] {res.url} => {res.error_message}")

if __name__ == "__main__":
    asyncio.run(quick_parallel_example())
```

----------------------------------------

TITLE: Installing Crawl4AI and Running Setup Tasks Using Bash
DESCRIPTION: These Bash code snippets explain how to install the Crawl4AI package, optionally target a pre-release version, complete a post-installation setup step, and verify that the installation was successful. Required dependencies include Python and Bash; 'pip' must be available and a suitable virtual environment is recommended. Commands are designed for terminal execution and do not require prior setup beyond a functional Python environment. Outputs are printed to the console or system shell.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
# Install the package\npip install -U crawl4ai\n\n# For pre release versions\npip install crawl4ai --pre\n\n# Run post-installation setup\ncrawl4ai-setup\n\n# Verify your installation\ncrawl4ai-doctor
```

----------------------------------------

TITLE: Using arun() Method with CrawlerRunConfig in Python
DESCRIPTION: Example of using the arun() method with a CrawlerRunConfig object to set up crawl parameters like caching, content filtering, and screenshot capture.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import CrawlerRunConfig, CacheMode

run_cfg = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    css_selector="main.article",
    word_count_threshold=10,
    screenshot=True
)

async with AsyncWebCrawler(config=browser_cfg) as crawler:
    result = await crawler.arun("https://example.com/news", config=run_cfg)
    print("Crawled HTML length:", len(result.cleaned_html))
    if result.screenshot:
        print("Screenshot base64 length:", len(result.screenshot))
```

----------------------------------------

TITLE: Installing Crawl4AI via pip (Python, Bash)
DESCRIPTION: This group of bash code snippets demonstrates common pip commands for installing the Crawl4AI Python package. Instructions cover installing the latest stable version, opting into pre-release versions with the --pre flag, and targeting a specific version number. These commands require Python and pip to be installed, and may require the user to use a virtual environment for dependency management. Outputs include downloading and installing Crawl4AI with the appropriate version based on the command.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_24

LANGUAGE: bash
CODE:
```
pip install -U crawl4ai

```

LANGUAGE: bash
CODE:
```
pip install crawl4ai --pre

```

LANGUAGE: bash
CODE:
```
pip install crawl4ai==0.4.3b1

```

----------------------------------------

TITLE: Using Schema Generation Utility with Crawl4AI
DESCRIPTION: Code demonstrating how to automatically generate extraction schemas using LLM integrations. Shows two options: using OpenAI's GPT-4 or the open-source Ollama model, and how to apply the generated schema for extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_191

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy, JsonXPathExtractionStrategy
from crawl4ai import LLMConfig

# Sample HTML with product information
html = """
<div class="product-card">
    <h2 class="title">Gaming Laptop</h2>
    <div class="price">$999.99</div>
    <div class="specs">
        <ul>
            <li>16GB RAM</li>
            <li>1TB SSD</li>
        </ul>
    </div>
</div>
"""

# Option 1: Using OpenAI (requires API token)
css_schema = JsonCssExtractionStrategy.generate_schema(
    html,
    schema_type="css", 
    llm_config = LLMConfig(provider="openai/gpt-4o",api_token="your-openai-token")
)

# Option 2: Using Ollama (open source, no token needed)
xpath_schema = JsonXPathExtractionStrategy.generate_schema(
    html,
    schema_type="xpath",
    llm_config = LLMConfig(provider="ollama/llama3.3", api_token=None)  # Not needed for Ollama
)

# Use the generated schema for fast, repeated extractions
strategy = JsonCssExtractionStrategy(css_schema)
```

----------------------------------------

TITLE: LLM-Based Extraction in Crawl4AI
DESCRIPTION: This example demonstrates how to use LLM-based extraction in Crawl4AI. It configures an LLMExtractionStrategy with OpenAI's GPT-4 model to extract article data based on a defined schema.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_18

LANGUAGE: python
CODE:
```
import asyncio
import json
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class ArticleData(BaseModel):
    headline: str
    summary: str

async def main():
    llm_strategy = LLMExtractionStrategy(
        llm_config = LLMConfig(provider="openai/gpt-4",api_token="sk-YOUR_API_KEY")
        schema=ArticleData.schema(),
        extraction_type="schema",
        instruction="Extract 'headline' and a short 'summary' from the content."
    )

    config = CrawlerRunConfig(
        exclude_external_links=True,
        word_count_threshold=20,
        extraction_strategy=llm_strategy
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        article = json.loads(result.extracted_content)
        print(article)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Basic Crawl4AI Usage with CrawlerRunConfig
DESCRIPTION: Basic example showing how to initialize and use AsyncWebCrawler with CrawlerRunConfig. Demonstrates core parameters like verbose logging and cache mode settings.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    run_config = CrawlerRunConfig(
        verbose=True,            # Detailed logging
        cache_mode=CacheMode.ENABLED,  # Use normal read/write cache
        check_robots_txt=True,   # Respect robots.txt rules
        # ... other parameters
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_config
        )
        
        # Check if blocked by robots.txt
        if not result.success and result.status_code == 403:
            print(f"Error: {result.error_message}")
```

----------------------------------------

TITLE: Concurrent URL Processing in Python
DESCRIPTION: This method implements concurrent processing of multiple URLs using a configurable dispatcher strategy. It supports both batch processing and streaming of results, with options for caching and various content processing configurations.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_28

LANGUAGE: Python
CODE:
```
async def arun_many(
    self,
    urls: List[str],
    config: Optional[CrawlerRunConfig] = None,
    dispatcher: Optional[BaseDispatcher] = None,
    **kwargs,
) -> RunManyReturn:
    config = config or CrawlerRunConfig()

    if dispatcher is None:
        dispatcher = MemoryAdaptiveDispatcher(
            rate_limiter=RateLimiter(
                base_delay=(1.0, 3.0), max_delay=60.0, max_retries=3
            ),
        )

    def transform_result(task_result):
        return (
            setattr(
                task_result.result,
                "dispatch_result",
                DispatchResult(
                    task_id=task_result.task_id,
                    memory_usage=task_result.memory_usage,
                    peak_memory=task_result.peak_memory,
                    start_time=task_result.start_time,
                    end_time=task_result.end_time,
                    error_message=task_result.error_message,
                ),
            )
            or task_result.result
        )

    stream = config.stream

    if stream:
        async def result_transformer():
            async for task_result in dispatcher.run_urls_stream(
                crawler=self, urls=urls, config=config
            ):
                yield transform_result(task_result)

        return result_transformer()
    else:
        _results = await dispatcher.run_urls(crawler=self, urls=urls, config=config)
        return [transform_result(res) for res in _results]
```

----------------------------------------

TITLE: Pattern-Based Extraction with JsonCssExtractionStrategy in Crawl4AI
DESCRIPTION: This example demonstrates how to use JsonCssExtractionStrategy for structured data extraction based on CSS patterns. It extracts news items from Hacker News with a defined schema that captures title and link information from repeated elements.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # Minimal schema for repeated items
    schema = {
        "name": "News Items",
        "baseSelector": "tr.athing",
        "fields": [
            {"name": "title", "selector": "span.titleline a", "type": "text"},
            {
                "name": "link", 
                "selector": "span.titleline a", 
                "type": "attribute", 
                "attribute": "href"
            }
        ]
    }

    config = CrawlerRunConfig(
        # Content filtering
        excluded_tags=["form", "header"],
        exclude_domains=["adsite.com"],
        
        # CSS selection or entire page
        css_selector="table.itemlist",

        # No caching for demonstration
        cache_mode=CacheMode.BYPASS,

        # Extraction strategy
        extraction_strategy=JsonCssExtractionStrategy(schema)
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com/newest", 
            config=config
        )
        data = json.loads(result.extracted_content)
        print("Sample extracted item:", data[:1])  # Show first item

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Basic Link Extraction with AsyncWebCrawler (Python)
DESCRIPTION: This snippet demonstrates the basic usage of `AsyncWebCrawler` to crawl a single URL. After a successful crawl (`result.success`), it accesses the extracted links stored in `result.links`, separating them into internal and external lists. It also prints the counts and a sample internal link dictionary.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_2

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun("https://www.example.com")
    if result.success:
        internal_links = result.links.get("internal", [])
        external_links = result.links.get("external", [])
        print(f"Found {len(internal_links)} internal links.")
        print(f"Found {len(internal_links)} external links.")
        print(f"Found {len(result.media)} media items.")

        # Each link is typically a dictionary with fields like:
        # { "href": "...", "text": "...", "title": "...", "base_domain": "..." }
        if internal_links:
            print("Sample Internal Link:", internal_links[0])
    else:
        print("Crawl failed:", result.error_message)
```

----------------------------------------

TITLE: Basic Usage of Crawl4AI CLI
DESCRIPTION: Demonstrates basic commands for using the Crawl4AI CLI, including crawling a website, getting markdown output, and using verbose JSON output with cache bypass.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_7

LANGUAGE: bash
CODE:
```
# Basic crawling
crwl https://example.com

# Get markdown output
crwl https://example.com -o markdown

# Verbose JSON output with cache bypass
crwl https://example.com -o json -v --bypass-cache

# See usage examples
crwl --example
```

----------------------------------------

TITLE: Extracting Structured Data from Dynamic Content using Crawl4AI in Python
DESCRIPTION: This snippet shows how to extract structured data from a dynamic webpage using Crawl4AI. It uses JsonCssExtractionStrategy for data extraction, and executes JavaScript to interact with the page before extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_128

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_structured_data_using_css_extractor():
    print("\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---")
    schema = {
        "name": "KidoCode Courses",
        "baseSelector": "section.charge-methodology .w-tab-content > div",
        "fields": [
            {
                "name": "section_title",
                "selector": "h3.heading-50",
                "type": "text",
            },
            {
                "name": "section_description",
                "selector": ".charge-content",
                "type": "text",
            },
            {
                "name": "course_name",
                "selector": ".text-block-93",
                "type": "text",
            },
            {
                "name": "course_description",
                "selector": ".course-content-text",
                "type": "text",
            },
            {
                "name": "course_icon",
                "selector": ".image-92",
                "type": "attribute",
                "attribute": "src",
            },
        ],
    }

    browser_config = BrowserConfig(headless=True, java_script_enabled=True)

    js_click_tabs = """
    (async () => {
        const tabs = document.querySelectorAll("section.charge-methodology .tabs-menu-3 > div");
        for(let tab of tabs) {
            tab.scrollIntoView();
            tab.click();
            await new Promise(r => setTimeout(r, 500));
        }
    })();
    """

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema),
        js_code=[js_click_tabs],
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.kidocode.com/degrees/technology", config=crawler_config
        )

        companies = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(companies)} companies")
        print(json.dumps(companies[0], indent=2))

async def main():
    await extract_structured_data_using_css_extractor()

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Basic Usage of arun_many() for Batch Crawling in Python
DESCRIPTION: This example demonstrates the basic usage of arun_many() for batch crawling multiple URLs. It uses the default dispatcher and processes the results after all crawls are completed.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
# Minimal usage: The default dispatcher will be used
results = await crawler.arun_many(
    urls=["https://site1.com", "https://site2.com"],
    config=CrawlerRunConfig(stream=False)  # Default behavior
)

for res in results:
    if res.success:
        print(res.url, "crawled OK!")
    else:
        print("Failed:", res.url, "-", res.error_message)
```

----------------------------------------

TITLE: Streaming Crawl Results with Crawl4AI REST API
DESCRIPTION: Shows how to stream crawl results asynchronously using the Crawl4AI REST API, processing multiple URLs and handling the streaming response.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_63

LANGUAGE: python
CODE:
```
async def test_stream_crawl(session, token: str):
    """Test the /crawl/stream endpoint with multiple URLs."""
    url = "http://localhost:8000/crawl/stream"
    payload = {
        "urls": [
            "https://example.com",
            "https://example.com/page1",  
            "https://example.com/page2",  
            "https://example.com/page3",  
        ],
        "browser_config": {"headless": True, "viewport": {"width": 1200}},
        "crawler_config": {"stream": True, "cache_mode": "bypass"}
    }

    # headers = {"Authorization": f"Bearer {token}"} # If JWT is enabled, more on this later
    
    try:
        async with session.post(url, json=payload, headers=headers) as response:
            status = response.status
            print(f"Status: {status} (Expected: 200)")
            assert status == 200, f"Expected 200, got {status}"
            
            # Read streaming response line-by-line (NDJSON)
            async for line in response.content:
                if line:
                    data = json.loads(line.decode('utf-8').strip())
                    print(f"Streamed Result: {json.dumps(data, indent=2)}")
    except Exception as e:
        print(f"Error in streaming crawl test: {str(e)}")
```

----------------------------------------

TITLE: Comprehensive Crawl4AI Example with Multiple Input Sources in Python
DESCRIPTION: This complete example demonstrates all three crawling methods in Crawl4AI. It crawls a Wikipedia page, saves the HTML to a file, crawls the local file, extracts raw HTML, and crawls the raw content. It also verifies consistency between the different methods by checking markdown length.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/local-files.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import os
import sys
import asyncio
from pathlib import Path
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import CrawlerRunConfig

async def main():
    wikipedia_url = "https://en.wikipedia.org/wiki/apple"
    script_dir = Path(__file__).parent
    html_file_path = script_dir / "apple.html"

    async with AsyncWebCrawler() as crawler:
        # Step 1: Crawl the Web URL
        print("\n=== Step 1: Crawling the Wikipedia URL ===")
        web_config = CrawlerRunConfig(bypass_cache=True)
        result = await crawler.arun(url=wikipedia_url, config=web_config)

        if not result.success:
            print(f"Failed to crawl {wikipedia_url}: {result.error_message}")
            return

        with open(html_file_path, 'w', encoding='utf-8') as f:
            f.write(result.html)
        web_crawl_length = len(result.markdown)
        print(f"Length of markdown from web crawl: {web_crawl_length}\n")

        # Step 2: Crawl from the Local HTML File
        print("=== Step 2: Crawling from the Local HTML File ===")
        file_url = f"file://{html_file_path.resolve()}"
        file_config = CrawlerRunConfig(bypass_cache=True)
        local_result = await crawler.arun(url=file_url, config=file_config)

        if not local_result.success:
            print(f"Failed to crawl local file {file_url}: {local_result.error_message}")
            return

        local_crawl_length = len(local_result.markdown)
        assert web_crawl_length == local_crawl_length, "Markdown length mismatch"
        print(" Markdown length matches between web and local file crawl.\n")

        # Step 3: Crawl Using Raw HTML Content
        print("=== Step 3: Crawling Using Raw HTML Content ===")
        with open(html_file_path, 'r', encoding='utf-8') as f:
            raw_html_content = f.read()
        raw_html_url = f"raw:{raw_html_content}"
        raw_config = CrawlerRunConfig(bypass_cache=True)
        raw_result = await crawler.arun(url=raw_html_url, config=raw_config)

        if not raw_result.success:
            print(f"Failed to crawl raw HTML content: {raw_result.error_message}")
            return

        raw_crawl_length = len(raw_result.markdown)
        assert web_crawl_length == raw_crawl_length, "Markdown length mismatch"
        print(" Markdown length matches between web and raw HTML crawl.\n")

        print("All tests passed successfully!")
    if html_file_path.exists():
        os.remove(html_file_path)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Initializing RateLimiter for Crawl4AI
DESCRIPTION: Constructor for the RateLimiter class, which handles rate limiting and backoff for multiple URL crawling. It defines parameters for delay between requests, maximum backoff, retry attempts, and status codes that trigger rate limiting.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_141

LANGUAGE: python
CODE:
```
class RateLimiter:
    def __init__(
        # Random delay range between requests
        base_delay: Tuple[float, float] = (1.0, 3.0),  
        
        # Maximum backoff delay
        max_delay: float = 60.0,                        
        
        # Retries before giving up
        max_retries: int = 3,                          
        
        # Status codes triggering backoff
        rate_limit_codes: List[int] = [429, 503]        
    )
```

----------------------------------------

TITLE: Executing JavaScript and Clicking Elements in Python with Crawl4AI
DESCRIPTION: This function demonstrates how to execute JavaScript code to locate and click a 'Load More' button on a webpage. It uses a browser instance with JavaScript enabled to interact with an NBC News business page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_130

LANGUAGE: python
CODE:
```
async def simple_example_with_running_js_code():
    print("\n--- Executing JavaScript and Using CSS Selectors ---")

    browser_config = BrowserConfig(headless=True, java_script_enabled=True)

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        js_code="const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();",
        # wait_for="() => { return Array.from(document.querySelectorAll('article.tease-card')).length > 10; }"
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business", config=crawler_config
        )
        print(result.markdown[:500])
```

----------------------------------------

TITLE: Extracting Crypto Prices using JsonCssExtractionStrategy (Python)
DESCRIPTION: This snippet demonstrates defining a schema with a base selector and fields to extract cryptocurrency names and prices from a webpage using CSS selectors. It initializes JsonCssExtractionStrategy, configures the crawler, runs the extraction, and prints the results without involving an LLM.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_0

LANGUAGE: python
CODE:
```
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_crypto_prices():
    # 1. Define a simple extraction schema
    schema = {
        "name": "Crypto Prices",
        "baseSelector": "div.crypto-row",    # Repeated elements
        "fields": [
            {
                "name": "coin_name",
                "selector": "h2.coin-name",
                "type": "text"
            },
            {
                "name": "price",
                "selector": "span.coin-price",
                "type": "text"
            }
        ]
    }

    # 2. Create the extraction strategy
    extraction_strategy = JsonCssExtractionStrategy(schema, verbose=True)

    # 3. Set up your crawler config (if needed)
    config = CrawlerRunConfig(
        # e.g., pass js_code or wait_for if the page is dynamic
        # wait_for="css:.crypto-row:nth-child(20)"
        cache_mode = CacheMode.BYPASS,
        extraction_strategy=extraction_strategy,
    )

    async with AsyncWebCrawler(verbose=True) as crawler:
        # 4. Run the crawl and extraction
        result = await crawler.arun(
            url="https://example.com/crypto-prices",
            
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return

        # 5. Parse the extracted JSON
        data = json.loads(result.extracted_content)
        print(f"Extracted {len(data)} coin entries")
        print(json.dumps(data[0], indent=2) if data else "No data found")

asyncio.run(extract_crypto_prices())
```

----------------------------------------

TITLE: Using CSS Selectors for Content Extraction with Crawl4AI
DESCRIPTION: This function demonstrates how to use CSS selectors to target specific elements on a webpage for extraction. It targets description elements on the NBC News business page and returns the first 500 characters of extracted content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_131

LANGUAGE: python
CODE:
```
async def simple_example_with_css_selector():
    print("\n--- Using CSS Selectors ---")
    browser_config = BrowserConfig(headless=True)
    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, css_selector=".wide-tease-item__description"
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business", config=crawler_config
        )
        print(result.markdown[:500])
```

----------------------------------------

TITLE: Running Web Crawls via the Crawl4AI Command Line Interface in Bash
DESCRIPTION: These Bash commands demonstrate the usage of the Crawl4AI CLI tool 'crwl', supporting various crawling strategies, output formats, and prompting LLM capabilities for extraction. No coding is necessary; only the CLI and its dependencies need to be installed. Parameters such as URL, output mode, crawl strategy, maximum page limit, and LLM queries can be customized as shown. Output is saved or displayed per the chosen options, with constraints determined by the specific CLI subcommands.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_3

LANGUAGE: bash
CODE:
```
# Basic crawl with markdown output\ncrwl https://www.nbcnews.com/business -o markdown\n\n# Deep crawl with BFS strategy, max 10 pages\ncrwl https://docs.crawl4ai.com --deep-crawl bfs --max-pages 10\n\n# Use LLM extraction with a specific question\ncrwl https://www.example.com/products -q \"Extract all product prices\"
```

----------------------------------------

TITLE: Executing Step-by-Step Button Clicks with Crawl4AI in Python
DESCRIPTION: This snippet demonstrates how to use Crawl4AI to load a page, click a 'Next' button, and wait for new content to load. It uses a session ID to maintain state across multiple arun() calls.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CacheMode

js_code = [
    # This JS finds the "Next" button and clicks it
    "const nextButton = document.querySelector('button.next'); nextButton && nextButton.click();"
]

wait_for_condition = "css:.new-content-class"

async with AsyncWebCrawler(headless=True, verbose=True) as crawler:
    # 1. Load the initial page
    result_initial = await crawler.arun(
        url="https://example.com",
        cache_mode=CacheMode.BYPASS,
        session_id="my_session"
    )

    # 2. Click the 'Next' button and wait for new content
    result_next = await crawler.arun(
        url="https://example.com",
        session_id="my_session",
        js_code=js_code,
        wait_for=wait_for_condition,
        js_only=True,
        cache_mode=CacheMode.BYPASS
    )

# `result_next` now contains the updated HTML after clicking 'Next'
```

----------------------------------------

TITLE: Running Basic Crawl4AI Container via Docker Run (Bash)
DESCRIPTION: Illustrates the basic command to run the Crawl4AI Docker container in detached mode (`-d`), mapping the host port 11235 to the container port 11235 (`-p`), naming the container `crawl4ai`, and setting the shared memory size (`--shm-size`). Uses a specific pre-built image tag.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_2

LANGUAGE: bash
CODE:
```
docker run -d \
  -p 11235:11235 \
  --name crawl4ai \
  --shm-size=1g \
  unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number
```

----------------------------------------

TITLE: Structured Data Extraction Complete Example for Crawl4AI
DESCRIPTION: Comprehensive example showing structured data extraction with CSS-based extraction and schema.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_16

LANGUAGE: bash
CODE:
```
crwl https://example.com \
    -e extract_css.yml \
    -s css_schema.json \
    -o json \
    -v
```

----------------------------------------

TITLE: Perform One-Shot Crawl and Extraction
DESCRIPTION: Executes a single crawl and extraction run. Configuration can be provided via inline flags or separate YAML/JSON files. Supports routing through a saved profile and outputting results to stdout or a file.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/codebase/cli.md#_snippet_8

LANGUAGE: Shell
CODE:
```
crwl crawl url [options]
```

----------------------------------------

TITLE: Extracting Data from Dynamic Pages with CSS and JS using Crawl4AI in Python
DESCRIPTION: This example demonstrates handling dynamic web pages and extracting structured data using CSS selectors with `Crawl4AI`. It defines a schema for `JsonCssExtractionStrategy` to specify the data fields and their corresponding CSS selectors for course information on a specific webpage. JavaScript code (`js_click_tabs`) is defined to simulate user interaction (clicking tabs) on the target page. The `AsyncWebCrawler` is configured with JavaScript enabled (`BrowserConfig`) and the JS code to execute (`CrawlerRunConfig`), allowing extraction from content loaded dynamically after tab clicks. The extracted data is parsed from JSON and printed.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_7

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_structured_data_using_css_extractor():
    print("\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---")
    schema = {
        "name": "KidoCode Courses",
        "baseSelector": "section.charge-methodology .w-tab-content > div",
        "fields": [
            {
                "name": "section_title",
                "selector": "h3.heading-50",
                "type": "text",
            },
            {
                "name": "section_description",
                "selector": ".charge-content",
                "type": "text",
            },
            {
                "name": "course_name",
                "selector": ".text-block-93",
                "type": "text",
            },
            {
                "name": "course_description",
                "selector": ".course-content-text",
                "type": "text",
            },
            {
                "name": "course_icon",
                "selector": ".image-92",
                "type": "attribute",
                "attribute": "src",
            },
        ],
    }

    browser_config = BrowserConfig(headless=True, java_script_enabled=True)

    js_click_tabs = """
    (async () => {
        const tabs = document.querySelectorAll("section.charge-methodology .tabs-menu-3 > div");
        for(let tab of tabs) {
            tab.scrollIntoView();
            tab.click();
            await new Promise(r => setTimeout(r, 500)); // Wait for content to potentially load
        }
    })();
    """

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema),
        js_code=[js_click_tabs],
        page_timeout=30000 # Increased timeout for JS execution
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.kidocode.com/degrees/technology", config=crawler_config
        )

        if result.success and result.extracted_content:
            try:
              companies = json.loads(result.extracted_content)
              print(f"Successfully extracted {len(companies)} items")
              if companies:
                 print(json.dumps(companies[0], indent=2))
              else:
                 print("No items extracted according to the schema.")
            except json.JSONDecodeError:
                print("Error decoding extracted JSON content.")
                print(f"Raw extracted content: {result.extracted_content}")
        elif result.success:
             print("Extraction successful, but no content was extracted.")
        else:
             print(f"Crawling failed: {result.error_message}")

async def main():
    await extract_structured_data_using_css_extractor()

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Main Runner Function for Crawl4AI Demos in Python
DESCRIPTION: An async main function that sequentially runs all the demo functions for the Crawl4AI library. It demonstrates the complete set of capabilities from basic crawling to advanced features like proxy rotation and raw HTML processing. The function serves as the entry point for the demo script.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_159

LANGUAGE: python
CODE:
```
async def main():
    """Run all demo functions sequentially"""
    print("=== Comprehensive Crawl4AI Demo ===")
    print("Note: Some examples require API keys or other configurations")

    # Run all demos
    await demo_basic_crawl()
    await demo_parallel_crawl()
    await demo_fit_markdown()
    await demo_llm_structured_extraction_no_schema()
    await demo_css_structured_extraction_no_schema()
    await demo_deep_crawl()
    await demo_js_interaction()
    await demo_media_and_links()
    await demo_screenshot_and_pdf()
    # # await demo_proxy_rotation()
    await demo_raw_html_and_file()

    # Clean up any temp files that may have been created
    print("\n=== Demo Complete ===")
    print("Check for any generated files (screenshots, PDFs) in the current directory")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring CrawlerRunConfig for Content Processing
DESCRIPTION: This example shows how to set up a CrawlerRunConfig object to control crawl behavior, including waiting for specific page elements, setting word count thresholds for content filtering, excluding specific HTML tags, and configuring link handling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

run_cfg = CrawlerRunConfig(
    wait_for="css:.main-content",
    word_count_threshold=15,
    excluded_tags=["nav", "footer"],
    exclude_external_links=True,
    stream=True,  # Enable streaming for arun_many()
)
```

----------------------------------------

TITLE: Initializing CrawlerRunConfig in Python for Crawl4AI
DESCRIPTION: This snippet shows the structure of the CrawlerRunConfig class, which controls how each crawl operates in Crawl4AI. It includes parameters for content extraction, caching, JavaScript execution, resource management, and various other crawl-specific options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
class CrawlerRunConfig:
    def __init__(
        word_count_threshold=200,
        extraction_strategy=None,
        markdown_generator=None,
        cache_mode=None,
        js_code=None,
        wait_for=None,
        screenshot=False,
        pdf=False,
        capture_mhtml=False,
        # Location and Identity Parameters
        locale=None,            # e.g. "en-US", "fr-FR"
        timezone_id=None,       # e.g. "America/New_York"
        geolocation=None,       # GeolocationConfig object
        # Resource Management
        enable_rate_limiting=False,
        rate_limit_config=None,
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=20,
        display_mode=None,
        verbose=True,
        stream=False,  # Enable streaming for arun_many()
        # ... other advanced parameters omitted
    ):
        ...
```

----------------------------------------

TITLE: Error Handling with CosineStrategy and AsyncWebCrawler in Python
DESCRIPTION: Demonstrates proper error handling when using CosineStrategy with AsyncWebCrawler. The example includes checking for successful extraction, handling empty content, and catching exceptions during the extraction process.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_181

LANGUAGE: python
CODE:
```
try:
    result = await crawler.arun(
        url="https://example.com",
        extraction_strategy=strategy
    )
    
    if result.success:
        content = json.loads(result.extracted_content)
        if not content:
            print("No relevant content found")
    else:
        print(f"Extraction failed: {result.error_message}")
        
except Exception as e:
    print(f"Error during extraction: {str(e)}")
```

----------------------------------------

TITLE: Dynamic Content Crawling with Session Management
DESCRIPTION: Complex example showing how to crawl GitHub commits across multiple pages while maintaining session state.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_162

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai.cache_context import CacheMode

async def crawl_dynamic_content():
    async with AsyncWebCrawler() as crawler:
        session_id = "github_commits_session"
        url = "https://github.com/microsoft/TypeScript/commits/main"
        all_commits = []

        # Define extraction schema
        schema = {
            "name": "Commit Extractor",
            "baseSelector": "li.Box-sc-g0xbh4-0",
            "fields": [{
                "name": "title", "selector": "h4.markdown-title", "type": "text"
            }],
        }
        extraction_strategy = JsonCssExtractionStrategy(schema)

        # JavaScript and wait configurations
        js_next_page = """document.querySelector('a[data-testid="pagination-next-button"]').click();"""
        wait_for = """() => document.querySelectorAll('li.Box-sc-g0xbh4-0').length > 0"""

        # Crawl multiple pages
        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                extraction_strategy=extraction_strategy,
                js_code=js_next_page if page > 0 else None,
                wait_for=wait_for if page > 0 else None,
                js_only=page > 0,
                cache_mode=CacheMode.BYPASS
            )

            result = await crawler.arun(config=config)
            if result.success:
                commits = json.loads(result.extracted_content)
                all_commits.extend(commits)
                print(f"Page {page + 1}: Found {len(commits)} commits")

        # Clean up session
        await crawler.crawler_strategy.kill_session(session_id)
        return all_commits
```

----------------------------------------

TITLE: Crawling a Web URL with Crawl4AI
DESCRIPTION: Demonstrates how to crawl a live website (Wikipedia) using Crawl4AI. The code initializes an AsyncWebCrawler with a configuration to bypass cache, then extracts and prints the markdown content of the page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_100

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler
from crawl4ai.async_configs import CrawlerRunConfig

async def crawl_web():
    config = CrawlerRunConfig(bypass_cache=True)
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://en.wikipedia.org/wiki/apple", 
            config=config
        )
        if result.success:
            print("Markdown Content:")
            print(result.markdown)
        else:
            print(f"Failed to crawl: {result.error_message}")

asyncio.run(crawl_web())
```

----------------------------------------

TITLE: AsyncWebCrawler Class Definition and Initialization
DESCRIPTION: Core crawler class implementation with initialization logic, configuration handling, and context management. Supports both context manager and explicit lifecycle patterns.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_20

LANGUAGE: python
CODE:
```
class AsyncWebCrawler:
    _domain_last_hit = {}

    def __init__(
        self,
        crawler_strategy: AsyncCrawlerStrategy = None,
        config: BrowserConfig = None,
        base_directory: str = str(
            os.getenv("CRAWL4_AI_BASE_DIRECTORY", Path.home())),
        thread_safe: bool = False,
        logger: AsyncLoggerBase = None,
        **kwargs,
    ):
        browser_config = config or BrowserConfig()
        self.browser_config = browser_config
        self.logger = logger or AsyncLogger(
            log_file=os.path.join(base_directory, ".crawl4ai", "crawler.log"),
            verbose=self.browser_config.verbose,
            tag_width=10,
        )
        params = {k: v for k, v in kwargs.items() if k in [
            "browser_config", "logger"]}
        self.crawler_strategy = crawler_strategy or AsyncPlaywrightCrawlerStrategy(
            browser_config=browser_config,
            logger=self.logger,
            **params
        )
        self._lock = asyncio.Lock() if thread_safe else None
        self.crawl4ai_folder = os.path.join(base_directory, ".crawl4ai")
        os.makedirs(self.crawl4ai_folder, exist_ok=True)
        os.makedirs(f"{self.crawl4ai_folder}/cache", exist_ok=True)
        self.robots_parser = RobotsParser()
        self.ready = False
        self._deep_handler = DeepCrawlDecorator(self)
        self.arun = self._deep_handler(self.arun)
```

----------------------------------------

TITLE: Using Session Persistence with AsyncWebCrawler
DESCRIPTION: This example demonstrates how to maintain session state across requests by providing storage_state to AsyncWebCrawler. It shows how to pass cookies and localStorage data to access protected pages without needing to authenticate again.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_134

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    storage_dict = {
        "cookies": [
            {
                "name": "session",
                "value": "abcd1234",
                "domain": "example.com",
                "path": "/",
                "expires": 1699999999.0,
                "httpOnly": False,
                "secure": False,
                "sameSite": "None"
            }
        ],
        "origins": [
            {
                "origin": "https://example.com",
                "localStorage": [
                    {"name": "token", "value": "my_auth_token"}
                ]
            }
        ]
    }

    # Provide the storage state as a dictionary to start "already logged in"
    async with AsyncWebCrawler(
        headless=True,
        storage_state=storage_dict
    ) as crawler:
        result = await crawler.arun("https://example.com/protected")
        if result.success:
            print("Protected page content length:", len(result.html))
        else:
            print("Failed to crawl protected page")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Setting Up Automatic Schema Generation with LLM
DESCRIPTION: Generates extraction schemas automatically using LLM instead of manual CSS/XPath writing. Extracts specific data points from HTML content based on natural language queries.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
schema = JsonCssExtractionStrategy.generate_schema(
    html_content,
    schema_type="CSS",
    query="Extract product name, price, and description"
)
```

----------------------------------------

TITLE: Building a Knowledge Graph with LLMExtractionStrategy in Python
DESCRIPTION: A complete example demonstrating how to extract structured knowledge graph data from a webpage using the LLMExtractionStrategy with Pydantic models. The code shows how to define entity and relationship schemas, configure the LLM extraction strategy, and process a webpage to build a knowledge graph.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import os
import json
import asyncio
from typing import List
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class Entity(BaseModel):
    name: str
    description: str

class Relationship(BaseModel):
    entity1: Entity
    entity2: Entity
    description: str
    relation_type: str

class KnowledgeGraph(BaseModel):
    entities: List[Entity]
    relationships: List[Relationship]

async def main():
    # LLM extraction strategy
    llm_strat = LLMExtractionStrategy(
        llmConfig = LlmConfig(provider="openai/gpt-4", api_token=os.getenv('OPENAI_API_KEY')),
        schema=KnowledgeGraph.schema_json(),
        extraction_type="schema",
        instruction="Extract entities and relationships from the content. Return valid JSON.",
        chunk_token_threshold=1400,
        apply_chunking=True,
        input_format="html",
        extra_args={"temperature": 0.1, "max_tokens": 1500}
    )

    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strat,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
        # Example page
        url = "https://www.nbcnews.com/business"
        result = await crawler.arun(url=url, config=crawl_config)

        if result.success:
            with open("kb_result.json", "w", encoding="utf-8") as f:
                f.write(result.extracted_content)
            llm_strat.show_usage()
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring BestFirstCrawlingStrategy with KeywordRelevanceScorer in Python
DESCRIPTION: Explains how to configure `BestFirstCrawlingStrategy` for intelligent, prioritized crawling using a scorer. It initializes a `KeywordRelevanceScorer` with specific keywords and a weight, then assigns this scorer to the `url_scorer` parameter of the strategy. This approach evaluates discovered URLs and visits higher-scoring ones first, focusing the crawl on relevant content. Optional `max_pages` parameter limits the total pages crawled. Depends on `BestFirstCrawlingStrategy` and `KeywordRelevanceScorer` from `crawl4ai.deep_crawling`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_3

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer

# Create a scorer
scorer = KeywordRelevanceScorer(
    keywords=["crawl", "example", "async", "configuration"],
    weight=0.7
)

# Configure the strategy
strategy = BestFirstCrawlingStrategy(
    max_depth=2,
    include_external=False,
    url_scorer=scorer,
    max_pages=25,              # Maximum number of pages to crawl (optional)
)
```

----------------------------------------

TITLE: Pull and Run Crawl4AI Docker Container (Bash)
DESCRIPTION: Pulls a specific revision of the Crawl4AI Docker image from Docker Hub and runs it as a detached container named 'crawl4ai'. It maps the container's port 11235 to the host's port 11235 and allocates 1GB of shared memory.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_14

LANGUAGE: bash
CODE:
```
# Pull and run the latest release candidate
docker pull unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number
docker run -d -p 11235:11235 --name crawl4ai --shm-size=1g unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number

# Visit the playground at http://localhost:11235/playground
```

----------------------------------------

TITLE: Implementing URLPatternFilter Class for Pattern-based URL Filtering in Python
DESCRIPTION: This class provides advanced pattern-based URL filtering. It supports various pattern types including suffixes, prefixes, domains, and complex path patterns. It uses optimized data structures and algorithms for efficient matching.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_185

LANGUAGE: Python
CODE:
```
class URLPatternFilter(URLFilter):
    """Pattern filter balancing speed and completeness"""

    __slots__ = (
        "_simple_suffixes",
        "_simple_prefixes",
        "_domain_patterns",
        "_path_patterns",
        "_reverse",
    )

    PATTERN_TYPES = {
        "SUFFIX": 1,  # *.html
        "PREFIX": 2,  # /foo/*
        "DOMAIN": 3,  # *.example.com
        "PATH": 4,  # Everything else
        "REGEX": 5,
    }

    def __init__(
        self,
        patterns: Union[str, Pattern, List[Union[str, Pattern]]],
        use_glob: bool = True,
        reverse: bool = False,
    ):
        super().__init__()
        self._reverse = reverse
        patterns = [patterns] if isinstance(patterns, (str, Pattern)) else patterns

        self._simple_suffixes = set()
        self._simple_prefixes = set()
        self._domain_patterns = []
        self._path_patterns = []

        for pattern in patterns:
            pattern_type = self._categorize_pattern(pattern)
            self._add_pattern(pattern, pattern_type)

    # ... (methods _categorize_pattern and _add_pattern omitted for brevity)

    @lru_cache(maxsize=10000)
    def apply(self, url: str) -> bool:
        # Quick suffix check (*.html)
        if self._simple_suffixes:
            path = url.split("?")[0]
            if path.split("/")[-1].split(".")[-1] in self._simple_suffixes:
                result = True
                self._update_stats(result)
                return not result if self._reverse else result

        # Domain check
        if self._domain_patterns:
            for pattern in self._domain_patterns:
                if pattern.match(url):
                    result = True
                    self._update_stats(result)
                    return not result if self._reverse else result

        # Prefix check (/foo/*)
        if self._simple_prefixes:
            path = url.split("?")[0]
            if any(path.startswith(p) for p in self._simple_prefixes):
                result = True
                self._update_stats(result)
                return not result if self._reverse else result

        # Complex patterns
        if self._path_patterns:
            if any(p.search(url) for p in self._path_patterns):
                result = True
                self._update_stats(result)
                return not result if self._reverse else result

        result = False
        self._update_stats(result)
        return not result if self._reverse else result
```

----------------------------------------

TITLE: Parallel Web Crawling of Multiple URLs in Python
DESCRIPTION: This function demonstrates parallel crawling of multiple URLs using AsyncWebCrawler's arun_many method. It crawls three different websites simultaneously.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_148

LANGUAGE: python
CODE:
```
async def demo_parallel_crawl():
    """Crawl multiple URLs in parallel"""
    print("\n=== 2. Parallel Crawling ===")

    urls = [
        "https://news.ycombinator.com/",
        "https://example.com/",
        "https://httpbin.org/html",
    ]

    async with AsyncWebCrawler() as crawler:
        results: List[CrawlResult] = await crawler.arun_many(
            urls=urls,
        )

        print(f"Crawled {len(results)} URLs in parallel:")
        for i, result in enumerate(results):
            print(
                f"  {i + 1}. {result.url} - {'Success' if result.success else 'Failed'}"
            )
```

----------------------------------------

TITLE: Implementing Robots.txt Compliance in Crawl4AI
DESCRIPTION: Demonstrates how to enable and use robots.txt compliance in Crawl4AI. It shows how to configure the crawler to check and respect robots.txt rules, with efficient caching.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Enable robots.txt checking in config
    config = CrawlerRunConfig(
        check_robots_txt=True  # Will check and respect robots.txt rules
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            "https://example.com",
            config=config
        )
        
        if not result.success and result.status_code == 403:
            print("Access denied by robots.txt")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Initializing BrowserConfig in crawl4ai
DESCRIPTION: This code demonstrates how to create a BrowserConfig object to control browser behavior including browser type, headless mode, viewport dimensions, proxy settings, and user agent specification.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_cfg = BrowserConfig(
    browser_type="chromium",
    headless=True,
    viewport_width=1280,
    viewport_height=720,
    proxy="http://user:pass@proxy:8080",
    user_agent="Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/116.0.0.0 Safari/537.36",
)
```

----------------------------------------

TITLE: Implementing Batch Crawling with Memory Adaptive Dispatcher in Python
DESCRIPTION: Demonstrates batch processing implementation using AsyncWebCrawler with MemoryAdaptiveDispatcher for controlled concurrent crawling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
async def crawl_batch():
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=False  # Default: get all results at once
    )
    
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=10,
        monitor=CrawlerMonitor(
            display_mode=DisplayMode.DETAILED
        )
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # Get all results at once
        results = await crawler.arun_many(
            urls=urls,
            config=run_config,
            dispatcher=dispatcher
        )
        
        # Process all results after completion
        for result in results:
            if result.success:
                await process_result(result)
            else:
                print(f"Failed to crawl {result.url}: {result.error_message}")
```

----------------------------------------

TITLE: Structured Data Extraction with CSS Selectors Command for Crawl4AI
DESCRIPTION: Shows how to extract structured data using CSS selectors with configuration and schema files.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_11

LANGUAGE: bash
CODE:
```
crwl https://example.com \
    -e extract_css.yml \
    -s css_schema.json \
    -o json
```

----------------------------------------

TITLE: Installing Crawl4AI with All Features in Python
DESCRIPTION: Command for installing Crawl4AI with all available features and dependencies.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_3

LANGUAGE: bash
CODE:
```
pip install crawl4ai[all]
```

----------------------------------------

TITLE: Extracting Structured Data Using CSS Selectors with Crawl4AI
DESCRIPTION: This function shows how to extract structured data using CSS selectors and a predefined schema. It uses JsonCssExtractionStrategy to extract course information from a website, executes JavaScript to click through tabs for complete data collection, and prints the structured results.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_138

LANGUAGE: python
CODE:
```
async def extract_structured_data_using_css_extractor():
    print("\n--- Using JsonCssExtractionStrategy for Fast Structured Output ---")
    schema = {
        "name": "KidoCode Courses",
        "baseSelector": "section.charge-methodology .framework-collection-item.w-dyn-item",
        "fields": [
            {
                "name": "section_title",
                "selector": "h3.heading-50",
                "type": "text",
            },
            {
                "name": "section_description",
                "selector": ".charge-content",
                "type": "text",
            },
            {
                "name": "course_name",
                "selector": ".text-block-93",
                "type": "text",
            },
            {
                "name": "course_description",
                "selector": ".course-content-text",
                "type": "text",
            },
            {
                "name": "course_icon",
                "selector": ".image-92",
                "type": "attribute",
                "attribute": "src",
            },
        ],
    }

    browser_config = BrowserConfig(headless=True, java_script_enabled=True)

    js_click_tabs = """
    (async () => {
        const tabs = document.querySelectorAll("section.charge-methodology .tabs-menu-3 > div");
        for(let tab of tabs) {
            tab.scrollIntoView();
            tab.click();
            await new Promise(r => setTimeout(r, 500));
        }
    })();
    """

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        extraction_strategy=JsonCssExtractionStrategy(schema),
        js_code=[js_click_tabs],
        delay_before_return_html=1
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.kidocode.com/degrees/technology", config=crawler_config
        )

        companies = json.loads(result.extracted_content)
        print(f"Successfully extracted {len(companies)} companies")
        print(json.dumps(companies[0], indent=2))
```

----------------------------------------

TITLE: Session-Based Crawling with Crawl4AI
DESCRIPTION: This snippet demonstrates session-based crawling using Crawl4AI. It shows how to navigate through multiple pages while maintaining the same session context, which is useful for scenarios involving sequential actions or multi-page content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_8

LANGUAGE: python
CODE:
```
async def multi_page_session_crawl():
    async with AsyncWebCrawler() as crawler:
        session_id = "page_navigation_session"
        url = "https://example.com/paged-content"

        for page_number in range(1, 4):
            result = await crawler.arun(
                url=url,
                session_id=session_id,
                js_code="document.querySelector('.next-page-button').click();" if page_number > 1 else None,
                css_selector=".content-section",
                bypass_cache=True
            )
            print(f"Page {page_number} Content:")
            print(result.markdown.raw_markdown[:500])  # Print first 500 characters

# asyncio.run(multi_page_session_crawl())
```

----------------------------------------

TITLE: Making Direct REST API Call for Simple Crawl (Python)
DESCRIPTION: Python script using the `requests` library to make a direct HTTP POST request to the /crawl endpoint. It demonstrates how to manually construct the JSON payload, including nesting configuration objects like `BrowserConfig` and `CrawlerRunConfig` using the specified `{"type": "ClassName", "params": {...}}` format for direct API communication.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_31

LANGUAGE: python
CODE:
```
import requests

# Configuration objects converted to the required JSON structure
browser_config_payload = {
    "type": "BrowserConfig",
    "params": {"headless": True}
}
crawler_config_payload = {
    "type": "CrawlerRunConfig",
    "params": {"stream": False, "cache_mode": "bypass"} # Use string value of enum
}

crawl_payload = {
    "urls": ["https://httpbin.org/html"],
    "browser_config": browser_config_payload,
    "crawler_config": crawler_config_payload
}
response = requests.post(
    "http://localhost:11235/crawl", # Updated port
    # headers={"Authorization": f"Bearer {token}"},  # If JWT is enabled
    json=crawl_payload
)
print(f"Status Code: {response.status_code}")
if response.ok:
    print(response.json())
else:
    print(f"Error: {response.text}")

```

----------------------------------------

TITLE: CSS-Based Wait Conditions in Crawl4AI
DESCRIPTION: Demonstrates how to use wait conditions based on CSS selectors to ensure specific elements are loaded before proceeding with crawling. Includes example of waiting for multiple items to appear on a page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_115

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    config = CrawlerRunConfig(
        # Wait for at least 30 items on Hacker News
        wait_for="css:.athing:nth-child(30)"  
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            config=config
        )
        print("We have at least 30 items loaded!")
        # Rough check
        print("Total items in HTML:", result.cleaned_html.count("athing"))  

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: LLM-based Extraction Configuration in YAML for Crawl4AI
DESCRIPTION: Defines extraction configuration for using a Language Learning Model to extract structured data from web pages, specifying provider, instruction, and parameters.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_8

LANGUAGE: yaml
CODE:
```
# extract_llm.yml
type: "llm"
provider: "openai/gpt-4"
instruction: "Extract all articles with their titles and links"
api_token: "your-token"
params:
  temperature: 0.3
  max_tokens: 1000
```

----------------------------------------

TITLE: Generating LLM-based Extraction Schema
DESCRIPTION: Demonstrates how to generate extraction schemas using LLM providers (OpenAI or Ollama) for structured data extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_124

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai import LLMConfig

# Generate a schema (one-time cost)
html = "<div class='product'><h2>Gaming Laptop</h2><span class='price'>$999.99</span></div>"

# Using OpenAI (requires API token)
schema = JsonCssExtractionStrategy.generate_schema(
    html,
    llm_config = LLMConfig(provider="openai/gpt-4o",api_token="your-openai-token")  # Required for OpenAI
)

# Or using Ollama (open source, no token needed)
schema = JsonCssExtractionStrategy.generate_schema(
    html,
    llm_config = LLMConfig(provider="ollama/llama3.3", api_token=None)  # Not needed for Ollama
)

# Use the schema for fast, repeated extractions
strategy = JsonCssExtractionStrategy(schema)
```

----------------------------------------

TITLE: Basic Session Usage with AsyncWebCrawler in Python
DESCRIPTION: Demonstrates the fundamental pattern for using session management in Crawl4AI. This example shows how to create a session, use it across multiple requests, and properly clean up when finished. It uses session_id to maintain state between different page requests.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig

async with AsyncWebCrawler() as crawler:
    session_id = "my_session"

    # Define configurations
    config1 = CrawlerRunConfig(
        url="https://example.com/page1", session_id=session_id
    )
    config2 = CrawlerRunConfig(
        url="https://example.com/page2", session_id=session_id
    )

    # First request
    result1 = await crawler.arun(config=config1)

    # Subsequent request using the same session
    result2 = await crawler.arun(config=config2)

    # Clean up when done
    await crawler.crawler_strategy.kill_session(session_id)
```

----------------------------------------

TITLE: Building Knowledge Graph with LLMExtractionStrategy in Python
DESCRIPTION: Demonstrates how to create a knowledge graph by combining LLMExtractionStrategy with Pydantic models. Uses OpenAI's GPT-4 to extract entities and relationships from web content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_184

LANGUAGE: python
CODE:
```
import os
import json
import asyncio
from typing import List
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class Entity(BaseModel):
    name: str
    description: str

class Relationship(BaseModel):
    entity1: Entity
    entity2: Entity
    description: str
    relation_type: str

class KnowledgeGraph(BaseModel):
    entities: List[Entity]
    relationships: List[Relationship]

async def main():
    # LLM extraction strategy
    llm_strat = LLMExtractionStrategy(
        provider="openai/gpt-4",
        api_token=os.getenv('OPENAI_API_KEY'),
        schema=KnowledgeGraph.schema_json(),
        extraction_type="schema",
        instruction="Extract entities and relationships from the content. Return valid JSON.",
        chunk_token_threshold=1400,
        apply_chunking=True,
        input_format="html",
        extra_args={"temperature": 0.1, "max_tokens": 1500}
    )

    crawl_config = CrawlerRunConfig(
        extraction_strategy=llm_strat,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
        # Example page
        url = "https://www.nbcnews.com/business"
        result = await crawler.arun(url=url, config=crawl_config)

        if result.success:
            with open("kb_result.json", "w", encoding="utf-8") as f:
                f.write(result.extracted_content)
            llm_strat.show_usage()
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Executing JavaScript in Web Crawling with Crawl4AI in Python
DESCRIPTION: Demonstrates how to execute JavaScript commands in web pages using Crawl4AI. The example shows both single and multiple JS commands for scrolling to the bottom of a page and clicking a 'More' link on Hacker News.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Single JS command
    config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);"
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",  # Example site
            config=config
        )
        print("Crawled length:", len(result.cleaned_html))

    # Multiple commands
    js_commands = [
        "window.scrollTo(0, document.body.scrollHeight);",
        # 'More' link on Hacker News
        "document.querySelector('a.morelink')?.click();",  
    ]
    config = CrawlerRunConfig(js_code=js_commands)

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",  # Another pass
            config=config
        )
        print("After scroll+click, length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Implementing Text Extraction Algorithm in Python
DESCRIPTION: Provides methods for extracting text chunks from HTML documents. This optimized implementation uses a queue-based approach to efficiently process HTML elements, categorizing them as headers or content while maintaining proper order.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_91

LANGUAGE: python
CODE:
```
    def extract_text_chunks(
        self, body: Tag, min_word_threshold: int = None
    ) -> List[Tuple[str, str]]:
        """
        Extracts text chunks from a BeautifulSoup body element while preserving order.
        Returns list of tuples (text, tag_name) for classification.

        Args:
            body: BeautifulSoup Tag object representing the body element

        Returns:
            List of (text, tag_name) tuples
        """
        # Tags to ignore - inline elements that shouldn't break text flow
        INLINE_TAGS = {
            "a",
            "abbr",
            "acronym",
            "b",
            "bdo",
            "big",
            "br",
            "button",
            "cite",
            "code",
            "dfn",
            "em",
            "i",
            "img",
            "input",
            "kbd",
            "label",
            "map",
            "object",
            "q",
            "samp",
            "script",
            "select",
            "small",
            "span",
            "strong",
            "sub",
            "sup",
            "textarea",
            "time",
            "tt",
            "var",
        }

        # Tags that typically contain meaningful headers
        HEADER_TAGS = {"h1", "h2", "h3", "h4", "h5", "h6", "header"}

        chunks = []
        current_text = []
        chunk_index = 0

        def should_break_chunk(tag: Tag) -> bool:
            """Determine if a tag should cause a break in the current text chunk"""
            return tag.name not in INLINE_TAGS and not (
                tag.name == "p" and len(current_text) == 0
            )

        # Use deque for efficient push/pop operations
        stack = deque([(body, False)])

        while stack:
            element, visited = stack.pop()

            if visited:
                # End of block element - flush accumulated text
                if current_text and should_break_chunk(element):
                    text = " ".join("".join(current_text).split())
                    if text:
                        tag_type = (
                            "header" if element.name in HEADER_TAGS else "content"
                        )
                        chunks.append((chunk_index, text, tag_type, element))
                        chunk_index += 1
                    current_text = []
                continue

            if isinstance(element, NavigableString):
                if str(element).strip():
                    current_text.append(str(element).strip())
                continue

            # Pre-allocate children to avoid multiple list operations
            children = list(element.children)
            if not children:
                continue

            # Mark block for revisit after processing children
            stack.append((element, True))

            # Add children in reverse order for correct processing
            for child in reversed(children):
                if isinstance(child, (Tag, NavigableString)):
                    stack.append((child, False))

        # Handle any remaining text
        if current_text:
            text = " ".join("".join(current_text).split())
            if text:
                chunks.append((chunk_index, text, "content", body))

        if min_word_threshold:
            chunks = [
                chunk for chunk in chunks if len(chunk[1].split()) >= min_word_threshold
            ]

        return chunks
```

----------------------------------------

TITLE: Customizing Crawl Options with CrawlerRunConfig
DESCRIPTION: Demonstrates how to customize crawl behavior using CrawlerRunConfig parameters such as content thresholds, link handling, and element processing options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/simple-crawling.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
run_config = CrawlerRunConfig(
    word_count_threshold=10,        # Minimum words per content block
    exclude_external_links=True,    # Remove external links
    remove_overlay_elements=True,   # Remove popups/modals
    process_iframes=True           # Process iframe content
)

result = await crawler.arun(
    url="https://example.com",
    config=run_config
)
```

----------------------------------------

TITLE: LLM-Powered Schema Generation for Data Extraction
DESCRIPTION: Demonstrates how to use LLM capabilities to automatically generate extraction schemas for web scraping. The example uses JsonCssExtractionStrategy with Gemini LLM to create a schema for extracting product information from HTML.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy
from crawl4ai import LLMConfig

llm_config = LLMConfig(provider="gemini/gemini-1.5-pro", api_token="env:GEMINI_API_KEY")

schema = JsonCssExtractionStrategy.generate_schema(
    html="<div class='product'><h2>Product Name</h2><span class='price'>$99</span></div>",
    llm_config = llm_config,
    query="Extract product name and price"
)
print(schema)
```

----------------------------------------

TITLE: Combining CSS and Regex Extraction Strategies (Python)
DESCRIPTION: Demonstrates a two-pass extraction workflow. The first pass uses `JsonCssExtractionStrategy` to extract structured data based on CSS selectors. The second pass iterates through the extracted data and applies `RegexExtractionStrategy` to find specific patterns like emails, phone numbers, or custom dimensions within text fields.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_15

LANGUAGE: python
CODE:
```
# First pass: Extract structure with CSS
css_strategy = JsonCssExtractionStrategy(product_schema)
css_result = await crawler.arun(url, config=CrawlerRunConfig(extraction_strategy=css_strategy))
product_data = json.loads(css_result.extracted_content)

# Second pass: Extract specific fields with regex
descriptions = [product["description"] for product in product_data]
regex_strategy = RegexExtractionStrategy(
    pattern=RegexExtractionStrategy.Email | RegexExtractionStrategy.PhoneUS,
    custom={"dimension": r"\d+x\d+x\d+ (?:cm|in)"}
)

# Process descriptions with regex
for text in descriptions:
    matches = regex_strategy.extract("", text)  # Direct extraction
```

----------------------------------------

TITLE: Using RegexExtractionStrategy with Built-in or Custom Patterns (Python)
DESCRIPTION: Shows two basic ways to use RegexExtractionStrategy. Method 1 uses pre-defined built-in patterns (like Email or Url). Method 2 uses a dictionary of custom regex patterns provided by the user.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_10

LANGUAGE: python
CODE:
```
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, RegexExtractionStrategy

# Method 1: Use built-in patterns
strategy = RegexExtractionStrategy(
    pattern = RegexExtractionStrategy.Email | RegexExtractionStrategy.Url
)

# Method 2: Use custom patterns
price_pattern = {"usd_price": r"\$\s?\d{1,3}(?:,\d{3})*(?:\.\d{2})?"}
strategy = RegexExtractionStrategy(custom=price_pattern)
```

----------------------------------------

TITLE: Accessing Extracted Table Data with Crawl4AI in Python
DESCRIPTION: Demonstrates how to access structured table data extracted by Crawl4AI. It checks if the crawl was successful (`result.success`), retrieves the list of detected data tables from `result.media.get("tables", [])`, prints the total count, and then accesses properties like caption, headers, and rows for the first detected table. Extracted tables are those scoring above a defined threshold.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_8

LANGUAGE: python
CODE:
```
```python
if result.success:
    tables = result.media.get("tables", [])
    print(f"Found {len(tables)} data tables on the page")
    
    if tables:
        # Access the first table
        first_table = tables[0]
        print(f"Table caption: {first_table.get('caption', 'No caption')}")
        print(f"Headers: {first_table.get('headers', [])}")
        
        # Print the first 3 rows
        for i, row in enumerate(first_table.get('rows', [])[:3]):
            print(f"Row {i+1}: {row}")
```
```

----------------------------------------

TITLE: Installing Crawl4AI with all optional features
DESCRIPTION: Installs Crawl4AI with all optional features including PyTorch and Transformers, followed by running the setup command.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_6

LANGUAGE: bash
CODE:
```
pip install crawl4ai[all]
crawl4ai-setup
```

----------------------------------------

TITLE: Accessing Various Result Fields in Crawl4AI
DESCRIPTION: This code snippet demonstrates how to access different fields from a crawl result, including checking success status, status code, links count, markdown content, and extracted content. It also shows error handling when crawling fails.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_30

LANGUAGE: python
CODE:
```
if result.success:
    print(result.status_code, result.response_headers)
    print("Links found:", len(result.links.get("internal", [])))
    if result.markdown:
        print("Markdown snippet:", result.markdown.raw_markdown[:200])
    if result.extracted_content:
        print("Structured JSON:", result.extracted_content)
else:
    print("Error:", result.error_message)
```

----------------------------------------

TITLE: Advanced Usage Example for Crawl4AI CLI
DESCRIPTION: Shows how to use the Crawl4AI CLI with a JSON-CSS schema for structured data extraction from a specific webpage.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_8

LANGUAGE: bash
CODE:
```
crwl "https://www.infoq.com/ai-ml-data-eng/" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;
```

----------------------------------------

TITLE: Testing Streaming API Endpoint with Python
DESCRIPTION: Demonstrates how to send an asynchronous POST request to the /crawl/stream endpoint using the httpx library to process multiple URLs and stream results line by line (NDJSON). It shows the required payload structure for browser and crawler configurations and includes basic error handling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_32

LANGUAGE: python
CODE:
```
import json
import httpx # Use httpx for async streaming example

async def test_stream_crawl(token: str = None): # Made token optional
    """Test the /crawl/stream endpoint with multiple URLs."""
    url = "http://localhost:11235/crawl/stream" # Updated port
    payload = {
        "urls": [
            "https://httpbin.org/html",
            "https://httpbin.org/links/5/0",
        ],
        "browser_config": {
            "type": "BrowserConfig",
            "params": {"headless": True, "viewport": {"type": "dict", "value": {"width": 1200, "height": 800}}} # Viewport needs type:dict
        },
        "crawler_config": {
            "type": "CrawlerRunConfig",
            "params": {"stream": True, "cache_mode": "bypass"}
        }
    }

    headers = {}
    # if token:
    #    headers = {"Authorization": f"Bearer {token}"} # If JWT is enabled

    try:
        async with httpx.AsyncClient() as client:
            async with client.stream("POST", url, json=payload, headers=headers, timeout=120.0) as response:
                print(f"Status: {response.status_code} (Expected: 200)")
                response.raise_for_status() # Raise exception for bad status codes

                # Read streaming response line-by-line (NDJSON)
                async for line in response.aiter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            # Check for completion marker
                            if data.get("status") == "completed":
                                print("Stream completed.")
                                break
                            print(f"Streamed Result: {json.dumps(data, indent=2)}")
                        except json.JSONDecodeError:
                            print(f"Warning: Could not decode JSON line: {line}")

    except httpx.HTTPStatusError as e:
         print(f"HTTP error occurred: {e.response.status_code} - {e.response.text}")
    except Exception as e:
        print(f"Error in streaming crawl test: {str(e)}")
```

----------------------------------------

TITLE: File-based Storage State Implementation
DESCRIPTION: Example demonstrating how to use storage_state with a JSON file path
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    async with AsyncWebCrawler(
        headless=True,
        storage_state="mystate.json"  # Uses a JSON file instead of a dictionary
    ) as crawler:
        result = await crawler.arun(url='https://example.com/protected')
        if result.success:
            print("Crawl succeeded with pre-loaded session data!")
            print("Page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: LLM Schema JSON for Structured Data Extraction in Crawl4AI
DESCRIPTION: Defines a JSON schema for LLM-based extraction of article information, specifying the structure and properties to be extracted.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_9

LANGUAGE: json
CODE:
```
// llm_schema.json
{
  "title": "Article",
  "type": "object",
  "properties": {
    "title": {
      "type": "string",
      "description": "The title of the article"
    },
    "link": {
      "type": "string",
      "description": "URL to the full article"
    }
  }
}
```

----------------------------------------

TITLE: Combining CSS Selection Methods for Fine-Grained Content Control in Python
DESCRIPTION: This example shows how to combine css_selector and target_elements in CrawlerRunConfig to achieve precise control over content extraction. It demonstrates focusing on specific elements while preserving page context and applying global filters.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_12

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    # Target specific content but preserve page context
    config = CrawlerRunConfig(
        # Focus markdown on main content and sidebar
        target_elements=["#main-content", ".sidebar"],
        
        # Global filters applied to entire page
        excluded_tags=["nav", "footer", "header"],
        exclude_external_links=True,
        
        # Use basic content thresholds
        word_count_threshold=15,
        
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.com/article",
            config=config
        )
        
        print(f"Content focuses on specific elements, but all links still analyzed")
        print(f"Internal links: {len(result.links.get('internal', []))}")
        print(f"External links: {len(result.links.get('external', []))}")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring Browser and Crawler Settings in Python using Crawl4AI
DESCRIPTION: Demonstrates how to use the new BrowserConfig and CrawlerRunConfig objects to configure browser and crawler behavior. Shows initialization of an AsyncWebCrawler with custom viewport settings and cache mode.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.2.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai import BrowserConfig, CrawlerRunConfig, AsyncWebCrawler

browser_config = BrowserConfig(headless=True, viewport_width=1920, viewport_height=1080)
crawler_config = CrawlerRunConfig(cache_mode="BYPASS")

async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com", config=crawler_config)
    print(result.markdown[:500])
```

----------------------------------------

TITLE: Accessing All CrawlResult Fields in Python
DESCRIPTION: This asynchronous function demonstrates how to access and print various data points available in a `CrawlResult` object after a crawl. It checks for success, then accesses fields like URL, status code, HTML, Markdown, media, links, extracted content, binary data, network requests, and console messages, providing examples of basic analysis for the latter two. Requires an asynchronous context and a `CrawlResult` object.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_23

LANGUAGE: python
CODE:
```
async def handle_result(result: CrawlResult):
    if not result.success:
        print("Crawl error:", result.error_message)
        return
    
    # Basic info
    print("Crawled URL:", result.url)
    print("Status code:", result.status_code)
    
    # HTML
    print("Original HTML size:", len(result.html))
    print("Cleaned HTML size:", len(result.cleaned_html or ""))
    
    # Markdown output
    if result.markdown:
        print("Raw Markdown:", result.markdown.raw_markdown[:300])
        print("Citations Markdown:", result.markdown.markdown_with_citations[:300])
        if result.markdown.fit_markdown:
            print("Fit Markdown:", result.markdown.fit_markdown[:200])

    # Media & Links
    if "images" in result.media:
        print("Image count:", len(result.media["images"]))
    if "internal" in result.links:
        print("Internal link count:", len(result.links["internal"]))

    # Extraction strategy result
    if result.extracted_content:
        print("Structured data:", result.extracted_content)
    
    # Screenshot/PDF/MHTML
    if result.screenshot:
        print("Screenshot length:", len(result.screenshot))
    if result.pdf:
        print("PDF bytes length:", len(result.pdf))
    if result.mhtml:
        print("MHTML length:", len(result.mhtml))
        
    # Network and console capturing
    if result.network_requests:
        print(f"Network requests captured: {len(result.network_requests)}")
        # Analyze request types
        req_types = {}
        for req in result.network_requests:
            if "resource_type" in req:
                req_types[req["resource_type"]] = req_types.get(req["resource_type"], 0) + 1
        print(f"Resource types: {req_types}")
        
    if result.console_messages:
        print(f"Console messages captured: {len(result.console_messages)}")
        # Count by message type
        msg_types = {}
        for msg in result.console_messages:
            msg_types[msg.get("type", "unknown")] = msg_types.get(msg.get("type", "unknown"), 0) + 1
        print(f"Message types: {msg_types}")
```

----------------------------------------

TITLE: Running Docker Container with Custom Config Mount using Bash
DESCRIPTION: Shows a `docker run` command in Bash to start the `unclecode/crawl4ai` container in detached mode (`-d`). It maps port 11235, names the container, loads environment variables from `.llm.env`, allocates shared memory (`--shm-size`), and crucially, mounts a local `my-custom-config.yml` file to `/app/config.yml` inside the container using the `-v` flag. This overrides the default configuration at runtime. Requires Docker installed and the custom config file present.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_34

LANGUAGE: bash
CODE:
```
# Assumes my-custom-config.yml is in the current directory
docker run -d -p 11235:11235 \
  --name crawl4ai-custom-config \
  --env-file .llm.env \
  --shm-size=1g \
  -v $(pwd)/my-custom-config.yml:/app/config.yml \
  unclecode/crawl4ai:latest # Or your specific tag
```

----------------------------------------

TITLE: CSS-based Structured Data Extraction from Web Pages in Python
DESCRIPTION: This function demonstrates the use of JsonCssExtractionStrategy to extract structured data from a web page (The Hacker News) using CSS selectors. It generates a schema using LLM and then uses it for fast extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_151

LANGUAGE: python
CODE:
```
async def demo_css_structured_extraction_no_schema():
    """Extract structured data using CSS selectors"""
    print("\n=== 5. CSS-Based Structured Extraction ===")
    # Sample HTML for schema generation (one-time cost)
    sample_html = """
<div class="body-post clear">
    <a class="story-link" href="https://thehackernews.com/2025/04/malicious-python-packages-on-pypi.html">
        <div class="clear home-post-box cf">
            <div class="home-img clear">
                <div class="img-ratio">
                    <img alt="..." src="...">
                </div>
            </div>
            <div class="clear home-right">
                <h2 class="home-title">Malicious Python Packages on PyPI Downloaded 39,000+ Times, Steal Sensitive Data</h2>
                <div class="item-label">
                    <span class="h-datetime"><i class="icon-font icon-calendar"></i>Apr 05, 2025</span>
                    <span class="h-tags">Malware / Supply Chain Attack</span>
                </div>
                <div class="home-desc"> Cybersecurity researchers have...</div>
            </div>
        </div>
    </a>
</div>
    """

    # Check if schema file exists
    schema_file_path = f"{__cur_dir__}/tmp/schema.json"
    if os.path.exists(schema_file_path):
        with open(schema_file_path, "r") as f:
            schema = json.load(f)
    else:
        # Generate schema using LLM (one-time setup)
        schema = JsonCssExtractionStrategy.generate_schema(
            html=sample_html,
            llm_config=LLMConfig(
                provider="groq/qwen-2.5-32b",
                api_token="env:GROQ_API_KEY",
            ),
            query="From https://thehackernews.com/, I have shared a sample of one news div with a title, date, and description. Please generate a schema for this news div.",
        )

    print(f"Generated schema: {json.dumps(schema, indent=2)}")
    # Save the schema to a file , and use it for future extractions, in result for such extraction you will call LLM once
    with open(f"{__cur_dir__}/tmp/schema.json", "w") as f:
        json.dump(schema, f, indent=2)

    # Create no-LLM extraction strategy with the generated schema
    extraction_strategy = JsonCssExtractionStrategy(schema)
    config = CrawlerRunConfig(extraction_strategy=extraction_strategy)

    # Use the fast CSS extraction (no LLM calls during extraction)
    async with AsyncWebCrawler() as crawler:
        results: List[CrawlResult] = await crawler.arun(
            "https://thehackernews.com", config=config
        )

        for result in results:
            print(f"URL: {result.url}")
            print(f"Success: {result.success}")
            if result.success:
                data = json.loads(result.extracted_content)
                print(json.dumps(data, indent=2))
            else:
                print("Failed to extract structured data")
```

----------------------------------------

TITLE: Basic Crawl4AI CLI Usage Commands
DESCRIPTION: Demonstrates the basic usage patterns for the Crawl4AI CLI tool, including simple crawling, output format selection, and additional options like verbose mode and cache bypass.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
# Basic crawling
crwl https://example.com

# Get markdown output
crwl https://example.com -o markdown

# Verbose JSON output with cache bypass
crwl https://example.com -o json -v --bypass-cache

# See usage examples
crwl --example
```

----------------------------------------

TITLE: Configuring Basic Deep Crawl with BFS Strategy in Python
DESCRIPTION: This snippet demonstrates how to set up a basic deep crawl using the BFSDeepCrawlStrategy with a depth of 2 levels. It configures the crawler to only follow links within the same domain and uses the LXMLWebScrapingStrategy for content extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_205

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=2, include_external=False),
    scraping_strategy=LXMLWebScrapingStrategy(),
    verbose=True,  # Show progress during crawling
)
```

----------------------------------------

TITLE: LLM-based Structured Data Extraction from Web Pages in Python
DESCRIPTION: This function demonstrates the use of LLMExtractionStrategy to extract structured data from a web page (Hacker News) using a language model. It extracts news titles, source URLs, and comment counts.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_150

LANGUAGE: python
CODE:
```
async def demo_llm_structured_extraction_no_schema():
    # Create a simple LLM extraction strategy (no schema required)
    extraction_strategy = LLMExtractionStrategy(
        llm_config=LLMConfig(
            provider="groq/qwen-2.5-32b",
            api_token="env:GROQ_API_KEY",
        ),
        instruction="This is news.ycombinator.com, extract all news, and for each, I want title, source url, number of comments.",
        extract_type="schema",
        schema="{title: string, url: string, comments: int}",
        extra_args={
            "temperature": 0.0,
            "max_tokens": 4096,
        },
        verbose=True,
    )

    config = CrawlerRunConfig(extraction_strategy=extraction_strategy)

    async with AsyncWebCrawler() as crawler:
        results: List[CrawlResult] = await crawler.arun(
            "https://news.ycombinator.com/", config=config
        )

        for result in results:
            print(f"URL: {result.url}")
            print(f"Success: {result.success}")
            if result.success:
                data = json.loads(result.extracted_content)
                print(json.dumps(data, indent=2))
            else:
                print("Failed to extract structured data")
```

----------------------------------------

TITLE: Basic JavaScript Execution in Crawl4AI
DESCRIPTION: Shows how to execute JavaScript commands during web crawling, including scrolling and clicking elements. Demonstrates both single and multiple JavaScript command execution with session management.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_114

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Single JS command
    config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);"
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",  # Example site
            config=config
        )
        print("Crawled length:", len(result.cleaned_html))

    # Multiple commands
    js_commands = [
        "window.scrollTo(0, document.body.scrollHeight);",
        # 'More' link on Hacker News
        "document.querySelector('a.morelink')?.click();",  
    ]
    config = CrawlerRunConfig(js_code=js_commands)

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",  # Another pass
            config=config
        )
        print("After scroll+click, length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Basic Web Crawling with Crawl4AI
DESCRIPTION: This function demonstrates a simple web crawl using Crawl4AI. It configures the browser and crawler, then crawls a news website and prints the first 500 characters of the extracted markdown content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_127

LANGUAGE: python
CODE:
```
async def simple_crawl():
    print("\n--- Basic Usage ---")
    browser_config = BrowserConfig(headless=True)
    crawler_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business", config=crawler_config
        )
        print(result.markdown[:500])
```

----------------------------------------

TITLE: Installing or Updating Crawl4AI via Pip (Bash)
DESCRIPTION: Provides the shell command to install the latest version of the Crawl4AI library or upgrade an existing installation using the Python package manager, pip. The `-U` flag ensures that the package is upgraded if it's already installed. Requires pip to be available in the shell environment.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.6.0.md#_snippet_2

LANGUAGE: bash
CODE:
```
```bash
pip install -U crawl4ai
```
```

----------------------------------------

TITLE: Default Server Configuration Structure in YAML
DESCRIPTION: Provides a detailed example of the `config.yml` file structure used to configure the Crawl4AI server. It covers sections like `app` (basic info, host, port), `llm` (provider, API key), `redis` (connection details), `rate_limiting`, `security` (JWT, HTTPS, trusted hosts, headers), `crawler` (memory limits, delays, timeouts), `logging`, and `observability` (Prometheus, health checks). Comments explain specific settings, like the distinction between the `app.port` and the port used by Gunicorn. This file dictates the server's runtime behavior.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_33

LANGUAGE: yaml
CODE:
```
# Application Configuration
app:
  title: "Crawl4AI API"
  version: "1.0.0" # Consider setting this to match library version, e.g., "0.5.1"
  host: "0.0.0.0"
  port: 8020 # NOTE: This port is used ONLY when running server.py directly. Gunicorn overrides this (see supervisord.conf).
  reload: False # Default set to False - suitable for production
  timeout_keep_alive: 300

# Default LLM Configuration
llm:
  provider: "openai/gpt-4o-mini"
  api_key_env: "OPENAI_API_KEY"
  # api_key: sk-...  # If you pass the API key directly then api_key_env will be ignored

# Redis Configuration (Used by internal Redis server managed by supervisord)
redis:
  host: "localhost"
  port: 6379
  db: 0
  password: ""
  # ... other redis options ...

# Rate Limiting Configuration
rate_limiting:
  enabled: True
  default_limit: "1000/minute"
  trusted_proxies: []
  storage_uri: "memory://"  # Use "redis://localhost:6379" if you need persistent/shared limits

# Security Configuration
security:
  enabled: false # Master toggle for security features
  jwt_enabled: false # Enable JWT authentication (requires security.enabled=true)
  https_redirect: false # Force HTTPS (requires security.enabled=true)
  trusted_hosts: ["*"] # Allowed hosts (use specific domains in production)
  headers: # Security headers (applied if security.enabled=true)
    x_content_type_options: "nosniff"
    x_frame_options: "DENY"
    content_security_policy: "default-src 'self'"
    strict_transport_security: "max-age=63072000; includeSubDomains"

# Crawler Configuration
crawler:
  memory_threshold_percent: 95.0
  rate_limiter:
    base_delay: [1.0, 2.0] # Min/max delay between requests in seconds for dispatcher
  timeouts:
    stream_init: 30.0  # Timeout for stream initialization
    batch_process: 300.0 # Timeout for non-streaming /crawl processing

# Logging Configuration
logging:
  level: "INFO"
  format: "%(asctime)s - %(name)s - %(levelname)s - %(message)s"

# Observability Configuration
observability:
  prometheus:
    enabled: True
    endpoint: "/metrics"
  health_check:
    endpoint: "/health"
```

----------------------------------------

TITLE: Defining E-commerce Data Extraction Schema in Python
DESCRIPTION: This Python dictionary defines a schema for extracting structured data from an e-commerce HTML page. It uses CSS selectors to target elements and specifies different field types ('text', 'attribute', 'nested', 'list', 'nested_list') to handle various data structures, including single values, attributes, single sub-objects, simple lists, and complex lists of objects.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_2

LANGUAGE: python
CODE:
```
schema = {
    "name": "E-commerce Product Catalog",
    "baseSelector": "div.category",
    # (1) We can define optional baseFields if we want to extract attributes 
    # from the category container
    "baseFields": [
        {"name": "data_cat_id", "type": "attribute", "attribute": "data-cat-id"}, 
    ],
    "fields": [
        {
            "name": "category_name",
            "selector": "h2.category-name",
            "type": "text"
        },
        {
            "name": "products",
            "selector": "div.product",
            "type": "nested_list",    # repeated sub-objects
            "fields": [
                {
                    "name": "name",
                    "selector": "h3.product-name",
                    "type": "text"
                },
                {
                    "name": "price",
                    "selector": "p.product-price",
                    "type": "text"
                },
                {
                    "name": "details",
                    "selector": "div.product-details",
                    "type": "nested",  # single sub-object
                    "fields": [
                        {
                            "name": "brand",
                            "selector": "span.brand",
                            "type": "text"
                        },
                        {
                            "name": "model",
                            "selector": "span.model",
                            "type": "text"
                        }
                    ]
                },
                {
                    "name": "features",
                    "selector": "ul.product-features li",
                    "type": "list",
                    "fields": [
                        {"name": "feature", "type": "text"} 
                    ]
                },
                {
                    "name": "reviews",
                    "selector": "div.review",
                    "type": "nested_list",
                    "fields": [
                        {
                            "name": "reviewer", 
                            "selector": "span.reviewer", 
                            "type": "text"
                        },
                        {
                            "name": "rating", 
                            "selector": "span.rating", 
                            "type": "text"
                        },
                        {
                            "name": "comment", 
                            "selector": "p.review-text", 
                            "type": "text"
                        }
                    ]
                },
                {
                    "name": "related_products",
                    "selector": "ul.related-products li",
                    "type": "list",
                    "fields": [
                        {
                            "name": "name", 
                            "selector": "span.related-name", 
                            "type": "text"
                        },
                        {
                            "name": "price", 
                            "selector": "span.related-price", 
                            "type": "text"
                        }
                    ]
                }
            ]
        }
    ]
}
```

----------------------------------------

TITLE: Implementing CrawlerRunConfig Class in Python
DESCRIPTION: A comprehensive configuration class for controlling crawler runtime behavior including content extraction, page manipulation, caching, and timing parameters.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
class CrawlerRunConfig():
    _UNWANTED_PROPS = {
        'disable_cache' : 'Instead, use cache_mode=CacheMode.DISABLED',
        'bypass_cache' : 'Instead, use cache_mode=CacheMode.BYPASS',
        'no_cache_read' : 'Instead, use cache_mode=CacheMode.WRITE_ONLY',
        'no_cache_write' : 'Instead, use cache_mode=CacheMode.READ_ONLY',
    }

    """
    Configuration class for controlling how the crawler runs each crawl operation.
    This includes parameters for content extraction, page manipulation, waiting conditions,
    caching, and other runtime behaviors.
    """
```

----------------------------------------

TITLE: Form Interaction Example with JavaScript in Crawl4AI
DESCRIPTION: This snippet demonstrates how to interact with a search form on a website by filling in fields and submitting the form using JavaScript. It configures the crawler to wait for specific CSS elements after form submission.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_117

LANGUAGE: python
CODE:
```
js_form_interaction = """
document.querySelector('#your-search').value = 'TypeScript commits';
document.querySelector('form').submit();
"""

config = CrawlerRunConfig(
    js_code=js_form_interaction,
    wait_for="css:.commit"
)
result = await crawler.arun(url="https://github.com/search", config=config)
```

----------------------------------------

TITLE: Configuring AsyncWebCrawler for Lazy-Loaded Images in Python
DESCRIPTION: This snippet demonstrates how to configure the AsyncWebCrawler to handle lazy-loaded images. It uses wait_for_images, scan_full_page, and scroll_delay options to ensure all images are captured during the crawl.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/lazy-loading.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BrowserConfig
from crawl4ai.async_configs import CacheMode

async def main():
    config = CrawlerRunConfig(
        # Force the crawler to wait until images are fully loaded
        wait_for_images=True,

        # Option 1: If you want to automatically scroll the page to load images
        scan_full_page=True,  # Tells the crawler to try scrolling the entire page
        scroll_delay=0.5,     # Delay (seconds) between scroll steps

        # Option 2: If the site uses a 'Load More' or JS triggers for images,
        # you can also specify js_code or wait_for logic here.

        cache_mode=CacheMode.BYPASS,
        verbose=True
    )

    async with AsyncWebCrawler(config=BrowserConfig(headless=True)) as crawler:
        result = await crawler.arun("https://www.example.com/gallery", config=config)
        
        if result.success:
            images = result.media.get("images", [])
            print("Images found:", len(images))
            for i, img in enumerate(images[:5]):
                print(f"[Image {i}] URL: {img['src']}, Score: {img.get('score','N/A')}")
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Using BM25ContentFilter for Query-Based Content Filtering
DESCRIPTION: This snippet demonstrates how to use BM25ContentFilter to selectively filter content based on a search query. It configures a BM25ContentFilter with a specific user query, threshold, and stemming option, then integrates it with a markdown generator and crawler configuration.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_106

LANGUAGE: python
CODE:
```
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import BM25ContentFilter
from crawl4ai import CrawlerRunConfig

bm25_filter = BM25ContentFilter(
    user_query="machine learning",
    bm25_threshold=1.2,
    use_stemming=True
)

md_generator = DefaultMarkdownGenerator(
    content_filter=bm25_filter,
    options={"ignore_links": True}
)

config = CrawlerRunConfig(markdown_generator=md_generator)
```

----------------------------------------

TITLE: Extracting Structured Data Using LLM with Crawl4AI
DESCRIPTION: This function demonstrates how to extract structured data from webpages using LLM-based extraction. It configures an extraction strategy with a defined schema, custom instructions, and provider-specific parameters to extract OpenAI model fee information from their pricing page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_137

LANGUAGE: python
CODE:
```
async def extract_structured_data_using_llm(
    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None
):
    print(f"\n--- Extracting Structured Data with {provider} ---")

    if api_token is None and provider != "ollama":
        print(f"API token is required for {provider}. Skipping this example.")
        return

    browser_config = BrowserConfig(headless=True)

    extra_args = {"temperature": 0, "top_p": 0.9, "max_tokens": 2000}
    if extra_headers:
        extra_args["extra_headers"] = extra_headers

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        word_count_threshold=1,
        page_timeout=80000,
        extraction_strategy=LLMExtractionStrategy(
            llm_config=LLMConfig(provider=provider,api_token=api_token),
            schema=OpenAIModelFee.model_json_schema(),
            extraction_type="schema",
            instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content.""",
            extra_args=extra_args,
        ),
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://openai.com/api/pricing/", config=crawler_config
        )
        print(result.extracted_content)
```

----------------------------------------

TITLE: Implementing Rotating Proxies in Crawl4AI
DESCRIPTION: Demonstrates implementing a dynamic proxy rotation system using async functions. Shows how to create new browser contexts with different proxies for each URL crawl.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_next_proxy():
    # Your proxy rotation logic here
    return {"server": "http://next.proxy.com:8080"}

async def main():
    browser_config = BrowserConfig()
    run_config = CrawlerRunConfig()
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # For each URL, create a new run config with different proxy
        for url in urls:
            proxy = await get_next_proxy()
            # Clone the config and update proxy - this creates a new browser context
            current_config = run_config.clone(proxy_config=proxy)
            result = await crawler.arun(url=url, config=current_config)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

----------------------------------------

TITLE: Installing Crawl4AI with all extras using pip
DESCRIPTION: This shell command uses pip, the Python package installer, to install the 'crawl4ai' library. The '[all]' specifier ensures that all optional dependencies (like those for different LLM providers, deep crawling features, etc.) are installed along with the core package, providing access to the full range of features described in the release notes.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/index.md#_snippet_0

LANGUAGE: shell
CODE:
```
pip install \"crawl4ai[all]\"
```

----------------------------------------

TITLE: Combining Interaction with CSS-Based Extraction in Crawl4AI
DESCRIPTION: This example demonstrates how to combine page interaction with structured data extraction using JsonCssExtractionStrategy. It defines a schema to extract commit information from GitHub's commit list after interacting with the page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_120

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

schema = {
    "name": "Commits",
    "baseSelector": "li.Box-sc-g0xbh4-0",
    "fields": [
        {"name": "title", "selector": "h4.markdown-title", "type": "text"}
    ]
}
config = CrawlerRunConfig(
    session_id="ts_commits_session",
    js_code=js_next_page,
    wait_for=wait_for_more,
    extraction_strategy=JsonCssExtractionStrategy(schema)
)
```

----------------------------------------

TITLE: Complete Content Filtering Example in Crawl4AI
DESCRIPTION: This snippet demonstrates a complete example of content filtering with Crawl4AI, combining CSS selection with various filtering options. It targets a specific content area while excluding navigation, footers, external links, and more.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_14

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    config = CrawlerRunConfig(
        css_selector="main.content", 
        word_count_threshold=10,
        excluded_tags=["nav", "footer"],
        exclude_external_links=True,
        exclude_social_media_links=True,
        exclude_domains=["ads.com", "spammytrackers.net"],
        exclude_external_images=True,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        print("Cleaned HTML length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Best-First Crawling Core Logic
DESCRIPTION: Core crawling implementation using priority queue for URL ordering. Processes URLs in batches and yields CrawlResults asynchronously.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_171

LANGUAGE: python
CODE:
```
async def _arun_best_first(
        self,
        start_url: str,
        crawler: AsyncWebCrawler,
        config: CrawlerRunConfig,
    ) -> AsyncGenerator[CrawlResult, None]:
        queue: asyncio.PriorityQueue = asyncio.PriorityQueue()
        await queue.put((0, 0, start_url, None))
        visited: Set[str] = set()
        depths: Dict[str, int] = {start_url: 0}

        while not queue.empty() and not self._cancel_event.is_set():
            if self._pages_crawled >= self.max_pages:
                self.logger.info(f"Max pages limit ({self.max_pages}) reached, stopping crawl")
                break
                
            batch: List[Tuple[float, int, str, Optional[str]]] = []
            for _ in range(BATCH_SIZE):
                if queue.empty():
                    break
                item = await queue.get()
                score, depth, url, parent_url = item
                if url in visited:
                    continue
                visited.add(url)
                batch.append(item)

            if not batch:
                continue

            urls = [item[2] for item in batch]
            batch_config = config.clone(deep_crawl_strategy=None, stream=True)
            stream_gen = await crawler.arun_many(urls=urls, config=batch_config)
            async for result in stream_gen:
                result_url = result.url
                corresponding = next((item for item in batch if item[2] == result_url), None)
                if not corresponding:
                    continue
                score, depth, url, parent_url = corresponding
                result.metadata = result.metadata or {}
                result.metadata["depth"] = depth
                result.metadata["parent_url"] = parent_url
                result.metadata["score"] = score
                
                if result.success:
                    self._pages_crawled += 1
                
                yield result
                
                if result.success:
                    new_links: List[Tuple[str, Optional[str]]] = []
                    await self.link_discovery(result, result_url, depth, visited, new_links, depths)
                    
                    for new_url, new_parent in new_links:
                        new_depth = depths.get(new_url, depth + 1)
                        new_score = self.url_scorer.score(new_url) if self.url_scorer else 0
                        await queue.put((new_score, new_depth, new_url, new_parent))
```

----------------------------------------

TITLE: CrawlerRunConfig Class Definition in Python
DESCRIPTION: Python class definition for CrawlerRunConfig which controls crawl operation parameters. Includes settings for content extraction, caching, JavaScript execution, media capture, and system resource management during crawls.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
class CrawlerRunConfig:
    def __init__(
        word_count_threshold=200,
        extraction_strategy=None,
        markdown_generator=None,
        cache_mode=None,
        js_code=None,
        wait_for=None,
        screenshot=False,
        pdf=False,
        capture_mhtml=False,
        enable_rate_limiting=False,
        rate_limit_config=None,
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=20,
        display_mode=None,
        verbose=True,
        stream=False,  # Enable streaming for arun_many()
        # ... other advanced parameters omitted
    ):
        ...
```

----------------------------------------

TITLE: Extracting HTML Element Attributes using crawl4ai
DESCRIPTION: Shows how to extract attributes like href, src, or data-xxx from HTML elements using crawl4ai's extraction schema. This JSON snippet demonstrates the structure for defining attribute extraction in the schema.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_189

LANGUAGE: json
CODE:
```
{
  "name": "href",
  "type": "attribute",
  "attribute": "href",
  "default": null
}
```

----------------------------------------

TITLE: Combined Filtering Configuration Example
DESCRIPTION: Example showing how to combine multiple filtering options including word count threshold, tag exclusions, and content filtering in a single configuration.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    word_count_threshold=10,
    excluded_tags=["nav", "footer", "header"],
    exclude_external_links=True,
    markdown_generator=DefaultMarkdownGenerator(
        content_filter=PruningContentFilter(threshold=0.5)
    )
)
```

----------------------------------------

TITLE: Combining Multiple Filters (URL, Domain, ContentType) in Crawl4AI using Python
DESCRIPTION: Illustrates creating a sophisticated `FilterChain` by combining multiple filter types: `URLPatternFilter` (for URL patterns like '*guide*' or '*tutorial*'), `DomainFilter` (to specify allowed and blocked domains), and `ContentTypeFilter` (to restrict crawling to specific content types like 'text/html'). This chain is then passed to the `BFSDeepCrawlStrategy` within `CrawlerRunConfig` for fine-grained control over the deep crawl process. Depends on `FilterChain`, `URLPatternFilter`, `DomainFilter`, `ContentTypeFilter` from `crawl4ai.deep_crawling.filters`, `CrawlerRunConfig`, and `BFSDeepCrawlStrategy`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_7

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling.filters import (
    FilterChain,
    URLPatternFilter,
    DomainFilter,
    ContentTypeFilter
)

# Create a chain of filters
filter_chain = FilterChain([
    # Only follow URLs with specific patterns
    URLPatternFilter(patterns=["*guide*", "*tutorial*"]),
    
    # Only crawl specific domains
    DomainFilter(
        allowed_domains=["docs.example.com"],
        blocked_domains=["old.docs.example.com"]
    ),
    
    # Only include specific content types
    ContentTypeFilter(allowed_types=["text/html"])
])

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=2,
        filter_chain=filter_chain
    )
)
```

----------------------------------------

TITLE: Initial Login and Storage State Export
DESCRIPTION: First-run implementation that performs login and exports the storage state for future use
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def on_browser_created_hook(browser):
    context = browser.contexts[0]
    page = await context.new_page()
    
    await page.goto("https://example.com/login", wait_until="domcontentloaded")
    
    await page.fill("input[name='username']", "myuser")
    await page.fill("input[name='password']", "mypassword")
    await page.click("button[type='submit']")
    await page.wait_for_load_state("networkidle")
    
    await context.storage_state(path="my_storage_state.json")
    await page.close()

async def main():
    async with AsyncWebCrawler(
        headless=True,
        verbose=True,
        hooks={"on_browser_created": on_browser_created_hook},
        use_persistent_context=True,
        user_data_dir="./my_user_data"
    ) as crawler:
        
        result = await crawler.arun(
            url='https://example.com/protected-page',
            cache_mode=CacheMode.BYPASS,
            markdown_generator=DefaultMarkdownGenerator(options={"ignore_links": True}),
        )
        print("First run result success:", result.success)
        if result.success:
            print("Protected page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: New CacheMode-based Caching in Crawl4AI (Python)
DESCRIPTION: This code snippet showcases the new recommended way of controlling caching in Crawl4AI using the CacheMode enum. It uses CrawlerRunConfig to set the cache mode to BYPASS, which is equivalent to the old 'bypass_cache=True' flag.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cache-modes.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.async_configs import CrawlerRunConfig

async def use_proxy():
    # Use CacheMode in CrawlerRunConfig
    config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)  
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            config=config  # Pass the configuration object
        )
        print(len(result.markdown))

async def main():
    await use_proxy()

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Accessing and Processing Links from CrawlResult in Python
DESCRIPTION: Shows how to access the extracted internal links from a CrawlResult object. The links field contains dictionaries of internal and external links, each with href, text, and other attributes.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
print(result.links["internal"][:3])  # Show first 3 internal links
```

----------------------------------------

TITLE: Handling Crawler Extraction Errors (Python)
DESCRIPTION: Provides a basic error handling pattern for crawler extraction tasks. It wraps the `crawler.arun` call in a `try...except` block to catch potential exceptions during the process. If successful, it checks the result's success status and loads the extracted content; otherwise, it prints an error message.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_16

LANGUAGE: python
CODE:
```
try:
    result = await crawler.arun(
        url="https://example.com",
        extraction_strategy=strategy
    )
    if result.success:
        content = json.loads(result.extracted_content)
except Exception as e:
    print(f"Extraction failed: {e}")
```

----------------------------------------

TITLE: API Request Body: JSON CSS Extraction Strategy Config (JSON)
DESCRIPTION: Example JSON payload snippet demonstrating how to configure a `JsonCssExtractionStrategy` within the `crawler_config` for direct API calls. It shows the required `{"type": "ClassName", "params": {...}}` structure for configuration objects and how to embed a dictionary (`schema`) using `{"type": "dict", "value": {...}}`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_28

LANGUAGE: json
CODE:
```
{
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "extraction_strategy": {
                "type": "JsonCssExtractionStrategy",
                "params": {
                    "schema": {
                        "type": "dict",
                        "value": {
                           "baseSelector": "article.post",
                           "fields": [
                               {"name": "title", "selector": "h1", "type": "text"},
                               {"name": "content", "selector": ".content", "type": "html"}
                           ]
                         }
                    }
                }
            }
        }
    }
}
```

----------------------------------------

TITLE: Implementing Proxy Rotation with AsyncWebCrawler in Python
DESCRIPTION: An async function that demonstrates how to configure and use proxy rotation with AsyncWebCrawler. It sets up a RoundRobinProxyStrategy with a list of proxy servers and configures the crawler to use this strategy. This example uses placeholder proxies that would need to be replaced with real ones.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_157

LANGUAGE: python
CODE:
```
async def demo_proxy_rotation():
    """Proxy rotation for multiple requests"""
    print("\n=== 10. Proxy Rotation ===")

    # Example proxies (replace with real ones)
    proxies = [
        ProxyConfig(server="http://proxy1.example.com:8080"),
        ProxyConfig(server="http://proxy2.example.com:8080"),
    ]

    proxy_strategy = RoundRobinProxyStrategy(proxies)

    print(f"Using {len(proxies)} proxies in rotation")
    print(
        "Note: This example uses placeholder proxies - replace with real ones to test"
    )

    async with AsyncWebCrawler() as crawler:
        config = CrawlerRunConfig(
            proxy_rotation_strategy=proxy_strategy
        )

        # In a real scenario, these would be run and the proxies would rotate
        print("In a real scenario, requests would rotate through the available proxies")
```

----------------------------------------

TITLE: Implementing Filters and Scorers in Python with AsyncWebCrawler
DESCRIPTION: This function demonstrates the use of filters and scorers for targeted crawling. It shows examples of using a single URL pattern filter, multiple filters in a chain, and a keyword relevance scorer for prioritizing pages.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_208

LANGUAGE: python
CODE:
```
async def filters_and_scorers():
    """
    PART 3: Demonstrates the use of filters and scorers for more targeted crawling.

    This function progressively adds:
    1. A single URL pattern filter
    2. Multiple filters in a chain
    3. Scorers for prioritizing pages
    """
    print("\n===== FILTERS AND SCORERS =====")

    async with AsyncWebCrawler() as crawler:
        # SINGLE FILTER EXAMPLE
        print("\n EXAMPLE 1: SINGLE URL PATTERN FILTER")
        print("  Only crawl pages containing 'core' in the URL")

        # Create a filter that only allows URLs with 'guide' in them
        url_filter = URLPatternFilter(patterns=["*core*"])

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1,
                include_external=False,
                filter_chain=FilterChain([url_filter]),  # Single filter
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            cache_mode=CacheMode.BYPASS,
            verbose=True,
        )

        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        print(f"   Crawled {len(results)} pages matching '*core*'")
        for result in results[:3]:  # Show first 3 results
            print(f"   {result.url}")
        if len(results) > 3:
            print(f"  ... and {len(results) - 3} more")

        # MULTIPLE FILTERS EXAMPLE
        print("\n EXAMPLE 2: MULTIPLE FILTERS IN A CHAIN")
        print("  Only crawl pages that:")
        print("  1. Contain '2024' in the URL")
        print("  2. Are from 'techcrunch.com'")
        print("  3. Are of text/html or application/javascript content type")

        # Create a chain of filters
        filter_chain = FilterChain(
            [
                URLPatternFilter(patterns=["*2024*"]),
                DomainFilter(
                    allowed_domains=["techcrunch.com"],
                    blocked_domains=["guce.techcrunch.com", "oidc.techcrunch.com"],
                ),
                ContentTypeFilter(
                    allowed_types=["text/html", "application/javascript"]
                ),
            ]
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1, include_external=False, filter_chain=filter_chain
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
        )

        results = await crawler.arun(url="https://techcrunch.com", config=config)

        print(f"   Crawled {len(results)} pages after applying all filters")
        for result in results[:3]:
            print(f"   {result.url}")
        if len(results) > 3:
            print(f"  ... and {len(results) - 3} more")

        # SCORERS EXAMPLE
        print("\n EXAMPLE 3: USING A KEYWORD RELEVANCE SCORER")
        print(
            "Score pages based on relevance to keywords: 'crawl', 'example', 'async', 'configuration','javascript','css'"
        )

        # Create a keyword relevance scorer
        keyword_scorer = KeywordRelevanceScorer(
            keywords=["crawl", "example", "async", "configuration","javascript","css"], weight=1
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BestFirstCrawlingStrategy(  
                max_depth=1, include_external=False, url_scorer=keyword_scorer
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            cache_mode=CacheMode.BYPASS,
            verbose=True,
            stream=True,
        )

        results = []
        async for result in await crawler.arun(
            url="https://docs.crawl4ai.com", config=config
        ):
            results.append(result)
            score = result.metadata.get("score")
            print(f"   Score: {score:.2f} | {result.url}")

        print(f"   Crawler prioritized {len(results)} pages by relevance score")
        print("   Note: BestFirstCrawlingStrategy visits highest-scoring pages first")
```

----------------------------------------

TITLE: Crawler Configuration in YAML for Crawl4AI
DESCRIPTION: Defines crawler behavior settings including cache mode, page timeout, content handling, and performance optimizations.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_4

LANGUAGE: yaml
CODE:
```
# crawler.yml
cache_mode: "bypass"
wait_until: "networkidle"
page_timeout: 30000
delay_before_return_html: 0.5
word_count_threshold: 100
scan_full_page: true
scroll_delay: 0.3
process_iframes: false
remove_overlay_elements: true
magic: true
verbose: true
```

----------------------------------------

TITLE: Basic Installation of Crawl4AI
DESCRIPTION: Command to install the core Crawl4AI library with essential dependencies using pip.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_78

LANGUAGE: bash
CODE:
```
pip install crawl4ai
```

----------------------------------------

TITLE: Content Cleaning and Filtering with Crawl4AI
DESCRIPTION: This function showcases content cleaning and filtering capabilities of Crawl4AI. It configures the crawler to exclude certain HTML tags, remove overlay elements, and apply content filtering. It then crawls a Wikipedia page and compares the lengths of full and filtered markdown content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_128

LANGUAGE: python
CODE:
```
async def clean_content():
    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        excluded_tags=["nav", "footer", "aside"],
        remove_overlay_elements=True,
        markdown_generator=DefaultMarkdownGenerator(
            content_filter=PruningContentFilter(
                threshold=0.48, threshold_type="fixed", min_word_threshold=0
            ),
            options={"ignore_links": True},
        ),
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://en.wikipedia.org/wiki/Apple",
            config=crawler_config,
        )
        full_markdown_length = len(result.markdown.raw_markdown)
        fit_markdown_length = len(result.markdown.fit_markdown)
        print(f"Full Markdown Length: {full_markdown_length}")
        print(f"Fit Markdown Length: {fit_markdown_length}")
```

----------------------------------------

TITLE: Configuring PruningContentFilter for General Content Cleaning
DESCRIPTION: This snippet shows how to initialize and configure a PruningContentFilter for removing junk content without requiring a specific query. It sets up the filter with threshold settings and minimum word count requirements.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_107

LANGUAGE: python
CODE:
```
from crawl4ai.content_filter_strategy import PruningContentFilter

prune_filter = PruningContentFilter(
    threshold=0.5,
    threshold_type="fixed",  # or "dynamic"
    min_word_threshold=50
)
```

----------------------------------------

TITLE: User Simulation for Anti-Bot Evasion in Python Web Crawling
DESCRIPTION: This function demonstrates how to use user simulation techniques to evade anti-bot measures. It configures the crawler with random user agents, simulates user behavior, and overrides navigator properties to mimic real user interactions.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_143

LANGUAGE: python
CODE:
```
async def crawl_with_user_simulation():
    browser_config = BrowserConfig(
        headless=True,
        user_agent_mode="random",
        user_agent_generator_config={"device_type": "mobile", "os_type": "android"},
    )

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        magic=True,
        simulate_user=True,
        override_navigator=True,
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url="YOUR-URL-HERE", config=crawler_config)
        print(result.markdown)
```

----------------------------------------

TITLE: Storage State JSON Structure Example
DESCRIPTION: Example structure showing how to format the storage_state JSON with cookies and localStorage data
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_0

LANGUAGE: json
CODE:
```
{
  "cookies": [
    {
      "name": "session",
      "value": "abcd1234",
      "domain": "example.com",
      "path": "/",
      "expires": 1675363572.037711,
      "httpOnly": false,
      "secure": false,
      "sameSite": "None"
    }
  ],
  "origins": [
    {
      "origin": "https://example.com",
      "localStorage": [
        { "name": "token", "value": "my_auth_token" },
        { "name": "refreshToken", "value": "my_refresh_token" }
      ]
    }
  ]
}
```

----------------------------------------

TITLE: Setting Up LLM Environment Variables for Crawl4AI
DESCRIPTION: Example of an environment file for storing API keys for various Language Model providers that can be used with Crawl4AI.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_46

LANGUAGE: env
CODE:
```
# OpenAI
OPENAI_API_KEY=sk-your-key

# Anthropic
ANTHROPIC_API_KEY=your-anthropic-key

# DeepSeek
DEEPSEEK_API_KEY=your-deepseek-key

# Check out https://docs.litellm.ai/docs/providers for more providers!
```

----------------------------------------

TITLE: Using Managed Browsers in Crawl4AI (Python)
DESCRIPTION: Complete Python script showcasing how to use managed browsers in Crawl4AI, including setting up the browser configuration and running a crawl with a persistent profile.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # 1) Reference your persistent data directory
    browser_config = BrowserConfig(
        headless=True,             # 'True' for automated runs
        verbose=True,
        use_managed_browser=True,  # Enables persistent browser strategy
        browser_type="chromium",
        user_data_dir="/path/to/my-chrome-profile"
    )

    # 2) Standard crawl config
    crawl_config = CrawlerRunConfig(
        wait_for="css:.logged-in-content"
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url="https://example.com/private", config=crawl_config)
        if result.success:
            print("Successfully accessed private data with your identity!")
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Installing Crawl4AI with PyTorch features
DESCRIPTION: Installs Crawl4AI with PyTorch-based features such as cosine similarity or advanced semantic chunking, followed by running the setup command.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/installation.md#2025-04-23_snippet_4

LANGUAGE: bash
CODE:
```
pip install crawl4ai[torch]
crawl4ai-setup
```

----------------------------------------

TITLE: Configuring API Keys for AI Services in Python
DESCRIPTION: Sets up API key constants for accessing different AI services including Groq, OpenAI, and Anthropic. The keys are stored as string variables that would be replaced with actual API keys when deploying the application. The comment indicates that additional API keys can be added to this configuration file.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/.env.txt#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
GROQ_API_KEY = "YOUR_GROQ_API"
OPENAI_API_KEY = "YOUR_OPENAI_API"
ANTHROPIC_API_KEY = "YOUR_ANTHROPIC_API"
# You can add more API keys here
```

----------------------------------------

TITLE: Configuring Proxy Settings with AsyncWebCrawler
DESCRIPTION: This code demonstrates how to configure and use proxy settings with AsyncWebCrawler. It includes setting up a proxy server with authentication credentials and testing the connection by visiting a website that displays IP information.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_130

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    browser_cfg = BrowserConfig(
        proxy_config={
            "server": "http://proxy.example.com:8080",
            "username": "myuser",
            "password": "mypass",
        },
        headless=True
    )
    crawler_cfg = CrawlerRunConfig(
        verbose=True
    )

    async with AsyncWebCrawler(config=browser_cfg) as crawler:
        result = await crawler.arun(
            url="https://www.whatismyip.com/",
            config=crawler_cfg
        )
        if result.success:
            print("[OK] Page fetched via proxy.")
            print("Page HTML snippet:", result.html[:200])
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Defining Data Schema for LLM Extraction with Pydantic
DESCRIPTION: This code defines a Pydantic model for structured data extraction of OpenAI model fees. It specifies fields for model name, input token fee, and output token fee with appropriate descriptions for the LLM to understand what to extract.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_136

LANGUAGE: python
CODE:
```
class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(
        ..., description="Fee for output token for the OpenAI model."
    )
```

----------------------------------------

TITLE: Extract Crypto Prices using XPath and raw:// scheme
DESCRIPTION: This Python snippet demonstrates using `crawl4ai` to extract data from a dummy HTML string using XPath selectors. It utilizes the `JsonXPathExtractionStrategy` and the `raw://` scheme to avoid a network request, passing the HTML content directly to the crawler. The extraction configuration, including the XPath schema, is defined within the `CrawlerRunConfig`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_1

LANGUAGE: python
CODE:
```
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonXPathExtractionStrategy

async def extract_crypto_prices_xpath():
    # 1. Minimal dummy HTML with some repeating rows
    dummy_html = """
    <html>
      <body>
        <div class='crypto-row'>
          <h2 class='coin-name'>Bitcoin</h2>
          <span class='coin-price'>$28,000</span>
        </div>
        <div class='crypto-row'>
          <h2 class='coin-name'>Ethereum</h2>
          <span class='coin-price'>$1,800</span>
        </div>
      </body>
    </html>
    """

    # 2. Define the JSON schema (XPath version)
    schema = {
        "name": "Crypto Prices via XPath",
        "baseSelector": "//div[@class='crypto-row']",
        "fields": [
            {
                "name": "coin_name",
                "selector": ".//h2[@class='coin-name']",
                "type": "text"
            },
            {
                "name": "price",
                "selector": ".//span[@class='coin-price']",
                "type": "text"
            }
        ]
    }

    # 3. Place the strategy in the CrawlerRunConfig
    config = CrawlerRunConfig(
        extraction_strategy=JsonXPathExtractionStrategy(schema, verbose=True)
    )

    # 4. Use raw:// scheme to pass dummy_html directly
    raw_url = f"raw://{dummy_html}"

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url=raw_url,
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return

        data = json.loads(result.extracted_content)
        print(f"Extracted {len(data)} coin rows")
        if data:
            print("First item:", data[0])

asyncio.run(extract_crypto_prices_xpath())
```

----------------------------------------

TITLE: Configuring BFSDeepCrawlStrategy for Breadth-First Crawling in Python
DESCRIPTION: Illustrates the initialization of `BFSDeepCrawlStrategy` for breadth-first web crawling. It shows key configuration parameters: `max_depth` (limits crawl levels), `include_external` (controls following off-domain links), `max_pages` (optional limit on total pages), and `score_threshold` (optional minimum score for a URL to be crawled). This strategy explores all links at the current depth before moving to the next level. Depends on `BFSDeepCrawlStrategy` from `crawl4ai.deep_crawling`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_1

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling import BFSDeepCrawlStrategy

# Basic configuration
strategy = BFSDeepCrawlStrategy(
    max_depth=2,               # Crawl initial page + 2 levels deep
    include_external=False,    # Stay within the same domain
    max_pages=50,              # Maximum number of pages to crawl (optional)
    score_threshold=0.3,       # Minimum score for URLs to be crawled (optional)
)
```

----------------------------------------

TITLE: Docker Deployment Commands for Crawl4AI
DESCRIPTION: These bash commands demonstrate how to build and run a Docker container for Crawl4AI. The Docker image includes a FastAPI server with both streaming and non-streaming endpoints for crawling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_3

LANGUAGE: bash
CODE:
```
# Build the image (from the project root)
docker build -t crawl4ai .

# Run the container
docker run -d -p 8000:8000 --name crawl4ai crawl4ai
```

----------------------------------------

TITLE: Using a Custom Browser Profile with Crawl4AI in Python
DESCRIPTION: This Python script demonstrates configuring `AsyncWebCrawler` to use a persistent browser user data directory. It creates a directory if it doesn't exist, sets up `BrowserConfig` to use this directory (`user_data_dir`) and enable persistent context (`use_persistent_context=True`). This allows maintaining browser state (like cookies or sessions) across runs, useful for sites requiring logins or having anti-bot measures. The script then crawls a specified URL with `magic=True` and prints the content length.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_19

LANGUAGE: python
CODE:
```
import os, sys
from pathlib import Path
import asyncio, time
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def test_news_crawl():
    # Create a persistent user data directory
    user_data_dir = os.path.join(Path.home(), ".crawl4ai", "browser_profile")
    os.makedirs(user_data_dir, exist_ok=True)

    browser_config = BrowserConfig(
        verbose=True,
        headless=True,
        user_data_dir=user_data_dir,
        use_persistent_context=True,
    )
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        url = "ADDRESS_OF_A_CHALLENGING_WEBSITE"
        
        result = await crawler.arun(
            url,
            config=run_config,
            magic=True,
        )
        
        print(f"Successfully crawled {url}")
        print(f"Content length: {len(result.markdown)}")
```

----------------------------------------

TITLE: BrowserConfig Class Definition in Python
DESCRIPTION: Python class definition for BrowserConfig which controls browser launch parameters and behavior. Includes parameters for browser type, headless mode, proxy configuration, viewport dimensions, and other browser behavior settings.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
class BrowserConfig:
    def __init__(
        browser_type="chromium",
        headless=True,
        proxy_config=None,
        viewport_width=1080,
        viewport_height=600,
        verbose=True,
        use_persistent_context=False,
        user_data_dir=None,
        cookies=None,
        headers=None,
        user_agent=None,
        text_mode=False,
        light_mode=False,
        extra_args=None,
        # ... other advanced parameters omitted here
    ):
        ...
```

----------------------------------------

TITLE: Initializing BrowserConfig in Python for Crawl4AI
DESCRIPTION: This snippet shows the structure of the BrowserConfig class, which controls how the browser is launched and behaves in Crawl4AI. It includes parameters for browser type, headless mode, proxy settings, viewport dimensions, and various other browser-related options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
class BrowserConfig:
    def __init__(
        browser_type="chromium",
        headless=True,
        proxy_config=None,
        viewport_width=1080,
        viewport_height=600,
        verbose=True,
        use_persistent_context=False,
        user_data_dir=None,
        cookies=None,
        headers=None,
        user_agent=None,
        text_mode=False,
        light_mode=False,
        extra_args=None,
        # ... other advanced parameters omitted here
    ):
        ...
```

----------------------------------------

TITLE: Implementing Best-First Crawling Strategy with Keyword Scoring in Crawl4AI
DESCRIPTION: This snippet shows how to use the BestFirstCrawlingStrategy with a KeywordRelevanceScorer to prioritize crawling pages that are most relevant to specified keywords. This intelligent approach focuses crawl resources on the most valuable content first.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_34

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer

# Create a scorer
scorer = KeywordRelevanceScorer(
    keywords=["crawl", "example", "async", "configuration"],
    weight=0.7
)

# Configure the strategy
strategy = BestFirstCrawlingStrategy(
    max_depth=2,
    include_external=False,
    url_scorer=scorer,
    max_pages=25,              # Maximum number of pages to crawl (optional)
)
```

----------------------------------------

TITLE: Configuring LLM Provider with LlmConfig in Python
DESCRIPTION: Sets up an LLM configuration using OpenAI's GPT-4o-mini model with an API key from environment variables. This is the first step in creating an LLM-based extraction strategy.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/llm-strategies.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
llmConfig = LlmConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"))
```

----------------------------------------

TITLE: LLM-based Data Extraction
DESCRIPTION: Advanced example of using LLM-based extraction with both open-source and closed-source providers for complex data structures.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_126

LANGUAGE: python
CODE:
```
import os
import json
import asyncio
from pydantic import BaseModel, Field
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(..., description="Fee for output token for the OpenAI model.")

async def extract_structured_data_using_llm(
    provider: str, api_token: str = None, extra_headers: Dict[str, str] = None
):
    print(f"\n--- Extracting Structured Data with {provider} ---")

    if api_token is None and provider != "ollama":
        print(f"API token is required for {provider}. Skipping this example.")
        return

    browser_config = BrowserConfig(headless=True)

    extra_args = {"temperature": 0, "top_p": 0.9, "max_tokens": 2000}
    if extra_headers:
        extra_args["extra_headers"] = extra_headers

    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        word_count_threshold=1,
        page_timeout=80000,
        extraction_strategy=LLMExtractionStrategy(
            llm_config = LLMConfig(provider=provider,api_token=api_token),
            schema=OpenAIModelFee.model_json_schema(),
            extraction_type="schema",
            instruction="""From the crawled content, extract all mentioned model names along with their fees for input and output tokens. 
            Do not miss any models in the entire content.""",
            extra_args=extra_args,
        ),
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://openai.com/api/pricing/", config=crawler_config
        )
        print(result.extracted_content)

if __name__ == "__main__":

    asyncio.run(
        extract_structured_data_using_llm(
            provider="openai/gpt-4o", api_token=os.getenv("OPENAI_API_KEY")
        )
    )
```

----------------------------------------

TITLE: Implementing Robots.txt Compliant Crawler in Python
DESCRIPTION: Demonstrates how to implement a crawler that respects robots.txt rules with error handling and status reporting.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    urls = [
        "https://example1.com",
        "https://example2.com",
        "https://example3.com"
    ]
    
    config = CrawlerRunConfig(
        cache_mode=CacheMode.ENABLED,
        check_robots_txt=True,  # Will respect robots.txt for each URL
        semaphore_count=3      # Max concurrent requests
    )
    
    async with AsyncWebCrawler() as crawler:
        async for result in crawler.arun_many(urls, config=config):
            if result.success:
                print(f"Successfully crawled {result.url}")
            elif result.status_code == 403 and "robots.txt" in result.error_message:
                print(f"Skipped {result.url} - blocked by robots.txt")
            else:
                print(f"Failed to crawl {result.url}: {result.error_message}")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Enabling Image Lazy Loading Support in Crawl4AI
DESCRIPTION: This code demonstrates how to enable the wait_for_images feature that ensures all lazy-loaded images are fully loaded before proceeding with the crawl. This improves content capture for modern websites that use lazy-loading techniques.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
await crawler.crawl(
    url="https://example.com",
    wait_for_images=True  # Add this argument to ensure images are fully loaded
)
```

----------------------------------------

TITLE: Implementing HTML Element Fallback and Extraction Methods
DESCRIPTION: This snippet contains methods for handling selector caching, element retrieval, and extraction of text, HTML, and attributes from elements. It includes a fallback mechanism for invalid selectors.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_80

LANGUAGE: python
CODE:
```
self._selector_cache[selector_str] = select_func
            except Exception as e:
                if self.verbose:
                    print(f"Error compiling selector '{selector_str}': {e}")
                
                # Fallback function for invalid selectors
                def fallback_func(element):
                    return []
                
                self._selector_cache[selector_str] = fallback_func
                
        return self._selector_cache[selector_str]
    
    def _get_base_elements(self, parsed_html, selector: str):
        selector_func = self._get_selector(selector)
        return selector_func(parsed_html)
    
    def _get_elements(self, element, selector: str):
        selector_func = self._get_selector(selector)
        return selector_func(element)
    
    def _get_element_text(self, element) -> str:
        return "".join(element.xpath(".//text()")).strip()
    
    def _get_element_html(self, element) -> str:
        from lxml import etree
        return etree.tostring(element, encoding='unicode')
    
    def _get_element_attribute(self, element, attribute: str):
        return element.get(attribute)
```

----------------------------------------

TITLE: Reusing Saved Storage State
DESCRIPTION: Second-run implementation that reuses previously saved storage state to skip login
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    async with AsyncWebCrawler(
        headless=True,
        verbose=True,
        use_persistent_context=True,
        user_data_dir="./my_user_data",
        storage_state="my_storage_state.json"  # Reuse previously exported state
    ) as crawler:
        
        result = await crawler.arun(
            url='https://example.com/protected-page',
            cache_mode=CacheMode.BYPASS,
            markdown_generator=DefaultMarkdownGenerator(options={"ignore_links": True}),
        )
        print("Second run result success:", result.success)
        if result.success:
            print("Protected page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Integrated JavaScript Execution and Waiting for Dynamic Content in Python
DESCRIPTION: Combines JavaScript execution and waiting logic into a single approach for handling dynamic content. This technique uses an IIFE with asynchronous JavaScript to click the pagination button and wait for new content to load before continuing.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
async def integrated_js_and_wait_crawl():
    async with AsyncWebCrawler() as crawler:
        session_id = "integrated_session"
        url = "https://github.com/example/repo/commits/main"

        js_next_page_and_wait = """
        (async () => {
            const getCurrentCommit = () => document.querySelector('li.commit-item h4').textContent.trim();
            const initialCommit = getCurrentCommit();
            document.querySelector('a.pagination-next').click();
            while (getCurrentCommit() === initialCommit) {
                await new Promise(resolve => setTimeout(resolve, 100));
            }
        })();
        """

        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                js_code=js_next_page_and_wait if page > 0 else None,
                css_selector="li.commit-item",
                js_only=page > 0,
                cache_mode=CacheMode.BYPASS
            )

            result = await crawler.arun(config=config)
            print(f"Page {page + 1}: Found {len(result.extracted_content)} commits")

        await crawler.crawler_strategy.kill_session(session_id)

asyncio.run(integrated_js_and_wait_crawl())
```

----------------------------------------

TITLE: Pulling Crawl4AI Docker Images from Docker Hub (Bash)
DESCRIPTION: Demonstrates how to pull specific release candidate or the latest stable version of the Crawl4AI Docker image from Docker Hub using the `docker pull` command. The images are multi-architecture.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_0

LANGUAGE: bash
CODE:
```
# Pull the release candidate (recommended for latest features)
docker pull unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number

# Or pull the latest stable version
docker pull unclecode/crawl4ai:latest
```

----------------------------------------

TITLE: CSS-based Data Extraction
DESCRIPTION: Implementation of CSS-based data extraction using JsonCssExtractionStrategy with a defined schema structure.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_125

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    schema = {
        "name": "Example Items",
        "baseSelector": "div.item",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link", "selector": "a", "type": "attribute", "attribute": "href"}
        ]
    }

    raw_html = "<div class='item'><h2>Item 1</h2><a href='https://example.com/item1'>Link 1</a></div>"

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="raw://" + raw_html,
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                extraction_strategy=JsonCssExtractionStrategy(schema)
            )
        )
        # The JSON output is stored in 'extracted_content'
        data = json.loads(result.extracted_content)
        print(data)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Excluding All Images with CrawlerRunConfig (Python)
DESCRIPTION: This snippet shows how to configure `CrawlerRunConfig` to completely exclude all images during the crawl process. Setting `exclude_all_images=True` removes images early in the pipeline, which significantly improves performance and reduces memory usage, especially for image-heavy pages or when only text content is required.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_1

LANGUAGE: python
CODE:
```
crawler_cfg = CrawlerRunConfig(
    exclude_all_images=True
)
```

----------------------------------------

TITLE: Initializing Basic Crawl4AI Configuration
DESCRIPTION: Basic setup of Crawl4AI crawler with browser and run configurations. Shows how to initialize and run a basic crawl with cache bypass settings.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_122

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode

async def main():
    browser_conf = BrowserConfig(headless=True)  # or False to see the browser
    run_conf = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler(config=browser_conf) as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=run_conf
        )
        print(result.markdown)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: API Request Body: HTML Extraction (JSON)
DESCRIPTION: Example JSON payload structure required for the POST request to the /html endpoint. It takes a single mandatory parameter, 'url', specifying the web page to crawl.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_19

LANGUAGE: json
CODE:
```
{
  "url": "https://example.com"
}
```

----------------------------------------

TITLE: Implementing Full-Page Screenshots and PDF Capture with Crawl4AI in Python
DESCRIPTION: This code demonstrates how to use Crawl4AI to capture both PDF and screenshot of a large webpage. It shows the complete process of initializing the crawler, configuring the capture request with cache bypass, and saving both the PDF and screenshot to disk.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/full_page_screenshot_and_pdf_export.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import os
import sys
import asyncio
from crawl4ai import AsyncWebCrawler, CacheMode, CrawlerRunConfig

# Adjust paths as needed
parent_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.append(parent_dir)
__location__ = os.path.realpath(os.path.join(os.getcwd(), os.path.dirname(__file__)))

async def main():
    async with AsyncWebCrawler() as crawler:
        # Request both PDF and screenshot
        result = await crawler.arun(
            url='https://en.wikipedia.org/wiki/List_of_common_misconceptions',
            config=CrawlerRunConfig(
                cache_mode=CacheMode.BYPASS,
                pdf=True,
                screenshot=True
            )
        )
        
        if result.success:
            # Save screenshot
            if result.screenshot:
                from base64 import b64decode
                with open(os.path.join(__location__, "screenshot.png"), "wb") as f:
                    f.write(b64decode(result.screenshot))
            
            # Save PDF
            if result.pdf:
                with open(os.path.join(__location__, "page.pdf"), "wb") as f:
                    f.write(result.pdf)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Implementing Sentence-Based Text Chunking using NLTK
DESCRIPTION: Uses NLTK's sentence tokenizer to split text into individual sentences. Provides clean, meaningful statement-level segmentation.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from nltk.tokenize import sent_tokenize

class NlpSentenceChunking:
    def chunk(self, text):
        sentences = sent_tokenize(text)
        return [sentence.strip() for sentence in sentences]

# Example Usage
text = "This is sentence one. This is sentence two."
chunker = NlpSentenceChunking()
print(chunker.chunk(text))
```

----------------------------------------

TITLE: Implementing Max Pages and Score Thresholds in Python with Crawl4AI
DESCRIPTION: This function demonstrates how to use max_pages and score_threshold parameters with different crawling strategies (BFS, DFS, and Best-First) in Crawl4AI. It shows how to limit the number of pages crawled and set score thresholds for more targeted crawling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_210

LANGUAGE: python
CODE:
```
async def max_pages_and_thresholds():
    """
    PART 5: Demonstrates using max_pages and score_threshold parameters with different strategies.
    
    This function shows:
    - How to limit the number of pages crawled
    - How to set score thresholds for more targeted crawling
    - Comparing BFS, DFS, and Best-First strategies with these parameters
    """
    print("\n===== MAX PAGES AND SCORE THRESHOLDS =====")
    
    from crawl4ai.deep_crawling import DFSDeepCrawlStrategy
    
    async with AsyncWebCrawler() as crawler:
        # Define a common keyword scorer for all examples
        keyword_scorer = KeywordRelevanceScorer(
            keywords=["browser", "crawler", "web", "automation"], 
            weight=1.0
        )
        
        # EXAMPLE 1: BFS WITH MAX PAGES
        print("\n EXAMPLE 1: BFS STRATEGY WITH MAX PAGES LIMIT")
        print("  Limit the crawler to a maximum of 5 pages")
        
        bfs_config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=2, 
                include_external=False,
                url_scorer=keyword_scorer,
                max_pages=5  # Only crawl 5 pages
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )
        
        results = await crawler.arun(url="https://docs.crawl4ai.com", config=bfs_config)
        
        print(f"   Crawled exactly {len(results)} pages as specified by max_pages")
        for result in results:
            depth = result.metadata.get("depth", 0)
            print(f"   Depth: {depth} | {result.url}")
            
        # EXAMPLE 2: DFS WITH SCORE THRESHOLD
        print("\n EXAMPLE 2: DFS STRATEGY WITH SCORE THRESHOLD")
        print("  Only crawl pages with a relevance score above 0.5")
        
        dfs_config = CrawlerRunConfig(
            deep_crawl_strategy=DFSDeepCrawlStrategy(
                max_depth=2,
                include_external=False, 
                url_scorer=keyword_scorer,
                score_threshold=0.7,  # Only process URLs with scores above 0.5
                max_pages=10
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )
        
        results = await crawler.arun(url="https://docs.crawl4ai.com", config=dfs_config)
        
        print(f"   Crawled {len(results)} pages with scores above threshold")
        for result in results:
            score = result.metadata.get("score", 0)
            depth = result.metadata.get("depth", 0)
            print(f"   Depth: {depth} | Score: {score:.2f} | {result.url}")
            
        # EXAMPLE 3: BEST-FIRST WITH BOTH CONSTRAINTS
        print("\n EXAMPLE 3: BEST-FIRST STRATEGY WITH BOTH CONSTRAINTS")
        print("  Limit to 7 pages with scores above 0.3, prioritizing highest scores")
        
        bf_config = CrawlerRunConfig(
            deep_crawl_strategy=BestFirstCrawlingStrategy(
                max_depth=2,
                include_external=False,
                url_scorer=keyword_scorer,
                max_pages=7,          # Limit to 7 pages total
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
            stream=True,
        )
        
        results = []
        async for result in await crawler.arun(url="https://docs.crawl4ai.com", config=bf_config):
            results.append(result)
            score = result.metadata.get("score", 0)
            depth = result.metadata.get("depth", 0)
            print(f"   Depth: {depth} | Score: {score:.2f} | {result.url}")
            
        print(f"   Crawled {len(results)} high-value pages with scores above 0.3")
        if results:
            avg_score = sum(r.metadata.get('score', 0) for r in results) / len(results)
            print(f"   Average score: {avg_score:.2f}")
            print("   Note: BestFirstCrawlingStrategy visited highest-scoring pages first")
```

----------------------------------------

TITLE: Creating a Custom Crawler4ai Pipeline
DESCRIPTION: Advanced example showing how to create a custom processing pipeline with Crawler4ai that loads pages, processes them, and exports the results in a specific format.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
from crawler4ai import Crawler4ai, Pipeline, processors

# Create a pipeline for processing
pipeline = Pipeline([
    processors.HtmlCleaner(),
    processors.TextExtractor(),
    processors.LanguageDetector(),
    processors.Summarizer(max_length=200),
    processors.JsonExporter(output_file="summaries.json")
])

# Initialize crawler with the pipeline
crawler = Crawler4ai(
    urls=["https://example.com/blog"],
    max_pages=25,
    pipeline=pipeline
)

# Run the crawl and processing
crawler.crawl()
```

----------------------------------------

TITLE: Implementing LLM-based Extraction for OpenAI Pricing Data
DESCRIPTION: Demonstrates how to use language model-based extraction strategy to retrieve structured pricing data from OpenAI's website. Uses Pydantic for data modeling and supports multiple LLM providers with configurable API tokens and headers.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from pydantic import BaseModel, Field
import os, json

class OpenAIModelFee(BaseModel):
    model_name: str = Field(..., description="Name of the OpenAI model.")
    input_fee: str = Field(..., description="Fee for input token for the OpenAI model.")
    output_fee: str = Field(
        ..., description="Fee for output token for the OpenAI model."
    )

async def extract_structured_data_using_llm(provider: str, api_token: str = None, extra_headers: dict = None):
    print(f"\n--- Extracting Structured Data with {provider} ---")
    
    # Skip if API token is missing (for providers that require it)
    if api_token is None and provider != "ollama":
        print(f"API token is required for {provider}. Skipping this example.")
        return

    extra_args = {"extra_headers": extra_headers} if extra_headers else {}

    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://openai.com/api/pricing/",
            word_count_threshold=1,
            extraction_strategy=LLMExtractionStrategy(
                provider=provider,
                api_token=api_token,
                schema=OpenAIModelFee.schema(),
                extraction_type="schema",
                instruction="""Extract all model names along with fees for input and output tokens."
                "{model_name: 'GPT-4', input_fee: 'US$10.00 / 1M tokens', output_fee: 'US$30.00 / 1M tokens'}.""",
                **extra_args
            ),
            bypass_cache=True,
        )
        print(json.loads(result.extracted_content)[:5])

# Usage:
await extract_structured_data_using_llm("openai/gpt-4o-mini", os.getenv("OPENAI_API_KEY"))
```

----------------------------------------

TITLE: Using AsyncWebCrawler with Context Manager in Python
DESCRIPTION: Recommended usage of AsyncWebCrawler within an async context manager, which automatically handles resource cleanup.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
async with AsyncWebCrawler(config=browser_cfg) as crawler:
    result = await crawler.arun("https://example.com")
    # The crawler automatically starts/closes resources
```

----------------------------------------

TITLE: Executing Streaming Deep Crawl with Crawl4AI in Python
DESCRIPTION: Illustrates how to enable and use streaming mode for deep crawls with `AsyncWebCrawler`. By setting `stream=True` in `CrawlerRunConfig`, the `crawler.arun` method returns an asynchronous iterator. This allows processing each `result` as soon as it becomes available, which is useful for real-time applications or handling large numbers of pages efficiently. Requires `AsyncWebCrawler`, `CrawlerRunConfig`, and a deep crawl strategy. Assumes `process_result` function is defined elsewhere.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_5

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),
    stream=True  # Enable streaming
)

async with AsyncWebCrawler() as crawler:
    # Returns an async iterator
    async for result in await crawler.arun("https://example.com", config=config):
        # Process each result as it becomes available
        process_result(result)
```

----------------------------------------

TITLE: Capturing Screenshots and PDFs with AsyncWebCrawler in Python
DESCRIPTION: An async function that demonstrates how to capture screenshots and PDFs of web pages using the AsyncWebCrawler. It configures the crawler to take a screenshot and generate a PDF of a Wikipedia page, then saves both to files. Base64 decoding is used for the screenshot data.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_156

LANGUAGE: python
CODE:
```
async def demo_screenshot_and_pdf():
    """Capture screenshot and PDF of a page"""
    print("\n=== 9. Screenshot and PDF Capture ===")

    async with AsyncWebCrawler() as crawler:
        result: List[CrawlResult] = await crawler.arun(
            # url="https://example.com",
            url="https://en.wikipedia.org/wiki/Giant_anteater",
            config=CrawlerRunConfig(screenshot=True, pdf=True),
        )

        for i, result in enumerate(result):
            # if result.screenshot_data:
            if result.screenshot:
                # Save screenshot
                screenshot_path = f"{__cur_dir__}/tmp/example_screenshot.png"
                with open(screenshot_path, "wb") as f:
                    f.write(base64.b64decode(result.screenshot))
                print(f"Screenshot saved to {screenshot_path}")

            # if result.pdf_data:
            if result.pdf:
                # Save PDF
                pdf_path = f"{__cur_dir__}/tmp/example.pdf"
                with open(pdf_path, "wb") as f:
                    f.write(result.pdf)
                print(f"PDF saved to {pdf_path}")
```

----------------------------------------

TITLE: Session Management with Storage State in Crawl4AI
DESCRIPTION: Shows how to use storage state for authenticated crawling by importing a saved JSON state file. This enables reuse of authentication credentials across multiple crawling sessions.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.2.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
result = await crawler.arun(
    url="https://example.com/protected",
    storage_state="my_storage_state.json"
)
```

----------------------------------------

TITLE: Extracting Element Attribute (JSON Schema)
DESCRIPTION: Demonstrates how to define a field in a Crawl4AI schema to extract an HTML element attribute like `href` or `src`. This can be used in `baseFields` or within nested field definitions.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_9

LANGUAGE: JSON
CODE:
```
{
  "name": "href",
  "type": "attribute",
  "attribute": "href",
  "default": null
}
```

----------------------------------------

TITLE: Applying Basic URL Pattern Filter in Crawl4AI Deep Crawl using Python
DESCRIPTION: Demonstrates configuring a `BFSDeepCrawlStrategy` to use a `FilterChain` containing a `URLPatternFilter`. This filter restricts the crawler to only follow URLs that match specific wildcard patterns (e.g., containing 'blog' or 'docs'). The filter is passed within a `FilterChain` list to the `filter_chain` argument of the strategy. Depends on `FilterChain`, `URLPatternFilter` from `crawl4ai.deep_crawling.filters`, `CrawlerRunConfig`, and `BFSDeepCrawlStrategy`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_6

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter

# Only follow URLs containing "blog" or "docs"
url_filter = URLPatternFilter(patterns=["*blog*", "*docs*"])

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=1,
        filter_chain=FilterChain([url_filter])
    )
)
```

----------------------------------------

TITLE: Basic Content Filter Pattern Examples
DESCRIPTION: Simplified code patterns showing the basic setup for both Pruning and BM25 content filters with their essential configuration parameters.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
prune_filter = PruningContentFilter(
    threshold=0.5,
    threshold_type="fixed",
    min_word_threshold=10
)
md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
config = CrawlerRunConfig(markdown_generator=md_generator)
```

LANGUAGE: python
CODE:
```
bm25_filter = BM25ContentFilter(
    user_query="health benefits fruit",
    bm25_threshold=1.2
)
md_generator = DefaultMarkdownGenerator(content_filter=bm25_filter)
config = CrawlerRunConfig(markdown_generator=md_generator)
```

----------------------------------------

TITLE: Initializing LLMExtractionStrategy in Python
DESCRIPTION: Initializes the LLMExtractionStrategy for extracting structured data using a Language Model. Requires specifying the LLM provider and supports configuration for API tokens, extraction instructions, schema, chunking parameters, and API settings.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_0

LANGUAGE: python
CODE:
```
LLMExtractionStrategy(
    # Required Parameters
    provider: str = DEFAULT_PROVIDER,     # LLM provider (e.g., "ollama/llama2")
    api_token: Optional[str] = None,      # API token
    
    # Extraction Configuration
    instruction: str = None,              # Custom extraction instruction
    schema: Dict = None,                  # Pydantic model schema for structured data
    extraction_type: str = "block",       # "block" or "schema"
    
    # Chunking Parameters
    chunk_token_threshold: int = 4000,    # Maximum tokens per chunk
    overlap_rate: float = 0.1,           # Overlap between chunks
    word_token_rate: float = 0.75,       # Word to token conversion rate
    apply_chunking: bool = True,         # Enable/disable chunking
    
    # API Configuration
    base_url: str = None,                # Base URL for API
    extra_args: Dict = {},               # Additional provider arguments
    verbose: bool = False                # Enable verbose logging
)
```

----------------------------------------

TITLE: Implementing Sliding Window Text Chunking in Python
DESCRIPTION: Creates overlapping text chunks using a sliding window approach. Provides better context preservation through chunk overlap.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/chunking.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
class SlidingWindowChunking:
    def __init__(self, window_size=100, step=50):
        self.window_size = window_size
        self.step = step

    def chunk(self, text):
        words = text.split()
        chunks = []
        for i in range(0, len(words) - self.window_size + 1, self.step):
            chunks.append(' '.join(words[i:i + self.window_size]))
        return chunks

# Example Usage
text = "This is a long text to demonstrate sliding window chunking."
chunker = SlidingWindowChunking(window_size=5, step=2)
print(chunker.chunk(text))
```

----------------------------------------

TITLE: Accessing Extracted Media (Images, Tables) in Crawl4AI (Python)
DESCRIPTION: This snippet demonstrates how to access media elements extracted by Crawl4AI, which are stored in the `result.media` dictionary. It shows retrieving lists of images (`images_info`) and tables (`tables`) and iterating through them to print details like image `src`, `alt` text, `score`, table `caption`, number of columns, and number of rows.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_6

LANGUAGE: python
CODE:
```
if result.success:
    # Get images
    images_info = result.media.get("images", [])
    print(f"Found {len(images_info)} images in total.")
    for i, img in enumerate(images_info[:3]):  # Inspect just the first 3
        print(f"[Image {i}] URL: {img['src']}")
        print(f"           Alt text: {img.get('alt', '')}")
        print(f"           Score: {img.get('score')}")
        print(f"           Description: {img.get('desc', '')}\n")
    
    # Get tables
    tables = result.media.get("tables", [])
    print(f"Found {len(tables)} data tables in total.")
    for i, table in enumerate(tables):
        print(f"[Table {i}] Caption: {table.get('caption', 'No caption')}")
        print(f"           Columns: {len(table.get('headers', []))}")
        print(f"           Rows: {len(table.get('rows', []))}")
```

----------------------------------------

TITLE: Implementing Deep Crawling with BestFirstCrawlingStrategy in Python
DESCRIPTION: This snippet demonstrates how to set up and use deep crawling in Crawl4AI v0.5.0, including configuring filters, scorers, and the crawling strategy. It uses BestFirstCrawlingStrategy to prioritize URLs based on relevance.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import time
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, BFSDeepCrawlStrategy
from crawl4ai.content_scraping_strategy import LXMLWebScrapingStrategy
from crawl4ai.deep_crawling import DomainFilter, ContentTypeFilter, FilterChain, URLPatternFilter, KeywordRelevanceScorer, BestFirstCrawlingStrategy
import asyncio

# Create a filter chain to filter urls based on patterns, domains and content type
filter_chain = FilterChain(
    [
        DomainFilter(
            allowed_domains=["docs.crawl4ai.com"],
            blocked_domains=["old.docs.crawl4ai.com"],
        ),
        URLPatternFilter(patterns=["*core*", "*advanced*"],),
        ContentTypeFilter(allowed_types=["text/html"]),
    ]
)

# Create a keyword scorer that prioritises the pages with certain keywords first
keyword_scorer = KeywordRelevanceScorer(
    keywords=["crawl", "example", "async", "configuration"], weight=0.7
)

# Set up the configuration
deep_crawl_config = CrawlerRunConfig(
    deep_crawl_strategy=BestFirstCrawlingStrategy(
        max_depth=2,
        include_external=False,
        filter_chain=filter_chain,
        url_scorer=keyword_scorer,
    ),
    scraping_strategy=LXMLWebScrapingStrategy(),
    stream=True,
    verbose=True,
)

async def main():
    async with AsyncWebCrawler() as crawler:
        start_time = time.perf_counter()
        results = []
        async for result in await crawler.arun(url="https://docs.crawl4ai.com", config=deep_crawl_config):
            print(f"Crawled: {result.url} (Depth: {result.metadata['depth']}), score: {result.metadata['score']:.2f}")
            results.append(result)
        duration = time.perf_counter() - start_time
        print(f"\n Crawled {len(results)} high-value pages in {duration:.2f} seconds")

asyncio.run(main())
```

----------------------------------------

TITLE: Accessing Crawl Cleaned HTML Content - crawl4ai - Python
DESCRIPTION: This snippet demonstrates how to access the `cleaned_html` attribute of a `CrawlResult` object. This field contains a version of the HTML content after applying cleaning rules specified in the `CrawlerRunConfig`, such as removing scripts or styles.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_9

LANGUAGE: python
CODE:
```
print(result.cleaned_html[:500])  # Show a snippet
```

----------------------------------------

TITLE: Basic Hello World Examples with Crawl4AI
DESCRIPTION: This example demonstrates basic usage of the Crawl4AI library, including connecting to a Chrome DevTools Protocol endpoint and performing a simple crawl with content filtering. It shows how to execute JavaScript on a page and how to configure the AsyncWebCrawler with different options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_161

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import (
    AsyncWebCrawler,
    BrowserConfig,
    CrawlerRunConfig,
    CacheMode,
    DefaultMarkdownGenerator,
    PruningContentFilter,
    CrawlResult
)

async def example_cdp():
    browser_conf = BrowserConfig(
        headless=False,
        cdp_url="http://localhost:9223"
    )
    crawler_config = CrawlerRunConfig(
        session_id="test",
        js_code = """(() => { return {"result": "Hello World!"} })()""",
        js_only=True
    )
    async with AsyncWebCrawler(
        config=browser_conf,
        verbose=True,
    ) as crawler:
        result : CrawlResult = await crawler.arun(
            url="https://www.helloworld.org",
            config=crawler_config,
        )
        print(result.js_execution_result)
                   

async def main():
    browser_config = BrowserConfig(headless=True, verbose=True)
    async with AsyncWebCrawler(config=browser_config) as crawler:
        crawler_config = CrawlerRunConfig(
            cache_mode=CacheMode.BYPASS,
            markdown_generator=DefaultMarkdownGenerator(
                content_filter=PruningContentFilter(
                     threshold=0.48, threshold_type="fixed", min_word_threshold=0
                )
            ),
        )
        result : CrawlResult = await crawler.arun(
            url="https://www.helloworld.org", config=crawler_config
        )
        print(result.markdown.raw_markdown[:500])

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Creating Customized Browser Context in Playwright
DESCRIPTION: Asynchronous method to create a new browser context with configured settings. It supports various configurations including viewport dimensions, proxy settings, and content blocking for optimized performance in different browsing modes.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_125

LANGUAGE: python
CODE:
```
async def create_browser_context(self, crawlerRunConfig: CrawlerRunConfig = None):
    """
    Creates and returns a new browser context with configured settings.
    Applies text-only mode settings if text_mode is enabled in config.

    Returns:
        Context: Browser context object with the specified configurations
    """
    # Base settings
    user_agent = self.config.headers.get("User-Agent", self.config.user_agent) 
    viewport_settings = {
        "width": self.config.viewport_width,
        "height": self.config.viewport_height,
    }
    proxy_settings = {"server": self.config.proxy} if self.config.proxy else None

    blocked_extensions = [
        # Images
        "jpg",
        "jpeg",
        "png",
        "gif",
        "webp",
        "svg",
        "ico",
        "bmp",
        "tiff",
        "psd",
        # Fonts
        "woff",
        "woff2",
        "ttf",
        "otf",
        "eot",
        # Styles
        # 'css', 'less', 'scss', 'sass',
        # Media
        "mp4",
        "webm",
        "ogg",
        "avi",
        "mov",
        "wmv",
        "flv",
        "m4v",
        "mp3",
        "wav",
        "aac",
        "m4a",
        "opus",
        "flac",
        # Documents
        "pdf",
        "doc",
        "docx",
        "xls",
        "xlsx",
        "ppt",
        "pptx",
        # Archives
        "zip",
        "rar",
        "7z",
        "tar",
        "gz",
        # Scripts and data
        "xml",
        "swf",
        "wasm",
    ]

    # Common context settings
    context_settings = {
        "user_agent": user_agent,
        "viewport": viewport_settings,
        "proxy": proxy_settings,
        "accept_downloads": self.config.accept_downloads,
        "storage_state": self.config.storage_state,
        "ignore_https_errors": self.config.ignore_https_errors,
        "device_scale_factor": 1.0,
        "java_script_enabled": self.config.java_script_enabled,
    }
    
    if crawlerRunConfig:
        # Check if there is value for crawlerRunConfig.proxy_config set add that to context
        if crawlerRunConfig.proxy_config:
            proxy_settings = {
                "server": crawlerRunConfig.proxy_config.server,
            }
            if crawlerRunConfig.proxy_config.username:
```

----------------------------------------

TITLE: Custom Pattern Example
DESCRIPTION: Demonstrates how to define a custom regex pattern directly within the script and use it with the RegexExtractionStrategy to extract specific data, such as prices, from a webpage.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_6

LANGUAGE: python
CODE:
```
import json
import asyncio
from crawl4ai import (
    AsyncWebCrawler,
    CrawlerRunConfig,
    RegexExtractionStrategy
)

async def extract_prices():
    # Define a custom pattern for US Dollar prices
    price_pattern = {"usd_price": r"\$\s?\d{1,3}(?:,\d{3})*(?:\.\d{2})?"}
    
    # Create strategy with custom pattern
    strategy = RegexExtractionStrategy(custom=price_pattern)
    config = CrawlerRunConfig(extraction_strategy=strategy)
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.example.com/products",
            config=config
        )
        
        if result.success:
            data = json.loads(result.extracted_content)
            for item in data:
                print(f"Found price: {item['value']}")

asyncio.run(extract_prices())
```

----------------------------------------

TITLE: LLM Extraction with Filtering Complete Example for Crawl4AI
DESCRIPTION: Comprehensive example showing LLM-based extraction with content filtering.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_17

LANGUAGE: bash
CODE:
```
crwl https://example.com \
    -B browser.yml \
    -e extract_llm.yml \
    -s llm_schema.json \
    -f filter_bm25.yml \
    -o json
```

----------------------------------------

TITLE: API Request Body: Deep Crawler Configuration (JSON)
DESCRIPTION: Example JSON payload snippet demonstrating how to configure a 'deep crawl' by specifying a `DeepCrawlerConfig` within the `crawler_config`. It shows the structure for defining recursion depth (`max_depth`) and patterns (`allowed_patterns`) to control link following.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_30

LANGUAGE: json
CODE:
```
{
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "deep_crawler_config": {
                "type": "DeepCrawlerConfig",
                "params": {
                    "max_depth": 1,
                    "allowed_patterns": [
                        ".*/links/.*"
                    ]
                }
            }
        }
    }
}
```

----------------------------------------

TITLE: Implementing Memory Adaptive Dispatcher in Python
DESCRIPTION: Configures a MemoryAdaptiveDispatcher that manages concurrency based on system memory usage with integrated rate limiting and monitoring.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher

dispatcher = MemoryAdaptiveDispatcher(
    memory_threshold_percent=90.0,  # Pause if memory exceeds this
    check_interval=1.0,             # How often to check memory
    max_session_permit=10,          # Maximum concurrent tasks
    rate_limiter=RateLimiter(       # Optional rate limiting
        base_delay=(1.0, 2.0),
        max_delay=30.0,
        max_retries=2
    ),
    monitor=CrawlerMonitor(         # Optional monitoring
        max_visible_rows=15,
        display_mode=DisplayMode.DETAILED
    )
)
```

----------------------------------------

TITLE: Customizing Crawler4ai with Callbacks
DESCRIPTION: Example showing how to define and use custom callback functions to process pages during crawling, allowing for custom extraction and processing logic.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
from crawler4ai import Crawler4ai
from bs4 import BeautifulSoup

def my_page_processor(html, url):
    soup = BeautifulSoup(html, 'html.parser')
    
    # Custom extraction logic
    custom_data = {
        'headings': [h.text for h in soup.find_all(['h1', 'h2', 'h3'])],
        'paragraphs': [p.text for p in soup.find_all('p')],
        'custom_field': soup.find(id='specific-element').text if soup.find(id='specific-element') else None
    }
    
    return custom_data

crawler = Crawler4ai(
    urls=["https://example.com"],
    max_pages=50,
    page_callback=my_page_processor
)

results = crawler.crawl()
```

----------------------------------------

TITLE: Network Request Capture Handler Implementation - Python
DESCRIPTION: Implements request capture functionality for tracking network requests, including headers, post data, and resource types. Handles binary data safely and includes error handling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/prompts/prompt_net_requests.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
async def handle_request_capture(request):
    try:
        post_data_str = None
        try:
            post_data = request.post_data_buffer
            if post_data:
                try:
                    post_data_str = post_data.decode('utf-8', errors='replace')
                except UnicodeDecodeError:
                    post_data_str = f"[Binary data: {len(post_data)} bytes]"
        except Exception:
            post_data_str = "[Error retrieving post data]"

        captured_requests.append({
            "event_type": "request",
            "url": request.url,
            "method": request.method,
            "headers": dict(request.headers),
            "post_data": post_data_str,
            "resource_type": request.resource_type,
            "is_navigation_request": request.is_navigation_request(),
            "timestamp": time.time()
        })
    except Exception as e:
        self.logger.warning(f"Error capturing request details for {request.url}: {e}", tag="CAPTURE")
        captured_requests.append({"event_type": "request_capture_error", "url": request.url, "error": str(e), "timestamp": time.time()})
```

----------------------------------------

TITLE: Content Filtering with Pruning in Python
DESCRIPTION: Implements pruning-based content filtering for removing unwanted content without specific search queries. Shows configuration of PruningContentFilter with customizable thresholds.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from crawl4ai.content_filter_strategy import PruningContentFilter

prune_filter = PruningContentFilter(
    threshold=0.5,
    threshold_type="fixed",  # or "dynamic"
    min_word_threshold=50
)
```

----------------------------------------

TITLE: Demonstrating Stream vs. Non-Stream Execution in Python with AsyncWebCrawler
DESCRIPTION: This function compares stream and non-stream execution modes of the AsyncWebCrawler. It demonstrates how non-streaming mode waits for all results before processing, while streaming mode processes results as they become available.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_207

LANGUAGE: python
CODE:
```
async def stream_vs_nonstream():
    """
    PART 2: Demonstrates the difference between stream and non-stream execution.

    Non-stream: Waits for all results before processing
    Stream: Processes results as they become available
    """
    print("\n===== STREAM VS. NON-STREAM EXECUTION =====")

    # Common configuration for both examples
    base_config = CrawlerRunConfig(
        deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1, include_external=False),
        scraping_strategy=LXMLWebScrapingStrategy(),
        verbose=False,
    )

    async with AsyncWebCrawler() as crawler:
        # NON-STREAMING MODE
        print("\n NON-STREAMING MODE:")
        print("  In this mode, all results are collected before being returned.")

        non_stream_config = base_config.clone()
        non_stream_config.stream = False

        start_time = time.perf_counter()
        results = await crawler.arun(
            url="https://docs.crawl4ai.com", config=non_stream_config
        )

        print(f"   Received all {len(results)} results at once")
        print(f"   Total duration: {time.perf_counter() - start_time:.2f} seconds")

        # STREAMING MODE
        print("\n STREAMING MODE:")
        print("  In this mode, results are processed as they become available.")

        stream_config = base_config.clone()
        stream_config.stream = True

        start_time = time.perf_counter()
        result_count = 0
        first_result_time = None

        async for result in await crawler.arun(
            url="https://docs.crawl4ai.com", config=stream_config
        ):
            result_count += 1
            if result_count == 1:
                first_result_time = time.perf_counter() - start_time
                print(
                    f"   First result received after {first_result_time:.2f} seconds: {result.url}"
                )
            elif result_count % 5 == 0:  # Show every 5th result for brevity
                print(f"   Result #{result_count}: {result.url}")

        print(f"   Total: {result_count} results")
        print(f"   First result: {first_result_time:.2f} seconds")
        print(f"   All results: {time.perf_counter() - start_time:.2f} seconds")
        print("\n Key Takeaway: Streaming allows processing results immediately")
```

----------------------------------------

TITLE: Configuring Crawler Settings and Extraction Strategy
DESCRIPTION: Sets up crawler configuration including LLM extraction strategy, JSON CSS/XPath extraction, and cache settings. Handles different extraction configurations and validates required parameters.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_52

LANGUAGE: python
CODE:
```
llm_config=LLMConfig(provider=provider, api_token=token),
instruction=instruction,
schema=load_schema_file(schema),
extraction_type="schema",
apply_chunking=False,
force_json_response=True,
verbose=verbose
```

----------------------------------------

TITLE: Processing Raw HTML and Local Files with AsyncWebCrawler in Python
DESCRIPTION: An async function that demonstrates how to process both raw HTML strings and local HTML files using AsyncWebCrawler. It creates a sample HTML file, then processes both the raw HTML string and the file URL format. The function shows how to bypass cache and access the markdown output from the crawler.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_158

LANGUAGE: python
CODE:
```
async def demo_raw_html_and_file():
    """Process raw HTML and local files"""
    print("\n=== 11. Raw HTML and Local Files ===")

    raw_html = """
    <html><body>
        <h1>Sample Article</h1>
        <p>This is sample content for testing Crawl4AI's raw HTML processing.</p>
    </body></html>
    """

    # Save to file
    file_path = Path("docs/examples/tmp/sample.html").absolute()
    with open(file_path, "w") as f:
        f.write(raw_html)

    async with AsyncWebCrawler() as crawler:
        # Crawl raw HTML
        raw_result = await crawler.arun(
            url="raw:" + raw_html, config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        )
        print("Raw HTML processing:")
        print(f"  Markdown: {raw_result.markdown.raw_markdown[:50]}...")

        # Crawl local file
        file_result = await crawler.arun(
            url=f"file://{file_path}",
            config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS),
        )
        print("\nLocal file processing:")
        print(f"  Markdown: {file_result.markdown.raw_markdown[:50]}...")

    # Clean up
    os.remove(file_path)
    print(f"Processed both raw HTML and local file ({file_path})")
```

----------------------------------------

TITLE: Filtering Links and Media during Crawling with Crawl4AI in Python
DESCRIPTION: Illustrates configuring `CrawlerRunConfig` for advanced filtering during a crawl with `AsyncWebCrawler`. It demonstrates excluding external links (`exclude_external_links=True`), specific domains (`exclude_domains`), social media links (`exclude_social_media_links=True`), and external images (`exclude_external_images=True`), while ensuring images are fully loaded (`wait_for_images=True`). The example then processes the results to show the counts of internal/external links and lists the first few found images with their source, alt text, and score.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_11

LANGUAGE: python
CODE:
```
```python
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # Suppose we want to keep only internal links, remove certain domains, 
    # and discard external images from the final crawl data.
    crawler_cfg = CrawlerRunConfig(
        exclude_external_links=True,
        exclude_domains=["spammyads.com"],
        exclude_social_media_links=True,   # skip Twitter, Facebook, etc.
        exclude_external_images=True,      # keep only images from main domain
        wait_for_images=True,             # ensure images are loaded
        verbose=True
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://www.example.com", config=crawler_cfg)

        if result.success:
            print("[OK] Crawled:", result.url)
            
            # 1. Links
            in_links = result.links.get("internal", [])
            ext_links = result.links.get("external", [])
            print("Internal link count:", len(in_links))
            print("External link count:", len(ext_links))  # should be zero with exclude_external_links=True
            
            # 2. Images
            images = result.media.get("images", [])
            print("Images found:", len(images))
            
            # Let's see a snippet of these images
            for i, img in enumerate(images[:3]):
                print(f"  - {img['src']} (alt={img.get('alt','')}, score={img.get('score','N/A')})")
        else:
            print("[ERROR] Failed to crawl. Reason:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```
```

----------------------------------------

TITLE: Comprehensive Crawl4AI Example
DESCRIPTION: Complete example demonstrating multiple configuration options and extraction strategies in Crawl4AI.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def main():
    # Example schema
    schema = {
        "name": "Articles",
        "baseSelector": "article.post",
        "fields": [
            {"name": "title", "selector": "h2", "type": "text"},
            {"name": "link",  "selector": "a",  "type": "attribute", "attribute": "href"}
        ]
    }

    run_config = CrawlerRunConfig(
        # Core
        verbose=True,
        cache_mode=CacheMode.ENABLED,
        check_robots_txt=True,   # Respect robots.txt rules
        
        # Content
        word_count_threshold=10,
        css_selector="main.content",
        excluded_tags=["nav", "footer"],
        exclude_external_links=True,
        
        # Page & JS
        js_code="document.querySelector('.show-more')?.click();",
        wait_for="css:.loaded-block",
        page_timeout=30000,
        
        # Extraction
        extraction_strategy=JsonCssExtractionStrategy(schema),
        
        # Session
        session_id="persistent_session",
        
        # Media
        screenshot=True,
        pdf=True,
        
        # Anti-bot
        simulate_user=True,
        magic=True,
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com/posts", config=run_config)
        if result.success:
            print("HTML length:", len(result.cleaned_html))
            print("Extraction JSON:", result.extracted_content)
            if result.screenshot:
                print("Screenshot length:", len(result.screenshot))
            if result.pdf:
                print("PDF bytes length:", len(result.pdf))
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Accessing Crawl Result URL - crawl4ai - Python
DESCRIPTION: This snippet demonstrates how to access the `url` attribute of a `CrawlResult` object. The `url` field contains the final URL of the page that was crawled, after following any redirects.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_1

LANGUAGE: python
CODE:
```
print(result.url)  # e.g., "https://example.com/"
```

----------------------------------------

TITLE: Using LLMContentFilter for Intelligent Markdown Generation
DESCRIPTION: Shows how to integrate LLM-based content filtering with Crawl4AI for generating more focused markdown output. The code configures a DefaultMarkdownGenerator with LLMContentFilter using Gemini API and applies specific content extraction instructions.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DefaultMarkdownGenerator
from crawl4ai.content_filter_strategy import LLMContentFilter
from crawl4ai import LLMConfig
import asyncio

llm_config = LLMConfig(provider="gemini/gemini-1.5-pro", api_token="env:GEMINI_API_KEY")

markdown_generator = DefaultMarkdownGenerator(
    content_filter=LLMContentFilter(llm_config=llm_config, instruction="Extract key concepts and summaries")
)

config = CrawlerRunConfig(markdown_generator=markdown_generator)
async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://docs.crawl4ai.com", config=config)
        print(result.markdown.fit_markdown)

asyncio.run(main())
```

----------------------------------------

TITLE: Configuring High-Traffic Settings for Crawl4AI in YAML
DESCRIPTION: This YAML snippet shows recommended settings for high-traffic scenarios in Crawl4AI, including memory thresholds and rate limiting.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_69

LANGUAGE: yaml
CODE:
```
crawler:
  memory_threshold_percent: 85.0  # More conservative memory limit
  rate_limiter:
    base_delay: [2.0, 4.0]       # More aggressive rate limiting
```

----------------------------------------

TITLE: LLMContentFilter Class Definition for AI-Powered Content Filtering
DESCRIPTION: Definition of the LLMContentFilter class that extends RelevantContentFilter. This class uses language models to generate markdown from HTML content and provides advanced filtering capabilities based on LLM-generated content scores.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_100

LANGUAGE: python
CODE:
```
class LLMContentFilter(RelevantContentFilter):
    """Content filtering using LLMs to generate relevant markdown.

    How it works:
    1. Extracts page metadata with fallbacks.
    2. Extracts text chunks from the body element.
    3. Applies LLMs to generate markdown for each chunk.
    4. Filters out chunks below the threshold.
    5. Sorts chunks by score in descending order.
    6. Returns the top N chunks.

    Attributes:
        llm_config (LLMConfig): LLM configuration object.
        instruction (str): Instruction for LLM markdown generation
        chunk_token_threshold (int): Chunk token threshold for splitting (default: 1e9).
        overlap_rate (float): Overlap rate for chunking (default: 0.5).
        word_token_rate (float): Word token rate for chunking (default: 0.2).
        verbose (bool): Enable verbose logging (default: False).
        logger (AsyncLogger): Custom logger for LLM operations (optional).
    """
    _UNWANTED_PROPS = {
        'provider' : 'Instead, use llm_config=LLMConfig(provider="...")',
        'api_token' : 'Instead, use llm_config=LlMConfig(api_token="...")',
        'base_url' : 'Instead, use llm_config=LLMConfig(base_url="...")',
        'api_base' : 'Instead, use llm_config=LLMConfig(base_url="...")',
    }

    def __init__(
        self,
        llm_config: "LLMConfig" = None,
        instruction: str = None,
        chunk_token_threshold: int = int(1e9),
        overlap_rate: float = OVERLAP_RATE,
        word_token_rate: float = WORD_TOKEN_RATE,
        # char_token_rate: float = WORD_TOKEN_RATE * 5,
        # chunk_mode: str = "char",
        verbose: bool = False,
        logger: Optional[AsyncLogger] = None,
        ignore_cache: bool = True,
        # Deprecated properties
        provider: str = DEFAULT_PROVIDER,
        api_token: Optional[str] = None,
        base_url: Optional[str] = None,
        api_base: Optional[str] = None,
        extra_args: Dict = None,
    ):
        super().__init__(None)
        self.provider = provider
        self.api_token = api_token
        self.base_url = base_url or api_base
        self.llm_config = llm_config
        self.instruction = instruction
        self.chunk_token_threshold = chunk_token_threshold
        self.overlap_rate = overlap_rate
        self.word_token_rate = word_token_rate or WORD_TOKEN_RATE
        # self.chunk_mode: str = chunk_mode
        # self.char_token_rate = char_token_rate or word_token_rate / 5
        # self.token_rate = word_token_rate if chunk_mode == "word" else self.char_token_rate
        self.token_rate = word_token_rate or WORD_TOKEN_RATE
        self.extra_args = extra_args or {}
        self.ignore_cache = ignore_cache
        self.verbose = verbose

        # Setup logger with custom styling for LLM operations
        if logger:
            self.logger = logger
        elif verbose:
            self.logger = AsyncLogger(
                verbose=verbose,
                icons={
                    **AsyncLogger.DEFAULT_ICONS,
                    "LLM": "",  # Star for LLM operations
                    "CHUNK": "",  # Diamond for chunks
                    "CACHE": "",  # Lightning for cache operations
                },
                colors={
                    **AsyncLogger.DEFAULT_COLORS,
                    LogLevel.INFO: Fore.MAGENTA
                    + Style.DIM,  # Dimmed purple for LLM ops
                },
            )
        else:
            self.logger = None

        self.usages = []
        self.total_usage = TokenUsage()
```

----------------------------------------

TITLE: Configuring Production Settings for Crawl4AI in YAML
DESCRIPTION: This YAML snippet shows recommended production settings for Crawl4AI, including app configuration, rate limiting, and security settings.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_67

LANGUAGE: yaml
CODE:
```
app:
  reload: False              # Disable reload in production
  timeout_keep_alive: 120    # Lower timeout for better resource management

rate_limiting:
  storage_uri: "redis://redis:6379"  # Use Redis for distributed rate limiting
  default_limit: "50/minute"         # More conservative rate limit

security:
  enabled: true                      # Enable all security features
  trusted_hosts: ["your-domain.com"] # Restrict to your domain
```

----------------------------------------

TITLE: Executing Multiple Button Clicks in a Single Crawl4AI Call with Python and JavaScript
DESCRIPTION: This snippet shows how to use Crawl4AI to execute a complex JavaScript snippet that clicks multiple modules on a page, waits for content updates, and returns control to Crawl4AI for extraction, all in a single arun() call.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/tutorial_dynamic_clicks.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CacheMode

js_code = [
    # Example JS that clicks multiple modules:
    """
    (async () => {
      const modules = document.querySelectorAll('.module-item');
      for (let i = 0; i < modules.length; i++) {
        modules[i].scrollIntoView();
        modules[i].click();
        // Wait for each module's content to load, adjust 100ms as needed
        await new Promise(r => setTimeout(r, 100));
      }
    })();
    """
]

async with AsyncWebCrawler(headless=True, verbose=True) as crawler:
    result = await crawler.arun(
        url="https://example.com",
        js_code=js_code,
        wait_for="css:.final-loaded-content-class",
        cache_mode=CacheMode.BYPASS
    )

# `result` now contains all content after all modules have been clicked in one go.
```

----------------------------------------

TITLE: Web Embedding Index Implementation
DESCRIPTION: Implements semantic search infrastructure for crawled content using vector embeddings. Features automatic embedding generation, content chunking, and efficient vector storage.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler
from crawl4ai.indexing import WebIndex

# Initialize and build index
index = WebIndex(model="efficient-mini")

async with AsyncWebCrawler() as crawler:
    # Crawl and index content
    await index.build(
        urls=["https://docs.example.com"],
        crawler=crawler,
        options={
            "chunk_method": "semantic",
            "update_policy": "incremental",
            "embedding_batch_size": 100
        }
    )

    # Search through indexed content
    results = await index.search(
        query="How to implement OAuth authentication?",
        filters={
            "content_type": "technical",
            "recency": "6months"
        },
        top_k=5
    )

    # Get similar content
    similar = await index.find_similar(
        url="https://docs.example.com/auth/oauth",
        threshold=0.85
    )
```

----------------------------------------

TITLE: Streaming Usage of arun_many() for Concurrent Crawling in Python
DESCRIPTION: This example shows how to use arun_many() with streaming enabled, allowing processing of results as they become available. It's ideal for handling large numbers of URLs efficiently.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    stream=True,  # Enable streaming mode
    cache_mode=CacheMode.BYPASS
)

# Process results as they complete
async for result in await crawler.arun_many(
    urls=["https://site1.com", "https://site2.com", "https://site3.com"],
    config=config
):
    if result.success:
        print(f"Just completed: {result.url}")
        # Process each result immediately
        process_result(result)
```

----------------------------------------

TITLE: Using MemoryAdaptiveDispatcher in Batch and Stream Modes with Python
DESCRIPTION: This code snippet illustrates how to use the new MemoryAdaptiveDispatcher in both batch and stream modes. It demonstrates configuring the dispatcher and using it with AsyncWebCrawler for efficient memory management during crawling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, MemoryAdaptiveDispatcher
import asyncio

# Configure the dispatcher (optional, defaults are used if not provided)
dispatcher = MemoryAdaptiveDispatcher(
    memory_threshold_percent=80.0,  # Pause if memory usage exceeds 80%
    check_interval=0.5,  # Check memory every 0.5 seconds
)

async def batch_mode():
    async with AsyncWebCrawler() as crawler:
        results = await crawler.arun_many(
            urls=["https://docs.crawl4ai.com", "https://github.com/unclecode/crawl4ai"],
            config=CrawlerRunConfig(stream=False),  # Batch mode
            dispatcher=dispatcher,
        )
        for result in results:
            print(f"Crawled: {result.url} with status code: {result.status_code}")

async def stream_mode():
    async with AsyncWebCrawler() as crawler:
        # OR, for streaming:
        async for result in await crawler.arun_many(
            urls=["https://docs.crawl4ai.com", "https://github.com/unclecode/crawl4ai"],
            config=CrawlerRunConfig(stream=True),
            dispatcher=dispatcher,
        ):
            print(f"Crawled: {result.url} with status code: {result.status_code}")

print("Dispatcher in batch mode:")
asyncio.run(batch_mode())
print("-" * 50)
print("Dispatcher in stream mode:")
asyncio.run(stream_mode())
```

----------------------------------------

TITLE: Implementing PruningContentFilter in Crawl4AI
DESCRIPTION: Example showing how to set up and use PruningContentFilter to remove low-value content based on text density, link density, and tag importance. Includes configuration of threshold parameters and markdown generation.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/fit-markdown.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    # Step 1: Create a pruning filter
    prune_filter = PruningContentFilter(
        # Lower  more content retained, higher  more content pruned
        threshold=0.45,           
        # "fixed" or "dynamic"
        threshold_type="dynamic",  
        # Ignore nodes with <5 words
        min_word_threshold=5      
    )

    # Step 2: Insert it into a Markdown Generator
    md_generator = DefaultMarkdownGenerator(content_filter=prune_filter)
    
    # Step 3: Pass it to CrawlerRunConfig
    config = CrawlerRunConfig(
        markdown_generator=md_generator
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com", 
            config=config
        )
        
        if result.success:
            # 'fit_markdown' is your pruned content, focusing on "denser" text
            print("Raw Markdown length:", len(result.markdown.raw_markdown))
            print("Fit Markdown length:", len(result.markdown.fit_markdown))
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Implementing Streaming URL Processing in Python
DESCRIPTION: Demonstrates real-time processing of crawled URLs using streaming configuration. Enables immediate processing of results as they become available instead of waiting for all crawls to complete.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(stream=True)

async with AsyncWebCrawler() as crawler:
    async for result in await crawler.arun_many(urls, config=config):
        print(f"Got result for {result.url}")
        # Process each result immediately
```

----------------------------------------

TITLE: Implementing LLM-Powered Markdown Generation
DESCRIPTION: Configures LLM-based content filtering and organization system. Uses GPT-4 to extract technical documentation and code examples from crawled content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    markdown_generator=DefaultMarkdownGenerator(
        content_filter=LLMContentFilter(
            provider="openai/gpt-4o",
            instruction="Extract technical documentation and code examples"
        )
    )
)
```

----------------------------------------

TITLE: Implementing Load More Functionality with Hacker News Example in Python
DESCRIPTION: This example demonstrates how to load the initial Hacker News page and then click the 'More' link to load additional content. It uses session management to maintain state between requests and custom JavaScript execution to trigger the load more action.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_116

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Step 1: Load initial Hacker News page
    config = CrawlerRunConfig(
        wait_for="css:.athing:nth-child(30)"  # Wait for 30 items
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            config=config
        )
        print("Initial items loaded.")

        # Step 2: Let's scroll and click the "More" link
        load_more_js = [
            "window.scrollTo(0, document.body.scrollHeight);",
            # The "More" link at page bottom
            "document.querySelector('a.morelink')?.click();"  
        ]
        
        next_page_conf = CrawlerRunConfig(
            js_code=load_more_js,
            wait_for="""js:() => {
                return document.querySelectorAll('.athing').length > 30;
            }""",
            # Mark that we do not re-navigate, but run JS in the same session:
            js_only=True,
            session_id="hn_session"
        )

        # Re-use the same crawler session
        result2 = await crawler.arun(
            url="https://news.ycombinator.com",  # same URL but continuing session
            config=next_page_conf
        )
        total_items = result2.cleaned_html.count("athing")
        print("Items after load-more:", total_items)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Defining Crawl4AI Configuration Grammar in JSON
DESCRIPTION: Specifies the grammar for Crawl4AI configuration objects, including type-params pattern, simple and complex values, and dictionary wrapping.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_56

LANGUAGE: json
CODE:
```
config := {
    "type": string,
    "params": {
        key: simple_value | complex_value
    }
}

simple_value := string | number | boolean | [simple_value]
complex_value := config | dict_value

dict_value := {
    "type": "dict",
    "value": object
}
```

----------------------------------------

TITLE: Interacting with crawl4ai Server using Python SDK
DESCRIPTION: Provides a Python example demonstrating how to use the `Crawl4aiDockerClient` from the `crawl4ai` SDK. It shows initializing the client, performing both non-streaming and streaming crawl operations by calling `client.crawl` with different configurations, handling results, and fetching the server's schema using `client.get_schema`. Requires the `crawl4ai` library and `asyncio`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_28

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai.docker_client import Crawl4aiDockerClient
from crawl4ai import BrowserConfig, CrawlerRunConfig, CacheMode # Assuming you have crawl4ai installed

async def main():
    # Point to the correct server port
    async with Crawl4aiDockerClient(base_url="http://localhost:11235", verbose=True) as client:
        # If JWT is enabled on the server, authenticate first:
        # await client.authenticate("user@example.com") # See Server Configuration section

        # Example Non-streaming crawl
        print("--- Running Non-Streaming Crawl ---")
        results = await client.crawl(
            ["https://httpbin.org/html"],
            browser_config=BrowserConfig(headless=True), # Use library classes for config aid
            crawler_config=CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
        )
        if results: # client.crawl returns None on failure
          print(f"Non-streaming results success: {results.success}")
          if results.success:
              for result in results: # Iterate through the CrawlResultContainer
                  print(f"URL: {result.url}, Success: {result.success}")
        else:
            print("Non-streaming crawl failed.")


        # Example Streaming crawl
        print("\n--- Running Streaming Crawl ---")
        stream_config = CrawlerRunConfig(stream=True, cache_mode=CacheMode.BYPASS)
        try:
            async for result in await client.crawl( # client.crawl returns an async generator for streaming
                ["https://httpbin.org/html", "https://httpbin.org/links/5/0"],
                browser_config=BrowserConfig(headless=True),
                crawler_config=stream_config
            ):
                print(f"Streamed result: URL: {result.url}, Success: {result.success}")
        except Exception as e:
            print(f"Streaming crawl failed: {e}")


        # Example Get schema
        print("\n--- Getting Schema ---")
        schema = await client.get_schema()
        print(f"Schema received: {bool(schema)}") # Print whether schema was received

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Excluding External and Social Media Links in Crawl4AI (Python)
DESCRIPTION: This snippet demonstrates configuring `CrawlerRunConfig` to exclude both external links (links pointing outside the primary domain) and links to known social media platforms. `exclude_external_links=True` handles the former, and `exclude_social_media_links=True` handles the latter. The example shows passing this configuration to the `arun` method.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    crawler_cfg = CrawlerRunConfig(
        exclude_external_links=True,          # No links outside primary domain
        exclude_social_media_links=True       # Skip recognized social media domains
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            "https://www.example.com",
            config=crawler_cfg
        )
        if result.success:
            print("[OK] Crawled:", result.url)
            print("Internal links count:", len(result.links.get("internal", [])))
            print("External links count:", len(result.links.get("external", [])))  
            # Likely zero external links in this scenario
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Implementing LLMContentFilter for Intelligent Content Processing
DESCRIPTION: This snippet demonstrates how to use the LLMContentFilter with a language model to generate high-quality markdown from web content. It shows the setup with custom instructions and configuration parameters for chunk processing.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_108

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig
from crawl4ai.content_filter_strategy import LLMContentFilter

async def main():
    # Initialize LLM filter with specific instruction
    filter = LLMContentFilter(
        llm_config = LLMConfig(provider="openai/gpt-4o",api_token="your-api-token"), #or use environment variable
        instruction="""
        Focus on extracting the core educational content.
        Include:
        - Key concepts and explanations
        - Important code examples
        - Essential technical details
        Exclude:
        - Navigation elements
        - Sidebars
        - Footer content
        Format the output as clean markdown with proper code blocks and headers.
        """,
        chunk_token_threshold=4096,  # Adjust based on your needs
        verbose=True
    )

    config = CrawlerRunConfig(
        content_filter=filter
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com", config=config)
        print(result.markdown.fit_markdown)  # Filtered markdown content
```

----------------------------------------

TITLE: Asynchronous Crawling with Iframe Processing in Crawl4AI
DESCRIPTION: This example shows how to use the AsyncWebCrawler with a configuration that processes iframes. It demonstrates the full usage pattern including the asynchronous context manager and error handling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_16

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    config = CrawlerRunConfig(
        process_iframes=True,
        remove_overlay_elements=True
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.org/iframe-demo", 
            config=config
        )
        print("Iframe-merged length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Authenticating and Using JWT Tokens with Crawl4AI in Python
DESCRIPTION: This Python code demonstrates how to authenticate and use JWT tokens with the Crawl4AI Docker client, including making authenticated requests.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_66

LANGUAGE: python
CODE:
```
from crawl4ai.docker_client import Crawl4aiDockerClient

async with Crawl4aiDockerClient() as client:
    # Authenticate first
    await client.authenticate("user@example.com")
    
    # Now all requests will include the token automatically
    result = await client.crawl(urls=["https://example.com"])
```

----------------------------------------

TITLE: Enhancing Chunking with Cosine Similarity for Relevance Extraction in Python
DESCRIPTION: Combines text chunking with cosine similarity to extract relevant content based on a query. This class uses TF-IDF vectorization and cosine similarity metrics to rank chunks based on their relevance to a specified query.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_168

LANGUAGE: python
CODE:
```
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class CosineSimilarityExtractor:
    def __init__(self, query):
        self.query = query
        self.vectorizer = TfidfVectorizer()

    def find_relevant_chunks(self, chunks):
        vectors = self.vectorizer.fit_transform([self.query] + chunks)
        similarities = cosine_similarity(vectors[0:1], vectors[1:]).flatten()
        return [(chunks[i], similarities[i]) for i in range(len(chunks))]

# Example Workflow
text = """This is a sample document. It has multiple sentences. 
We are testing chunking and similarity."""

chunker = SlidingWindowChunking(window_size=5, step=3)
chunks = chunker.chunk(text)
query = "testing chunking"
extractor = CosineSimilarityExtractor(query)
relevant_chunks = extractor.find_relevant_chunks(chunks)

print(relevant_chunks)
```

----------------------------------------

TITLE: Setting Up LLM Configuration in Python
DESCRIPTION: Configures the default LLM provider and API token, prompting the user if not already configured. Supports various LLM providers including OpenAI, Anthropic, and Ollama.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_33

LANGUAGE: python
CODE:
```
def setup_llm_config() -> tuple[str, str]:
    config = get_global_config()
    provider = config.get("DEFAULT_LLM_PROVIDER")
    token = config.get("DEFAULT_LLM_PROVIDER_TOKEN")
    
    if not provider:
        click.echo("\nNo default LLM provider configured.")
        click.echo("Provider format: 'company/model' (e.g., 'openai/gpt-4o', 'anthropic/claude-3-sonnet')")
        click.echo("See available providers at: https://docs.litellm.ai/docs/providers")
        provider = click.prompt("Enter provider")
        
    if not provider.startswith("ollama/"):
        if not token:
            token = click.prompt("Enter API token for " + provider, hide_input=True)
    else:
        token = "no-token"
    
    if not config.get("DEFAULT_LLM_PROVIDER") or not config.get("DEFAULT_LLM_PROVIDER_TOKEN"):
        config["DEFAULT_LLM_PROVIDER"] = provider
        config["DEFAULT_LLM_PROVIDER_TOKEN"] = token
        save_global_config(config)
        click.echo("\nConfiguration saved to ~/.crawl4ai/global.yml")
    
    return provider, token
```

----------------------------------------

TITLE: Streaming Crawl Results via API using Python
DESCRIPTION: Defines an asynchronous Python function `test_stream_crawl` that sends a POST request to the `/crawl/stream` endpoint (running locally on port 11235). It uses the `httpx` library for asynchronous streaming. The payload includes target URLs, browser configuration (headless, viewport), and crawler configuration (stream enabled, cache bypassed). It reads the NDJSON response line by line, prints each result, and checks for a completion marker. Error handling for HTTP errors and JSON decoding is included. Requires `httpx` and `json` libraries. An optional `token` parameter is present but commented out for JWT authentication.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_31

LANGUAGE: python
CODE:
```
import json
import httpx # Use httpx for async streaming example

async def test_stream_crawl(token: str = None): # Made token optional
    """Test the /crawl/stream endpoint with multiple URLs."""
    url = "http://localhost:11235/crawl/stream" # Updated port
    payload = {
        "urls": [
            "https://httpbin.org/html",
            "https://httpbin.org/links/5/0",
        ],
        "browser_config": {
            "type": "BrowserConfig",
            "params": {"headless": True, "viewport": {"type": "dict", "value": {"width": 1200, "height": 800}}} # Viewport needs type:dict
        },
        "crawler_config": {
            "type": "CrawlerRunConfig",
            "params": {"stream": True, "cache_mode": "bypass"}
        }
    }

    headers = {}
    # if token:
    #    headers = {"Authorization": f"Bearer {token}"} # If JWT is enabled

    try:
        async with httpx.AsyncClient() as client:
            async with client.stream("POST", url, json=payload, headers=headers, timeout=120.0) as response:
                print(f"Status: {response.status_code} (Expected: 200)")
                response.raise_for_status() # Raise exception for bad status codes

                # Read streaming response line-by-line (NDJSON)
                async for line in response.aiter_lines():
                    if line:
                        try:
                            data = json.loads(line)
                            # Check for completion marker
                            if data.get("status") == "completed":
                                print("Stream completed.")
                                break
                            print(f"Streamed Result: {json.dumps(data, indent=2)}")
                        except json.JSONDecodeError:
                            print(f"Warning: Could not decode JSON line: {line}")

    except httpx.HTTPStatusError as e:
         print(f"HTTP error occurred: {e.response.status_code} - {e.response.text}")
    except Exception as e:
        print(f"Error in streaming crawl test: {str(e)}")

# To run this example:
# import asyncio
# asyncio.run(test_stream_crawl())
```

----------------------------------------

TITLE: Implementing Streaming Web Crawler in Python
DESCRIPTION: Implementation of a streaming web crawler that processes results immediately as they become available. Uses async iterator pattern for real-time processing.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_36

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(max_depth=1),
    stream=True  # Enable streaming
)

async with AsyncWebCrawler() as crawler:
    # Returns an async iterator
    async for result in await crawler.arun("https://example.com", config=config):
        # Process each result as it becomes available
        process_result(result)
```

----------------------------------------

TITLE: Customizing Docker Compose Build Arguments
DESCRIPTION: These commands demonstrate how to pass build arguments like `INSTALL_TYPE` and `ENABLE_GPU` to the Docker Compose build process. This allows customizing the image with different feature sets or GPU support before building and running.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_9

LANGUAGE: Bash
CODE:
```
# Build with all features (includes torch and transformers)
INSTALL_TYPE=all docker compose up --build -d

# Build with GPU support (for AMD64 platforms)
ENABLE_GPU=true docker compose up --build -d
```

----------------------------------------

TITLE: Configuring LLMContentFilter for Focused Content Extraction
DESCRIPTION: This code example shows how to configure LLMContentFilter with instructions for extracting specific types of content from a webpage, such as technical documentation and code examples.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_110

LANGUAGE: python
CODE:
```
filter = LLMContentFilter(
    instruction="""
    Focus on extracting specific types of content:
    - Technical documentation
    - Code examples
    - API references
    Reformat the content into clear, well-structured markdown
    """,
    chunk_token_threshold=4096
)
```

----------------------------------------

TITLE: Configuring Network and Console Capture in Crawl4AI (Python)
DESCRIPTION: This snippet demonstrates how to enable network request and console message capturing in Crawl4AI using the CrawlerRunConfig.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/network-console-capture.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

# Enable both network request capture and console message capture
config = CrawlerRunConfig(
    capture_network_requests=True,  # Capture all network requests and responses
    capture_console_messages=True   # Capture all browser console output
)
```

----------------------------------------

TITLE: Setting Up Authenticated Proxy in Crawl4AI
DESCRIPTION: Shows how to configure an authenticated proxy using a dictionary configuration with server, username, and password credentials.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import BrowserConfig

proxy_config = {
    "server": "http://proxy.example.com:8080",
    "username": "user",
    "password": "pass"
}

browser_config = BrowserConfig(proxy_config=proxy_config)
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")
```

----------------------------------------

TITLE: Extracting HTML Table Data into Pandas DataFrame (Python)
DESCRIPTION: Illustrates how to extract data from an HTML table captured during a crawl and load it into a Pandas DataFrame. The snippet accesses the table data stored in `result.media["tables"]` and uses the headers and rows to construct the DataFrame. Requires the `pandas` library and a `result` object from a Crawl4AI execution.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.6.0.md#_snippet_1

LANGUAGE: python
CODE:
```
```python
raw_df = pd.DataFrame(
    result.media["tables"][0]["rows"],
    columns=result.media["tables"][0]["headers"]
)
```
```

----------------------------------------

TITLE: Excluding External and Social Media Links in Crawl4AI
DESCRIPTION: Complete example demonstrating how to configure a crawler to exclude external links and social media platforms, with output reporting on the resulting link counts.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_91

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    crawler_cfg = CrawlerRunConfig(
        exclude_external_links=True,          # No links outside primary domain
        exclude_social_media_links=True       # Skip recognized social media domains
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            "https://www.example.com",
            config=crawler_cfg
        )
        if result.success:
            print("[OK] Crawled:", result.url)
            print("Internal links count:", len(result.links.get("internal", [])))
            print("External links count:", len(result.links.get("external", [])))  
            # Likely zero external links in this scenario
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Generating Focused Markdown with LLM Content Filter in Python
DESCRIPTION: This function demonstrates the use of a PruningContentFilter to generate focused markdown content from a crawled page (Wikipedia article on Python programming language).
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_149

LANGUAGE: python
CODE:
```
async def demo_fit_markdown():
    """Generate focused markdown with LLM content filter"""
    print("\n=== 3. Fit Markdown with LLM Content Filter ===")

    async with AsyncWebCrawler() as crawler:
        result: CrawlResult = await crawler.arun(
            url = "https://en.wikipedia.org/wiki/Python_(programming_language)",
            config=CrawlerRunConfig(
                markdown_generator=DefaultMarkdownGenerator(
                    content_filter=PruningContentFilter()
                )
            ),
        )

        # Print stats and save the fit markdown
        print(f"Raw: {len(result.markdown.raw_markdown)} chars")
        print(f"Fit: {len(result.markdown.fit_markdown)} chars")
```

----------------------------------------

TITLE: Filtering Content with BM25 in crawl4ai (Python)
DESCRIPTION: Demonstrates how to use the BM25ContentFilter strategy with AsyncWebCrawler to extract content relevant to a specific query. It shows initializing the crawler with the filter and running it on a URL, then printing the filtered content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#_snippet_2

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler
from crawl4ai.content_filter_strategy import BM25ContentFilter

async def filter_content(url, query):
    async with AsyncWebCrawler() as crawler:
        content_filter = BM25ContentFilter(user_query=query)
        result = await crawler.arun(url=url, extraction_strategy=content_filter, fit_markdown=True)
        print(result.extracted_content)  # Or result.fit_markdown for the markdown version
        print(result.fit_html) # Or result.fit_html to show HTML with only the filtered content

asyncio.run(filter_content("https://en.wikipedia.org/wiki/Apple", "fruit nutrition health"))
```

----------------------------------------

TITLE: Dynamic Content Handling with Crawl4AI
DESCRIPTION: This snippet shows how to handle dynamic content using Crawl4AI. It includes options for waiting for specific conditions and executing JavaScript code to interact with the page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
async def crawl_dynamic_content():
    # You can use wait_for to wait for a condition to be met before returning the result
    # wait_for = """() => {
    #     return Array.from(document.querySelectorAll('article.tease-card')).length > 10;
    # }"""

    # wait_for can be also just a css selector
    # wait_for = "article.tease-card:nth-child(10)"

    async with AsyncWebCrawler(verbose=True) as crawler:
        js_code = [
            "const loadMoreButton = Array.from(document.querySelectorAll('button')).find(button => button.textContent.includes('Load More')); loadMoreButton && loadMoreButton.click();"
        ]
        result = await crawler.arun(
            url="https://www.nbcnews.com/business",
            js_code=js_code,
            # wait_for=wait_for,
            bypass_cache=True,
        )
        print(result.markdown.raw_markdown[:500])  # Print first 500 characters

asyncio.run(crawl_dynamic_content())
```

----------------------------------------

TITLE: Link Discovery and Processing for BFS Web Crawler in Python
DESCRIPTION: Handles extracting, validating, and scoring links from crawl results. This method respects depth and page limits, applies URL filtering, and scores URLs if a scorer is provided. It prepares the next batch of URLs to crawl based on the discovery process.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_178

LANGUAGE: python
CODE:
```
async def link_discovery(
    self,
    result: CrawlResult,
    source_url: str,
    current_depth: int,
    visited: Set[str],
    next_level: List[Tuple[str, Optional[str]]],
    depths: Dict[str, int],
) -> None:
    """
    Extracts links from the crawl result, validates and scores them, and
    prepares the next level of URLs.
    Each valid URL is appended to next_level as a tuple (url, parent_url)
    and its depth is tracked.
    """            
    next_depth = current_depth + 1
    if next_depth > self.max_depth:
        return

    # If we've reached the max pages limit, don't discover new links
    remaining_capacity = self.max_pages - self._pages_crawled
    if remaining_capacity <= 0:
        self.logger.info(f"Max pages limit ({self.max_pages}) reached, stopping link discovery")
        return

    # Get internal links and, if enabled, external links.
    links = result.links.get("internal", [])
    if self.include_external:
        links += result.links.get("external", [])

    valid_links = []
    
    # First collect all valid links
    for link in links:
        url = link.get("href")
        # Strip URL fragments to avoid duplicate crawling
        # base_url = url.split('#')[0] if url else url
        base_url = normalize_url_for_deep_crawl(url, source_url)
        if base_url in visited:
            continue
        if not await self.can_process_url(url, next_depth):
            self.stats.urls_skipped += 1
            continue

        # Score the URL if a scorer is provided
        score = self.url_scorer.score(base_url) if self.url_scorer else 0
        
        # Skip URLs with scores below the threshold
        if score < self.score_threshold:
            self.logger.debug(f"URL {url} skipped: score {score} below threshold {self.score_threshold}")
            self.stats.urls_skipped += 1
            continue
        
        valid_links.append((base_url, score))
    
    # If we have more valid links than capacity, sort by score and take the top ones
    if len(valid_links) > remaining_capacity:
        if self.url_scorer:
            # Sort by score in descending order
            valid_links.sort(key=lambda x: x[1], reverse=True)
        # Take only as many as we have capacity for
        valid_links = valid_links[:remaining_capacity]
        self.logger.info(f"Limiting to {remaining_capacity} URLs due to max_pages limit")
        
    # Process the final selected links
    for url, score in valid_links:
        # attach the score to metadata if needed
        if score:
            result.metadata = result.metadata or {}
            result.metadata["score"] = score
        next_level.append((url, source_url))
        depths[url] = next_depth
```

----------------------------------------

TITLE: Initializing LLMContentFilter for Web Crawling in Python
DESCRIPTION: This snippet demonstrates how to initialize and use LLMContentFilter with AsyncWebCrawler for intelligent content filtering. It includes setting up the filter with custom instructions and configuring the crawler.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, LLMConfig
from crawl4ai.content_filter_strategy import LLMContentFilter

async def main():
    # Initialize LLM filter with specific instruction
    filter = LLMContentFilter(
        llm_config = LLMConfig(provider="openai/gpt-4o",api_token="your-api-token"), #or use environment variable
        instruction="""
        Focus on extracting the core educational content.
        Include:
        - Key concepts and explanations
        - Important code examples
        - Essential technical details
        Exclude:
        - Navigation elements
        - Sidebars
        - Footer content
        Format the output as clean markdown with proper code blocks and headers.
        """,
        chunk_token_threshold=4096,  # Adjust based on your needs
        verbose=True
    )

    config = CrawlerRunConfig(
        content_filter=filter
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com", config=config)
        print(result.markdown.fit_markdown)  # Filtered markdown content
```

----------------------------------------

TITLE: Copying LLM Environment Example File
DESCRIPTION: This command copies the example LLM environment file provided in the repository's deploy directory to the project root. This file should then be edited to add actual API keys for LLM support when using Docker Compose or manual builds.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_6

LANGUAGE: Bash
CODE:
```
# Make sure you are in the 'crawl4ai' root directory
cp deploy/docker/.llm.env.example .llm.env

# Now edit .llm.env and add your API keys
```

----------------------------------------

TITLE: Loading Crawler4ai Configuration from JSON File
DESCRIPTION: Code example showing how to load a configuration from a JSON file and initialize the Crawler4ai with those settings.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
from crawler4ai import Crawler4ai
import json

# Load configuration from file
with open("config.json", "r") as f:
    config = json.load(f)

# Initialize crawler with config
crawler = Crawler4ai(**config)

# Start crawling
results = crawler.crawl()
```

----------------------------------------

TITLE: Configuring LLMContentFilter for Exact Content Preservation
DESCRIPTION: This code example shows how to set up LLMContentFilter with instructions focused on preserving original content while removing irrelevant elements. It maintains the exact language and structure of the educational content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_109

LANGUAGE: python
CODE:
```
filter = LLMContentFilter(
    instruction="""
    Extract the main educational content while preserving its original wording and substance completely.
    1. Maintain the exact language and terminology
    2. Keep all technical explanations and examples intact
    3. Preserve the original flow and structure
    4. Remove only clearly irrelevant elements like navigation menus and ads
    """,
    chunk_token_threshold=4096
)
```

----------------------------------------

TITLE: Configuring Download Settings in Crawl4AI
DESCRIPTION: Demonstrates how to enable downloads globally by configuring the BrowserConfig object with accept_downloads parameter.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import BrowserConfig, AsyncWebCrawler

async def main():
    config = BrowserConfig(accept_downloads=True)  # Enable downloads globally
    async with AsyncWebCrawler(config=config) as crawler:
        # ... your crawling logic ...

asyncio.run(main())
```

----------------------------------------

TITLE: Implementing Wrap-Up and Key Takeaways in Python with Crawl4AI
DESCRIPTION: This function demonstrates a complete crawler example combining filters, scorers, and streaming for an optimized crawl. It creates a sophisticated filter chain, a composite scorer, and executes the crawl using the BestFirstCrawlingStrategy.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_211

LANGUAGE: python
CODE:
```
async def wrap_up():
    """
    PART 6: Wrap-Up and Key Takeaways

    Summarize the key concepts learned in this tutorial.
    """
    print("\n===== COMPLETE CRAWLER EXAMPLE =====")
    print("Combining filters, scorers, and streaming for an optimized crawl")

    # Create a sophisticated filter chain
    filter_chain = FilterChain(
        [
            DomainFilter(
                allowed_domains=["docs.crawl4ai.com"],
                blocked_domains=["old.docs.crawl4ai.com"],
            ),
            URLPatternFilter(patterns=["*core*", "*advanced*", "*blog*"]),
            ContentTypeFilter(allowed_types=["text/html"]),
        ]
    )

    # Create a composite scorer that combines multiple scoring strategies
    keyword_scorer = KeywordRelevanceScorer(
        keywords=["crawl", "example", "async", "configuration"], weight=0.7
    )
    # Set up the configuration
    config = CrawlerRunConfig(
        deep_crawl_strategy=BestFirstCrawlingStrategy(
            max_depth=1,
            include_external=False,
            filter_chain=filter_chain,
            url_scorer=keyword_scorer,
        ),
        scraping_strategy=LXMLWebScrapingStrategy(),
        stream=True,
        verbose=True,
    )

    # Execute the crawl
    results = []
    start_time = time.perf_counter()

    async with AsyncWebCrawler() as crawler:
        async for result in await crawler.arun(
            url="https://docs.crawl4ai.com", config=config
        ):
            results.append(result)
            score = result.metadata.get("score", 0)
            depth = result.metadata.get("depth", 0)
            print(f" Depth: {depth} | Score: {score:.2f} | {result.url}")

    duration = time.perf_counter() - start_time

    # Summarize the results
    print(f"\n Crawled {len(results)} high-value pages in {duration:.2f} seconds")
    print(
        f" Average score: {sum(r.metadata.get('score', 0) for r in results) / len(results):.2f}"
    )

    # Group by depth
    depth_counts = {}
    for result in results:
        depth = result.metadata.get("depth", 0)
        depth_counts[depth] = depth_counts.get(depth, 0) + 1

    print("\n Pages crawled by depth:")
    for depth, count in sorted(depth_counts.items()):
        print(f"  Depth {depth}: {count} pages")
```

----------------------------------------

TITLE: Mounting Custom Config in Docker Compose using YAML
DESCRIPTION: Provides a YAML snippet for a `docker-compose.yml` file. It shows how to configure a service (e.g., `crawl4ai-hub-amd64`) to use a custom configuration file by adding a `volumes` section. The line `- ./my-custom-config.yml:/app/config.yml` mounts the local configuration file over the default one within the container. It also notes the need to keep existing volume mounts (like shared memory). Requires Docker Compose and the custom config file in the same directory as the compose file.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_35

LANGUAGE: yaml
CODE:
```
services:
  crawl4ai-hub-amd64: # Or your chosen service
    image: unclecode/crawl4ai:latest
    profiles: ["hub-amd64"]
    <<: *base-config
    volumes:
      # Mount local custom config over the default one in the container
      - ./my-custom-config.yml:/app/config.yml
      # Keep the shared memory volume from base-config
      - /dev/shm:/dev/shm
```

----------------------------------------

TITLE: Migrating to AsyncWebCrawler CacheMode (New)
DESCRIPTION: This snippet demonstrates the recommended new way to control cache behavior in AsyncWebCrawler using the `cache_mode` parameter with the `CacheMode` enum. This replaces the deprecated boolean flags for cache control in the `arun` method.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#_snippet_6

LANGUAGE: python
CODE:
```
from crawl4ai import CacheMode

crawler = AsyncWebCrawler(always_bypass_cache=True)
result = await crawler.arun(url="https://example.com", cache_mode=CacheMode.BYPASS)
```

----------------------------------------

TITLE: Accessing Page Metadata (Python)
DESCRIPTION: Demonstrates how to check if page-level metadata is available in the `metadata` field and print specific metadata values like title or author. This field contains a dictionary of discovered metadata.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_19

LANGUAGE: python
CODE:
```
if result.metadata:
    print("Title:", result.metadata.get("title"))
    print("Author:", result.metadata.get("author"))
```

----------------------------------------

TITLE: Advanced Schema with Nested Structures for E-commerce Data Extraction
DESCRIPTION: Shows how to define a complex schema with nested and list-type fields for extracting hierarchical data from e-commerce websites. The schema captures categories, products, features, reviews, and related items using a structured approach.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_187

LANGUAGE: python
CODE:
```
schema = {
    "name": "E-commerce Product Catalog",
    "baseSelector": "div.category",
    # (1) We can define optional baseFields if we want to extract attributes 
    # from the category container
    "baseFields": [
        {"name": "data_cat_id", "type": "attribute", "attribute": "data-cat-id"}, 
    ],
    "fields": [
        {
            "name": "category_name",
            "selector": "h2.category-name",
            "type": "text"
        },
        {
            "name": "products",
            "selector": "div.product",
            "type": "nested_list",    # repeated sub-objects
            "fields": [
                {
                    "name": "name",
                    "selector": "h3.product-name",
                    "type": "text"
                },
                {
                    "name": "price",
                    "selector": "p.product-price",
                    "type": "text"
                },
                {
                    "name": "details",
                    "selector": "div.product-details",
                    "type": "nested",  # single sub-object
                    "fields": [
                        {
                            "name": "brand",
                            "selector": "span.brand",
                            "type": "text"
                        },
                        {
                            "name": "model",
                            "selector": "span.model",
                            "type": "text"
                        }
                    ]
                },
                {
                    "name": "features",
                    "selector": "ul.product-features li",
                    "type": "list",
                    "fields": [
                        {"name": "feature", "type": "text"} 
                    ]
                },
                {
                    "name": "reviews",
                    "selector": "div.review",
                    "type": "nested_list",
                    "fields": [
                        {
                            "name": "reviewer", 
                            "selector": "span.reviewer", 
                            "type": "text"
                        },
                        {
                            "name": "rating", 
                            "selector": "span.rating", 
                            "type": "text"
                        },
                        {
                            "name": "comment", 
                            "selector": "p.review-text", 
                            "type": "text"
                        }
                    ]
                },
                {
                    "name": "related_products",
                    "selector": "ul.related-products li",
                    "type": "list",
                    "fields": [
                        {
                            "name": "name", 
                            "selector": "span.related-name", 
                            "type": "text"
                        },
                        {
                            "name": "price", 
                            "selector": "span.related-price", 
                            "type": "text"
                        }
                    ]
                }
            ]
        }
    ]
}
```

----------------------------------------

TITLE: CrawlResult and MarkdownGenerationResult Model Definitions
DESCRIPTION: This code snippet defines the core schema for CrawlResult and MarkdownGenerationResult models in Crawl4AI. It shows all available fields that capture different aspects of a crawl's result, including HTML variants, markdown generation, and structured extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_23

LANGUAGE: python
CODE:
```
class MarkdownGenerationResult(BaseModel):
    raw_markdown: str
    markdown_with_citations: str
    references_markdown: str
    fit_markdown: Optional[str] = None
    fit_html: Optional[str] = None

class CrawlResult(BaseModel):
    url: str
    html: str
    success: bool
    cleaned_html: Optional[str] = None
    media: Dict[str, List[Dict]] = {}
    links: Dict[str, List[Dict]] = {}
    downloaded_files: Optional[List[str]] = None
    screenshot: Optional[str] = None
    pdf : Optional[bytes] = None
    mhtml: Optional[str] = None
    markdown: Optional[Union[str, MarkdownGenerationResult]] = None
    extracted_content: Optional[str] = None
    metadata: Optional[dict] = None
    error_message: Optional[str] = None
    session_id: Optional[str] = None
    response_headers: Optional[dict] = None
    status_code: Optional[int] = None
    ssl_certificate: Optional[SSLCertificate] = None
    class Config:
        arbitrary_types_allowed = True
```

----------------------------------------

TITLE: Implementing AsyncWebCrawler Hooks with Authentication
DESCRIPTION: Complete example demonstrating the implementation of all available hooks in AsyncWebCrawler, including browser configuration, crawler setup, and hook definitions for authentication and customization. Shows proper hook usage for route filtering, viewport adjustment, and page navigation monitoring.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_135

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from playwright.async_api import Page, BrowserContext

async def main():
    print(" Hooks Example: Demonstrating recommended usage")

    # 1) Configure the browser
    browser_config = BrowserConfig(
        headless=True,
        verbose=True
    )

    # 2) Configure the crawler run
    crawler_run_config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);",
        wait_for="body",
        cache_mode=CacheMode.BYPASS
    )

    # 3) Create the crawler instance
    crawler = AsyncWebCrawler(config=browser_config)

    #
    # Define Hook Functions
    #

    async def on_browser_created(browser, **kwargs):
        # Called once the browser instance is created (but no pages or contexts yet)
        print("[HOOK] on_browser_created - Browser created successfully!")
        # Typically, do minimal setup here if needed
        return browser

    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):
        # Called right after a new page + context are created (ideal for auth or route config).
        print("[HOOK] on_page_context_created - Setting up page & context.")
        
        # Example 1: Route filtering (e.g., block images)
        async def route_filter(route):
            if route.request.resource_type == "image":
                print(f"[HOOK] Blocking image request: {route.request.url}")
                await route.abort()
            else:
                await route.continue_()

        await context.route("**", route_filter)

        # Example 2: (Optional) Simulate a login scenario
        # Example 3: Adjust the viewport
        await page.set_viewport_size({"width": 1080, "height": 600})
        return page

    async def before_goto(
        page: Page, context: BrowserContext, url: str, **kwargs
    ):
        print(f"[HOOK] before_goto - About to navigate: {url}")
        await page.set_extra_http_headers({
            "Custom-Header": "my-value"
        })
        return page

    async def after_goto(
        page: Page, context: BrowserContext, 
        url: str, response, **kwargs
    ):
        print(f"[HOOK] after_goto - Successfully loaded: {url}")
        try:
            await page.wait_for_selector('.content', timeout=1000)
            print("[HOOK] Found .content element!")
        except:
            print("[HOOK] .content not found, continuing anyway.")
        return page

    async def on_user_agent_updated(
        page: Page, context: BrowserContext, 
        user_agent: str, **kwargs
    ):
        print(f"[HOOK] on_user_agent_updated - New user agent: {user_agent}")
        return page

    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):
        print("[HOOK] on_execution_started - JS code is running!")
        return page

    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):
        print("[HOOK] before_retrieve_html - We can do final actions")
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
        return page

    async def before_return_html(
        page: Page, context: BrowserContext, html: str, **kwargs
    ):
        print(f"[HOOK] before_return_html - HTML length: {len(html)}")
        return page

    crawler.crawler_strategy.set_hook("on_browser_created", on_browser_created)
    crawler.crawler_strategy.set_hook(
        "on_page_context_created", on_page_context_created
    )
    crawler.crawler_strategy.set_hook("before_goto", before_goto)
    crawler.crawler_strategy.set_hook("after_goto", after_goto)
    crawler.crawler_strategy.set_hook(
        "on_user_agent_updated", on_user_agent_updated
    )
    crawler.crawler_strategy.set_hook(
        "on_execution_started", on_execution_started
    )
    crawler.crawler_strategy.set_hook(
        "before_retrieve_html", before_retrieve_html
    )
    crawler.crawler_strategy.set_hook(
        "before_return_html", before_return_html
    )

    await crawler.start()

    # 4) Run the crawler on an example page
    url = "https://example.com"
    result = await crawler.arun(url, config=crawler_run_config)
    
    if result.success:
        print("\nCrawled URL:", result.url)
        print("HTML length:", len(result.html))
    else:
        print("Error:", result.error_message)

    await crawler.close()

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Example Structure of result.links (Python)
DESCRIPTION: This snippet illustrates the typical dictionary structure of the `result.links` attribute in a `CrawlResult` object. It shows how internal and external links are grouped, with each link represented as a dictionary containing keys like `href`, `text`, `title`, and `base_domain`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_3

LANGUAGE: python
CODE:
```
result.links = {
  "internal": [
    {
      "href": "https://kidocode.com/",
      "text": "",
      "title": "",
      "base_domain": "kidocode.com"
    },
    {
      "href": "https://kidocode.com/degrees/technology",
      "text": "Technology Degree",
      "title": "KidoCode Tech Program",
      "base_domain": "kidocode.com"
    },
    # ...
  ],
  "external": [
    # possibly other links leading to third-party sites
  ]
}
```

----------------------------------------

TITLE: Mounting Custom Config with Docker Compose YAML
DESCRIPTION: Shows how to add a `volumes` directive to a service definition in a `docker-compose.yml` file to mount a local custom `config.yml` file into the Crawl4AI container. This is the recommended method for custom configurations in Docker Compose deployments.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_36

LANGUAGE: yaml
CODE:
```
services:
  crawl4ai-hub-amd64: # Or your chosen service
    image: unclecode/crawl4ai:latest
    profiles: ["hub-amd64"]
    <<: *base-config
    volumes:
      # Mount local custom config over the default one in the container
      - ./my-custom-config.yml:/app/config.yml
      # Keep the shared memory volume from base-config
      - /dev/shm:/dev/shm
```

----------------------------------------

TITLE: Loading Extracted Structured Content (Python)
DESCRIPTION: Demonstrates how to check if structured content was extracted and, if so, parse it as JSON from the `extracted_content` field of a `CrawlResult`. This field is populated when an `extraction_strategy` is used.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_14

LANGUAGE: python
CODE:
```
if result.extracted_content:
    data = json.loads(result.extracted_content)
    print(data)
```

----------------------------------------

TITLE: Form Interaction with Crawl4AI
DESCRIPTION: Shows how to interact with web forms by filling fields and submitting them using JavaScript execution in Crawl4AI. This example demonstrates a hypothetical search on GitHub.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_4

LANGUAGE: javascript
CODE:
```
js_form_interaction = """
document.querySelector('#your-search').value = 'TypeScript commits';
document.querySelector('form').submit();
"""

config = CrawlerRunConfig(
    js_code=js_form_interaction,
    wait_for="css:.commit"
)
result = await crawler.arun(url="https://github.com/search", config=config)
```

----------------------------------------

TITLE: Implementing Hooks in AsyncWebCrawler with Python
DESCRIPTION: A comprehensive example demonstrating how to configure and implement all available hooks in Crawl4AI's AsyncWebCrawler. The code shows how to set up browser configuration, define hook functions for different stages of the crawling process, attach hooks to the crawler, and execute a crawl operation.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/hooks-auth.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from playwright.async_api import Page, BrowserContext

async def main():
    print(" Hooks Example: Demonstrating recommended usage")

    # 1) Configure the browser
    browser_config = BrowserConfig(
        headless=True,
        verbose=True
    )

    # 2) Configure the crawler run
    crawler_run_config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);",
        wait_for="body",
        cache_mode=CacheMode.BYPASS
    )

    # 3) Create the crawler instance
    crawler = AsyncWebCrawler(config=browser_config)

    #
    # Define Hook Functions
    #

    async def on_browser_created(browser, **kwargs):
        # Called once the browser instance is created (but no pages or contexts yet)
        print("[HOOK] on_browser_created - Browser created successfully!")
        # Typically, do minimal setup here if needed
        return browser

    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):
        # Called right after a new page + context are created (ideal for auth or route config).
        print("[HOOK] on_page_context_created - Setting up page & context.")
        
        # Example 1: Route filtering (e.g., block images)
        async def route_filter(route):
            if route.request.resource_type == "image":
                print(f"[HOOK] Blocking image request: {route.request.url}")
                await route.abort()
            else:
                await route.continue_()

        await context.route("**", route_filter)

        # Example 2: (Optional) Simulate a login scenario
        # (We do NOT create or close pages here, just do quick steps if needed)
        # e.g., await page.goto("https://example.com/login")
        # e.g., await page.fill("input[name='username']", "testuser")
        # e.g., await page.fill("input[name='password']", "password123")
        # e.g., await page.click("button[type='submit']")
        # e.g., await page.wait_for_selector("#welcome")
        # e.g., await context.add_cookies([...])
        # Then continue

        # Example 3: Adjust the viewport
        await page.set_viewport_size({"width": 1080, "height": 600})
        return page

    async def before_goto(
        page: Page, context: BrowserContext, url: str, **kwargs
    ):
        # Called before navigating to each URL.
        print(f"[HOOK] before_goto - About to navigate: {url}")
        # e.g., inject custom headers
        await page.set_extra_http_headers({
            "Custom-Header": "my-value"
        })
        return page

    async def after_goto(
        page: Page, context: BrowserContext, 
        url: str, response, **kwargs
    ):
        # Called after navigation completes.
        print(f"[HOOK] after_goto - Successfully loaded: {url}")
        # e.g., wait for a certain element if we want to verify
        try:
            await page.wait_for_selector('.content', timeout=1000)
            print("[HOOK] Found .content element!")
        except:
            print("[HOOK] .content not found, continuing anyway.")
        return page

    async def on_user_agent_updated(
        page: Page, context: BrowserContext, 
        user_agent: str, **kwargs
    ):
        # Called whenever the user agent updates.
        print(f"[HOOK] on_user_agent_updated - New user agent: {user_agent}")
        return page

    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):
        # Called after custom JavaScript execution begins.
        print("[HOOK] on_execution_started - JS code is running!")
        return page

    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):
        # Called before final HTML retrieval.
        print("[HOOK] before_retrieve_html - We can do final actions")
        # Example: Scroll again
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
        return page

    async def before_return_html(
        page: Page, context: BrowserContext, html: str, **kwargs
    ):
        # Called just before returning the HTML in the result.
        print(f"[HOOK] before_return_html - HTML length: {len(html)}")
        return page

    #
    # Attach Hooks
    #

    crawler.crawler_strategy.set_hook("on_browser_created", on_browser_created)
    crawler.crawler_strategy.set_hook(
        "on_page_context_created", on_page_context_created
    )
    crawler.crawler_strategy.set_hook("before_goto", before_goto)
    crawler.crawler_strategy.set_hook("after_goto", after_goto)
    crawler.crawler_strategy.set_hook(
        "on_user_agent_updated", on_user_agent_updated
    )
    crawler.crawler_strategy.set_hook(
        "on_execution_started", on_execution_started
    )
    crawler.crawler_strategy.set_hook(
        "before_retrieve_html", before_retrieve_html
    )
    crawler.crawler_strategy.set_hook(
        "before_return_html", before_return_html
    )

    await crawler.start()

    # 4) Run the crawler on an example page
    url = "https://example.com"
    result = await crawler.arun(url, config=crawler_run_config)
    
    if result.success:
        print("\nCrawled URL:", result.url)
        print("HTML length:", len(result.html))
    else:
        print("Error:", result.error_message)

    await crawler.close()

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Executing Deep Crawl and Processing Results in Python
DESCRIPTION: This snippet shows how to execute a deep crawl using the AsyncWebCrawler, process the results, and group them by depth to visualize the crawl tree structure. It also includes performance measurement.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_206

LANGUAGE: python
CODE:
```
async with AsyncWebCrawler() as crawler:
    start_time = time.perf_counter()
    results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

    # Group results by depth to visualize the crawl tree
    pages_by_depth = {}
    for result in results:
        depth = result.metadata.get("depth", 0)
        if depth not in pages_by_depth:
            pages_by_depth[depth] = []
        pages_by_depth[depth].append(result.url)

    print(f" Crawled {len(results)} pages total")

    # Display crawl structure by depth
    for depth, urls in sorted(pages_by_depth.items()):
        print(f"\nDepth {depth}: {len(urls)} pages")
        # Show first 3 URLs for each depth as examples
        for url in urls[:3]:
            print(f"   {url}")
        if len(urls) > 3:
            print(f"  ... and {len(urls) - 3} more")

    print(
        f"\n Performance: {len(results)} pages in {time.perf_counter() - start_time:.2f} seconds"
    )
```

----------------------------------------

TITLE: JavaScript Interaction for Dynamic Content Loading in Python
DESCRIPTION: This function demonstrates how to interact with JavaScript on a web page to load more content. It crawls Hacker News, extracts initial news items, clicks the 'More' link, and extracts additional items.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_153

LANGUAGE: python
CODE:
```
async def demo_js_interaction():
    """Execute JavaScript to load more content"""
    print("\n=== 7. JavaScript Interaction ===")

    # A simple page that needs JS to reveal content
    async with AsyncWebCrawler(config=BrowserConfig(headless=False)) as crawler:
        # Initial load

        news_schema = {
            "name": "news",
            "baseSelector": "tr.athing",
            "fields": [
                {
                    "name": "title",
                    "selector": "span.titleline",
                    "type": "text",
                }
            ],
        }
        results: List[CrawlResult] = await crawler.arun(
            url="https://news.ycombinator.com",
            config=CrawlerRunConfig(
                session_id="hn_session",  # Keep session
                extraction_strategy=JsonCssExtractionStrategy(schema=news_schema),
            ),
        )

        news = []
        for result in results:
            if result.success:
                data = json.loads(result.extracted_content)
                news.extend(data)
                print(json.dumps(data, indent=2))
            else:
                print("Failed to extract structured data")

        print(f"Initial items: {len(news)}")

        # Click "More" link
        more_config = CrawlerRunConfig(
            js_code="document.querySelector('a.morelink').click();",
            js_only=True,  # Continue in same page
            session_id="hn_session",  # Keep session
            extraction_strategy=JsonCssExtractionStrategy(
                schema=news_schema,
            ),
        )

        result: List[CrawlResult] = await crawler.arun(
            url="https://news.ycombinator.com", config=more_config
        )

        # Extract new items
        for result in results:
            if result.success:
                data = json.loads(result.extracted_content)
                news.extend(data)
                print(json.dumps(data, indent=2))
            else:
                print("Failed to extract structured data")
        print(f"Total items: {len(news)}")
```

----------------------------------------

TITLE: Defining arun_many() Function for Concurrent URL Crawling in Python
DESCRIPTION: This code snippet defines the arun_many() function, which is used to crawl multiple URLs concurrently or in batches. It takes a list of URLs, an optional configuration, and an optional dispatcher for concurrency control.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
async def arun_many(
    urls: Union[List[str], List[Any]],
    config: Optional[CrawlerRunConfig] = None,
    dispatcher: Optional[BaseDispatcher] = None,
    ...
) -> Union[List[CrawlResult], AsyncGenerator[CrawlResult, None]]:
    """
    Crawl multiple URLs concurrently or in batches.

    :param urls: A list of URLs (or tasks) to crawl.
    :param config: (Optional) A default `CrawlerRunConfig` applying to each crawl.
    :param dispatcher: (Optional) A concurrency controller (e.g. MemoryAdaptiveDispatcher).
    ...
    :return: Either a list of `CrawlResult` objects, or an async generator if streaming is enabled.
    """
```

----------------------------------------

TITLE: Extracting Common Entities with RegexExtractionStrategy (Python)
DESCRIPTION: Provides a simple example of using RegexExtractionStrategy to extract predefined common data types like URLs and currencies from a webpage. It shows how to select built-in patterns and process the structured output.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_4

LANGUAGE: python
CODE:
```
import json
import asyncio
from crawl4ai import (
    AsyncWebCrawler,
    CrawlerRunConfig,
    RegexExtractionStrategy
)

async def extract_with_regex():
    # Create a strategy using built-in patterns for URLs and currencies
    strategy = RegexExtractionStrategy(
        pattern = RegexExtractionStrategy.Url | RegexExtractionStrategy.Currency
    )
    
    config = CrawlerRunConfig(extraction_strategy=strategy)
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://example.com",
            config=config
        )
        
        if result.success:
            data = json.loads(result.extracted_content)
            for item in data[:5]:  # Show first 5 matches
                print(f"{item['label']}: {item['value']}")
            print(f"Total matches: {len(data)}")

asyncio.run(extract_with_regex())
```

----------------------------------------

TITLE: Content Filtering Configuration in YAML for Crawl4AI
DESCRIPTION: Defines two content filtering approaches: BM25 for relevance filtering based on a query and threshold, and pruning for focusing on specific topics.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_13

LANGUAGE: yaml
CODE:
```
# filter_bm25.yml
type: "bm25"
query: "target content"
threshold: 1.0

# filter_pruning.yml
type: "pruning"
query: "focus topic"
threshold: 0.48
```

----------------------------------------

TITLE: Handling Media Content in Crawl4AI
DESCRIPTION: This function shows how to handle and extract image data from a webpage. It configures the crawler to exclude external images, capture screenshots, and then displays information about the first five images found on the page, including their URLs, alt text, and scores.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_132

LANGUAGE: python
CODE:
```
async def media_handling():
    crawler_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS, exclude_external_images=True, screenshot=True
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business", config=crawler_config
        )
        for img in result.media["images"][:5]:
            print(f"Image URL: {img['src']}, Alt: {img['alt']}, Score: {img['score']}")
```

----------------------------------------

TITLE: Handling Mixed Content Pages with CosineStrategy in Python
DESCRIPTION: Shows configuration for extracting content from pages with diverse content types. Uses a more flexible similarity threshold, larger clusters via max_dist, and retrieves multiple relevant sections.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_180

LANGUAGE: python
CODE:
```
# For mixed content pages
strategy = CosineStrategy(
    semantic_filter="product features",
    sim_threshold=0.4,      # More flexible matching
    max_dist=0.3,          # Larger clusters
    top_k=3                # Multiple relevant sections
)
```

----------------------------------------

TITLE: Using Proxy Configurations with Crawl4AI
DESCRIPTION: This function shows how to configure and use a proxy with the Crawl4AI crawler. It sets up a browser configuration with proxy settings including server URL, username, and password for authenticated proxy access.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_134

LANGUAGE: python
CODE:
```
async def use_proxy():
    print("\n--- Using a Proxy ---")
    browser_config = BrowserConfig(
        headless=True,
        proxy_config={
            "server": "http://proxy.example.com:8080",
            "username": "username",
            "password": "password",
        },
    )
    crawler_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(
            url="https://www.nbcnews.com/business", config=crawler_config
        )
        if result.success:
            print(result.markdown[:500])
```

----------------------------------------

TITLE: JavaScript Execution Configuration
DESCRIPTION: Settings for executing JavaScript code during crawling sessions.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
run_config = CrawlerRunConfig(
    js_code=[
        "window.scrollTo(0, document.body.scrollHeight);",
        "document.querySelector('.load-more')?.click();"
    ],
    js_only=False
)
```

----------------------------------------

TITLE: Content Selection Configuration
DESCRIPTION: Settings for selecting specific content regions and excluding unwanted elements using CSS selectors.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
run_config = CrawlerRunConfig(
    css_selector=".main-content",  # Focus on .main-content region only
    excluded_tags=["form", "nav"], # Remove entire tag blocks
    remove_forms=True,             # Specifically strip <form> elements
    remove_overlay_elements=True,  # Attempt to remove modals/popups
)
```

----------------------------------------

TITLE: Running Crawl4AI Docker Container with Custom Configuration
DESCRIPTION: This bash command shows how to run the Crawl4AI Docker container with a custom configuration file mounted at runtime.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_71

LANGUAGE: bash
CODE:
```
docker run -d -p 8000:8000 \
  -v $(pwd)/custom-config.yml:/app/config.yml \
  crawl4ai-server:prod
```

----------------------------------------

TITLE: Error Handling for Web Crawler Exception Handling
DESCRIPTION: This code implements error handling for the web crawler, capturing detailed information about exceptions including line numbers, function names, and code context. It formats error messages into a structured format and returns appropriate error results through the CrawlResultContainer.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_25

LANGUAGE: python
CODE:
```
except Exception as e:
    error_context = get_error_context(sys.exc_info())

    error_message = (
        f"Unexpected error in _crawl_web at line {error_context['line_no']} "
        f"in {error_context['function']} ({error_context['filename']}):\n"
        f"Error: {str(e)}\n\n"
        f"Code context:\n{error_context['code_context']}"
    )

    self.logger.error_status(
        url=url,
        error=create_box_message(error_message, type="error"),
        tag="ERROR",
    )

    return CrawlResultContainer(
        CrawlResult(
            url=url, html="", success=False, error_message=error_message
        )
    )
```

----------------------------------------

TITLE: Configuring and Using CosineStrategy for Web Content Extraction in Python
DESCRIPTION: Demonstrates how to instantiate and configure the CosineStrategy class for web content extraction based on semantic similarity. The example shows basic usage with an AsyncWebCrawler to extract content from a URL.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_169

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import CosineStrategy

strategy = CosineStrategy(
    semantic_filter="product reviews",    # Target content type
    word_count_threshold=10,             # Minimum words per cluster
    sim_threshold=0.3                    # Similarity threshold
)

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(
        url="https://example.com/reviews",
        extraction_strategy=strategy
    )
    
    content = result.extracted_content
```

----------------------------------------

TITLE: Deep Web Crawling with BFS Strategy in Python
DESCRIPTION: This function demonstrates deep crawling using a Breadth-First Search (BFS) strategy. It crawls the crawl4ai.com domain up to a depth of 1 and a maximum of 5 pages.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_152

LANGUAGE: python
CODE:
```
async def demo_deep_crawl():
    """Deep crawling with BFS strategy"""
    print("\n=== 6. Deep Crawling ===")

    filter_chain = FilterChain([DomainFilter(allowed_domains=["crawl4ai.com"])])

    deep_crawl_strategy = BFSDeepCrawlStrategy(
        max_depth=1, max_pages=5, filter_chain=filter_chain
    )

    async with AsyncWebCrawler() as crawler:
        results: List[CrawlResult] = await crawler.arun(
            url="https://docs.crawl4ai.com",
            config=CrawlerRunConfig(deep_crawl_strategy=deep_crawl_strategy),
        )

        print(f"Deep crawl returned {len(results)} pages:")
        for i, result in enumerate(results):
            depth = result.metadata.get("depth", "unknown")
            print(f"  {i + 1}. {result.url} (Depth: {depth})")
```

----------------------------------------

TITLE: Implementing Custom Scraping Strategy in Python
DESCRIPTION: This code demonstrates how to create a custom scraping strategy by inheriting from ContentScrapingStrategy. It includes the structure for implementing custom scraping logic and returning a ScrapingResult object with cleaned HTML, media items, links, and metadata.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
from crawl4ai import ContentScrapingStrategy, ScrapingResult, MediaItem, Media, Link, Links

class CustomScrapingStrategy(ContentScrapingStrategy):
    def scrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        # Implement your custom scraping logic here
        return ScrapingResult(
            cleaned_html="<html>...</html>",  # Cleaned HTML content
            success=True,                     # Whether scraping was successful
            media=Media(
                images=[                      # List of images found
                    MediaItem(
                        src="https://example.com/image.jpg",
                        alt="Image description",
                        desc="Surrounding text",
                        score=1,
                        type="image",
                        group_id=1,
                        format="jpg",
                        width=800
                    )
                ],
                videos=[],                    # List of videos (same structure as images)
                audios=[]                     # List of audio files (same structure as images)
            ),
            links=Links(
                internal=[                    # List of internal links
                    Link(
                        href="https://example.com/page",
                        text="Link text",
                        title="Link title",
                        base_domain="example.com"
                    )
                ],
                external=[]                   # List of external links (same structure)
            ),
            metadata={                        # Additional metadata
                "title": "Page Title",
                "description": "Page description"
            }
        )

    async def ascrap(self, url: str, html: str, **kwargs) -> ScrapingResult:
        # For simple cases, you can use the sync version
        return await asyncio.to_thread(self.scrap, url, html, **kwargs)
```

----------------------------------------

TITLE: Reusing Browser Sessions for Efficient Crawling
DESCRIPTION: This code shows how to create and reuse browser sessions across multiple crawl operations, which improves efficiency by avoiding the creation of new browser tabs for each crawled page, reducing memory usage and setup time.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
session_id = await crawler.create_session()

# Use the same session for multiple crawls
await crawler.crawl(
    url="https://example.com/page1",
    session_id=session_id  # Reuse the session
)
await crawler.crawl(
    url="https://example.com/page2",
    session_id=session_id
)
```

----------------------------------------

TITLE: Configuring LLM Extraction Strategy for Crawl4AI
DESCRIPTION: Shows how to set up an LLM (Language Model) extraction strategy in Crawl4AI, including instructions, provider details, and schema definition for extracted content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_60

LANGUAGE: json
CODE:
```
{
  "crawler_config": {
    "type": "CrawlerRunConfig",
    "params": {
      "extraction_strategy": {
        "type": "LLMExtractionStrategy",
        "params": {
          "instruction": "Extract article title, author, publication date and main content",
          "provider": "openai/gpt-4",
          "api_token": "your-api-token",
          "schema": {
            "type": "dict",
            "value": {
              "title": "Article Schema",
              "type": "object",
              "properties": {
                "title": {
                  "type": "string",
                  "description": "The article's headline"
                },
                "author": {
                  "type": "string",
                  "description": "The author's name"
                },
                "published_date": {
                  "type": "string",
                  "format": "date-time",
                  "description": "Publication date and time"
                },
                "content": {
                  "type": "string",
                  "description": "The main article content"
                }
              },
              "required": ["title", "content"]
            }
          }
        }
      }
    }
  }
}
```

----------------------------------------

TITLE: Structured Data Extraction with LLM Command for Crawl4AI
DESCRIPTION: Shows how to extract structured data using LLM-based extraction with configuration and schema files.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_12

LANGUAGE: bash
CODE:
```
crwl https://example.com \
    -e extract_llm.yml \
    -s llm_schema.json \
    -o json
```

----------------------------------------

TITLE: Advanced Crawler4ai Configuration with JSON
DESCRIPTION: Example of a JSON configuration file that can be used with Crawler4ai to set various crawling parameters and options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_2

LANGUAGE: json
CODE:
```
{
    "urls": ["https://example.com"],
    "max_pages": 100,
    "max_depth": 3,
    "respect_robots_txt": true,
    "crawl_delay": 1.0,
    "timeout": 30,
    "user_agent": "Crawler4ai/1.0",
    "output_dir": "crawl_results",
    "extract": {
        "title": true,
        "text": true,
        "links": true,
        "images": false,
        "metadata": true
    },
    "exclude_patterns": [
        ".*\\.pdf$",
        ".*\\.jpg$",
        "/login/",
        "/logout/"
    ]
}
```

----------------------------------------

TITLE: Running Crawler4ai from Command Line
DESCRIPTION: Command line example showing how to use the Crawler4ai CLI tool to start a crawl operation with a configuration file.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_6

LANGUAGE: bash
CODE:
```
crawler4ai --config config.json
```

----------------------------------------

TITLE: LLM Q&A Command Line Examples for Crawl4AI
DESCRIPTION: Demonstrates how to use the LLM Q&A feature to ask questions about crawled content, including simple questions, multi-step workflows, and combined approaches.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_10

LANGUAGE: bash
CODE:
```
# Simple question
crwl https://example.com -q "What is the main topic discussed?"

# View content then ask questions
crwl https://example.com -o markdown  # See content first
crwl https://example.com -q "Summarize the key points"
crwl https://example.com -q "What are the conclusions?"

# Combined with advanced crawling
crwl https://example.com \
    -B browser.yml \
    -c "css_selector=article,scan_full_page=true" \
    -q "What are the pros and cons mentioned?"
```

----------------------------------------

TITLE: Extracting Article Data with CSS Selectors and JSON Extraction Strategy in Python
DESCRIPTION: This function demonstrates how to use CSS selection, exclusion logic, and a JSON-based extraction strategy to extract article data from a webpage. It uses AsyncWebCrawler with custom configuration to filter content and extract structured data.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_9

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_main_articles(url: str):
    schema = {
        "name": "ArticleBlock",
        "baseSelector": "div.article-block",
        "fields": [
            {"name": "headline", "selector": "h2", "type": "text"},
            {"name": "summary", "selector": ".summary", "type": "text"},
            {
                "name": "metadata",
                "type": "nested",
                "fields": [
                    {"name": "author", "selector": ".author", "type": "text"},
                    {"name": "date", "selector": ".date", "type": "text"}
                ]
            }
        ]
    }

    config = CrawlerRunConfig(
        # Keep only #main-content
        css_selector="#main-content",
        
        # Filtering
        word_count_threshold=10,
        excluded_tags=["nav", "footer"],  
        exclude_external_links=True,
        exclude_domains=["somebadsite.com"],
        exclude_external_images=True,

        # Extraction
        extraction_strategy=JsonCssExtractionStrategy(schema),
        
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        if not result.success:
            print(f"Error: {result.error_message}")
            return None
        return json.loads(result.extracted_content)

async def main():
    articles = await extract_main_articles("https://news.ycombinator.com/newest")
    if articles:
        print("Extracted Articles:", articles[:2])  # Show first 2

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Crawling Local Files and Raw HTML in crawl4ai (Python)
DESCRIPTION: Illustrates how to use AsyncWebCrawler to process content from local files or raw HTML strings. It shows constructing the URL with 'file://' or 'raw:' prefixes and running the crawler.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/CHANGELOG.md#_snippet_3

LANGUAGE: python
CODE:
```
async def crawl_local_or_raw(crawler, content, content_type):
    prefix = "file://" if content_type == "local" else "raw:"
    url = f"{prefix}{content}"
    result = await crawler.arun(url=url)
    if result.success:
        print(f"Markdown Content from {content_type.title()} Source:")
        print(result.markdown)

# Example usage with local file and raw HTML
async def main():
    async with AsyncWebCrawler() as crawler:
        # Local File
        await crawl_local_or_raw(
            crawler, os.path.abspath('tests/async/sample_wikipedia.html'), "local"
        )
        # Raw HTML
        await crawl_raw_html(crawler, "<h1>Raw Test</h1><p>This is raw HTML.</p>")
        

asyncio.run(main())
```

----------------------------------------

TITLE: Streaming Mode Execution for BFS Web Crawler in Python
DESCRIPTION: Implements streaming mode execution for the BFS crawler. This method processes URLs level by level but yields results immediately as they arrive, enabling real-time processing of crawl results while maintaining BFS traversal order.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_180

LANGUAGE: python
CODE:
```
async def _arun_stream(
    self,
    start_url: str,
    crawler: AsyncWebCrawler,
    config: CrawlerRunConfig,
) -> AsyncGenerator[CrawlResult, None]:
    """
    Streaming mode:
    Processes one BFS level at a time and yields results immediately as they arrive.
    """
    visited: Set[str] = set()
    current_level: List[Tuple[str, Optional[str]]] = [(start_url, None)]
    depths: Dict[str, int] = {start_url: 0}

    while current_level and not self._cancel_event.is_set():
        next_level: List[Tuple[str, Optional[str]]] = []
        urls = [url for url, _ in current_level]
        visited.update(urls)

        stream_config = config.clone(deep_crawl_strategy=None, stream=True)
        stream_gen = await crawler.arun_many(urls=urls, config=stream_config)
        
        # Keep track of processed results for this batch
        results_count = 0
        async for result in stream_gen:
            url = result.url
            depth = depths.get(url, 0)
            result.metadata = result.metadata or {}
            result.metadata["depth"] = depth
            parent_url = next((parent for (u, parent) in current_level if u == url), None)
            result.metadata["parent_url"] = parent_url
            
            # Count only successful crawls
            if result.success:
                self._pages_crawled += 1
            
            results_count += 1
            yield result
            
            # Only discover links from successful crawls
            if result.success:
                # Link discovery will handle the max pages limit internally
                await self.link_discovery(result, url, depth, visited, next_level, depths)
        
        # If we didn't get results back (e.g. due to errors), avoid getting stuck in an infinite loop
        # by considering these URLs as visited but not counting them toward the max_pages limit
        if results_count == 0 and urls:
            self.logger.warning(f"No results returned for {len(urls)} URLs, marking as visited")
            
        current_level = next_level
```

----------------------------------------

TITLE: Defining AsyncCrawlResponse Model
DESCRIPTION: Defines a Pydantic model for asynchronous crawl responses that stores HTML content, response headers, and other crawl-related data. It includes support for various response types like screenshots, PDF data, and network requests.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_87

LANGUAGE: python
CODE:
```
class AsyncCrawlResponse(BaseModel):
    html: str
    response_headers: Dict[str, str]
    js_execution_result: Optional[Dict[str, Any]] = None
    status_code: int
    screenshot: Optional[str] = None
    pdf_data: Optional[bytes] = None
    mhtml_data: Optional[str] = None
    get_delayed_content: Optional[Callable[[Optional[float]], Awaitable[str]]] = None
    downloaded_files: Optional[List[str]] = None
    ssl_certificate: Optional[SSLCertificate] = None
    redirected_url: Optional[str] = None
    network_requests: Optional[List[Dict[str, Any]]] = None
    console_messages: Optional[List[Dict[str, Any]]] = None

    class Config:
        arbitrary_types_allowed = True
```

----------------------------------------

TITLE: Session Persistence and Local Storage in Crawl4AI
DESCRIPTION: Shows how to use storage_state in Crawl4AI to preserve cookies and localStorage, allowing for continued sessions and skipping repeated authentication flows. It includes an example of providing pre-set storage state.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    storage_dict = {
        "cookies": [
            {
                "name": "session",
                "value": "abcd1234",
                "domain": "example.com",
                "path": "/",
                "expires": 1699999999.0,
                "httpOnly": False,
                "secure": False,
                "sameSite": "None"
            }
        ],
        "origins": [
            {
                "origin": "https://example.com",
                "localStorage": [
                    {"name": "token", "value": "my_auth_token"}
                ]
            }
        ]
    }

    # Provide the storage state as a dictionary to start "already logged in"
    async with AsyncWebCrawler(
        headless=True,
        storage_state=storage_dict
    ) as crawler:
        result = await crawler.arun("https://example.com/protected")
        if result.success:
            print("Protected page content length:", len(result.html))
        else:
            print("Failed to crawl protected page")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Initializing AsyncWebCrawler in Python
DESCRIPTION: Constructor for the AsyncWebCrawler class, including parameters for crawler strategy, browser configuration, caching, and thread safety.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
class AsyncWebCrawler:
    def __init__(
        self,
        crawler_strategy: Optional[AsyncCrawlerStrategy] = None,
        config: Optional[BrowserConfig] = None,
        always_bypass_cache: bool = False,           # deprecated
        always_by_pass_cache: Optional[bool] = None, # also deprecated
        base_directory: str = ...,
        thread_safe: bool = False,
        **kwargs,
    ):
        """
        Create an AsyncWebCrawler instance.

        Args:
            crawler_strategy: 
                (Advanced) Provide a custom crawler strategy if needed.
            config: 
                A BrowserConfig object specifying how the browser is set up.
            always_bypass_cache: 
                (Deprecated) Use CrawlerRunConfig.cache_mode instead.
            base_directory:     
                Folder for storing caches/logs (if relevant).
            thread_safe: 
                If True, attempts some concurrency safeguards. Usually False.
            **kwargs: 
                Additional legacy or debugging parameters.
        """
    )
```

----------------------------------------

TITLE: Accessing and Processing Table Data in Crawl4AI
DESCRIPTION: Example demonstrating how to extract and display structured data from HTML tables captured during crawling, including headers, rows, and metadata.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_96

LANGUAGE: python
CODE:
```
if result.success:
    tables = result.media.get("tables", [])
    print(f"Found {len(tables)} data tables on the page")
    
    if tables:
        # Access the first table
        first_table = tables[0]
        print(f"Table caption: {first_table.get('caption', 'No caption')}")
        print(f"Headers: {first_table.get('headers', [])}")
        
        # Print the first 3 rows
        for i, row in enumerate(first_table.get('rows', [])[:3]):
            print(f"Row {i+1}: {row}")
```

----------------------------------------

TITLE: Basic Markdown Generation with DefaultMarkdownGenerator in Python
DESCRIPTION: Demonstrates minimal setup for generating markdown from a webpage using DefaultMarkdownGenerator without additional filtering. Uses AsyncWebCrawler to fetch content and convert it to markdown.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/markdown-generation.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com", config=config)
        
        if result.success:
            print("Raw Markdown Output:\n")
            print(result.markdown)  # The unfiltered markdown from the page
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Create a New Profile (Interactive)
DESCRIPTION: Initiates the process of creating a new profile via the interactive manager. This involves naming the profile, logging in through a browser window, and saving the browser state.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/codebase/cli.md#_snippet_10

LANGUAGE: Shell
CODE:
```
crwl profiles
```

----------------------------------------

TITLE: Capturing PDFs and Screenshots with AsyncWebCrawler
DESCRIPTION: This example shows how to capture both PDF exports and screenshots of web pages using Crawl4AI. It demonstrates how to save the base64-encoded screenshot and PDF data to files after a successful crawl, with options to bypass the cache.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_131

LANGUAGE: python
CODE:
```
import os, asyncio
from base64 import b64decode
from crawl4ai import AsyncWebCrawler, CacheMode

async def main():
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://en.wikipedia.org/wiki/List_of_common_misconceptions",
            cache_mode=CacheMode.BYPASS,
            pdf=True,
            screenshot=True
        )
        
        if result.success:
            # Save screenshot
            if result.screenshot:
                with open("wikipedia_screenshot.png", "wb") as f:
                    f.write(b64decode(result.screenshot))
            
            # Save PDF
            if result.pdf:
                with open("wikipedia_page.pdf", "wb") as f:
                    f.write(result.pdf)
            
            print("[OK] PDF & screenshot captured.")
        else:
            print("[ERROR]", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Browser Configuration in YAML for Crawl4AI
DESCRIPTION: Defines browser settings for the crawler including headless mode, viewport size, user agent configuration, and error handling options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_2

LANGUAGE: yaml
CODE:
```
# browser.yml
headless: true
viewport_width: 1280
user_agent_mode: "random"
verbose: true
ignore_https_errors: true
```

----------------------------------------

TITLE: Timing Control Configuration for Web Crawling in Python
DESCRIPTION: This example shows how to configure timing parameters for web crawling using CrawlerRunConfig. It sets a page timeout limit and adds a delay before capturing the final HTML content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_118

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    page_timeout=60000,  # 60s limit
    delay_before_return_html=2.5
)
```

----------------------------------------

TITLE: Implementing Hook Functions with AsyncWebCrawler in Python
DESCRIPTION: A comprehensive example demonstrating how to use hook functions with the AsyncWebCrawler class. The code shows the implementation of various hooks that intercept different stages of the crawling process, from browser creation to HTML retrieval. It includes custom handling for page context creation, user agent updates, and navigation events.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_162

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig, CacheMode
from playwright.async_api import Page, BrowserContext


async def main():
    print(" Hooks Example: Demonstrating different hook use cases")

    # Configure browser settings
    browser_config = BrowserConfig(headless=True)

    # Configure crawler settings
    crawler_run_config = CrawlerRunConfig(
        js_code="window.scrollTo(0, document.body.scrollHeight);",
        wait_for="body",
        cache_mode=CacheMode.BYPASS,
    )

    # Create crawler instance
    crawler = AsyncWebCrawler(config=browser_config)

    # Define and set hook functions
    async def on_browser_created(browser, context: BrowserContext, **kwargs):
        """Hook called after the browser is created"""
        print("[HOOK] on_browser_created - Browser is ready!")
        # Example: Set a cookie that will be used for all requests
        return browser

    async def on_page_context_created(page: Page, context: BrowserContext, **kwargs):
        """Hook called after a new page and context are created"""
        print("[HOOK] on_page_context_created - New page created!")
        # Example: Set default viewport size
        await context.add_cookies(
            [
                {
                    "name": "session_id",
                    "value": "example_session",
                    "domain": ".example.com",
                    "path": "/",
                }
            ]
        )
        await page.set_viewport_size({"width": 1080, "height": 800})
        return page

    async def on_user_agent_updated(
        page: Page, context: BrowserContext, user_agent: str, **kwargs
    ):
        """Hook called when the user agent is updated"""
        print(f"[HOOK] on_user_agent_updated - New user agent: {user_agent}")
        return page

    async def on_execution_started(page: Page, context: BrowserContext, **kwargs):
        """Hook called after custom JavaScript execution"""
        print("[HOOK] on_execution_started - Custom JS executed!")
        return page

    async def before_goto(page: Page, context: BrowserContext, url: str, **kwargs):
        """Hook called before navigating to each URL"""
        print(f"[HOOK] before_goto - About to visit: {url}")
        # Example: Add custom headers for the request
        await page.set_extra_http_headers({"Custom-Header": "my-value"})
        return page

    async def after_goto(
        page: Page, context: BrowserContext, url: str, response: dict, **kwargs
    ):
        """Hook called after navigating to each URL"""
        print(f"[HOOK] after_goto - Successfully loaded: {url}")
        # Example: Wait for a specific element to be loaded
        try:
            await page.wait_for_selector(".content", timeout=1000)
            print("Content element found!")
        except:
            print("Content element not found, continuing anyway")
        return page

    async def before_retrieve_html(page: Page, context: BrowserContext, **kwargs):
        """Hook called before retrieving the HTML content"""
        print("[HOOK] before_retrieve_html - About to get HTML content")
        # Example: Scroll to bottom to trigger lazy loading
        await page.evaluate("window.scrollTo(0, document.body.scrollHeight);")
        return page

    async def before_return_html(
        page: Page, context: BrowserContext, html: str, **kwargs
    ):
        """Hook called before returning the HTML content"""
        print(f"[HOOK] before_return_html - Got HTML content (length: {len(html)})")
        # Example: You could modify the HTML content here if needed
        return page

    # Set all the hooks
    crawler.crawler_strategy.set_hook("on_browser_created", on_browser_created)
    crawler.crawler_strategy.set_hook(
        "on_page_context_created", on_page_context_created
    )
    crawler.crawler_strategy.set_hook("on_user_agent_updated", on_user_agent_updated)
    crawler.crawler_strategy.set_hook("on_execution_started", on_execution_started)
    crawler.crawler_strategy.set_hook("before_goto", before_goto)
    crawler.crawler_strategy.set_hook("after_goto", after_goto)
    crawler.crawler_strategy.set_hook("before_retrieve_html", before_retrieve_html)
    crawler.crawler_strategy.set_hook("before_return_html", before_return_html)

    await crawler.start()

    # Example usage: crawl a simple website
    url = "https://example.com"
    result = await crawler.arun(url, config=crawler_run_config)
    print(f"\nCrawled URL: {result.url}")
    print(f"HTML length: {len(result.html)}")

    await crawler.close()


if __name__ == "__main__":
    import asyncio

    asyncio.run(main())
```

----------------------------------------

TITLE: Limiting Crawl Size by Page Count in BFS Strategy (Python)
DESCRIPTION: Shows how to set a hard upper bound on the number of pages crawled regardless of depth using max_pages in the BFS strategy. Configures BFSDeepCrawlStrategy with both max_depth and max_pages, enabling cost control and test runs. Requires Crawl4AI environment and BFSDeepCrawlStrategy definition.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_13

LANGUAGE: python
CODE:
```
# Limit to exactly 20 pages regardless of depth\nstrategy = BFSDeepCrawlStrategy(\n    max_depth=3,\n    max_pages=20\n)\n
```

----------------------------------------

TITLE: Installing Crawl4AI with Transformers Support
DESCRIPTION: Command to install Crawl4AI with Hugging Face transformers for advanced NLP capabilities.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_83

LANGUAGE: bash
CODE:
```
pip install crawl4ai[transformer]
crawl4ai-setup
```

----------------------------------------

TITLE: Manually Installing Browsers for Playwright in Bash
DESCRIPTION: This Bash snippet details a troubleshooting step for manually installing Chromium browser dependencies used by Playwright if browser-related errors prevent normal operation. The prerequisite is an environment where Python and Playwright are installed. The command modifies system software by downloading browser binaries needed to enable Crawl4AIs browser automation features. Output is displayed via the terminal.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
python -m playwright install --with-deps chromium
```

----------------------------------------

TITLE: Configuring LLMExtractionStrategy Chunking (Python)
DESCRIPTION: Configures the `LLMExtractionStrategy` for processing long documents. It sets a smaller `chunk_token_threshold` to break the document into smaller parts and specifies an `overlap_rate` to maintain context between chunks. This is useful for managing large inputs for LLM processing.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_14

LANGUAGE: python
CODE:
```
# For long documents
strategy = LLMExtractionStrategy(
    chunk_token_threshold=2000,  # Smaller chunks
    overlap_rate=0.1           # 10% overlap
)
```

----------------------------------------

TITLE: Basic Markdown Generation with Crawl4AI
DESCRIPTION: A minimal example demonstrating how to use DefaultMarkdownGenerator with AsyncWebCrawler to generate markdown from a web page. It shows how to configure CrawlerRunConfig and access the generated markdown.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_103

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

async def main():
    config = CrawlerRunConfig(
        markdown_generator=DefaultMarkdownGenerator()
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com", config=config)
        
        if result.success:
            print("Raw Markdown Output:\n")
            print(result.markdown)  # The unfiltered markdown from the page
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Crawler Configuration Command Line Examples for Crawl4AI
DESCRIPTION: Shows how to apply crawler configuration settings using either a YAML file or direct command line parameters.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_5

LANGUAGE: bash
CODE:
```
# Using config file
crwl https://example.com -C crawler.yml

# Using direct parameters
crwl https://example.com -c "css_selector=#main,delay_before_return_html=2,scan_full_page=true"
```

----------------------------------------

TITLE: Accessing Different Markdown Formats from MarkdownGenerationResult
DESCRIPTION: This snippet shows how to access the various markdown formats available in the MarkdownGenerationResult object, including raw markdown, markdown with citations, references, and filtered markdown.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_112

LANGUAGE: python
CODE:
```
md_obj = result.markdown  # your library's naming may vary
print("RAW:\n", md_obj.raw_markdown)
print("CITED:\n", md_obj.markdown_with_citations)
print("REFERENCES:\n", md_obj.references_markdown)
print("FIT:\n", md_obj.fit_markdown)
```

----------------------------------------

TITLE: Using DefaultMarkdownGenerator with Crawl4AI
DESCRIPTION: This snippet demonstrates how to configure and use the DefaultMarkdownGenerator in Crawl4AI. It shows how to enable citations and set options like body width for the markdown output.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_25

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

config = CrawlerRunConfig(
    markdown_generator=DefaultMarkdownGenerator(
        options={"citations": True, "body_width": 80}  # e.g. pass html2text style options
    )
)
result = await crawler.arun(url="https://example.com", config=config)

md_res = result.markdown  # or eventually 'result.markdown'
print(md_res.raw_markdown[:500])
print(md_res.markdown_with_citations)
print(md_res.references_markdown)
```

----------------------------------------

TITLE: Implementing a Content Filtering Pipeline with CosineStrategy in Python
DESCRIPTION: Shows how to create a complete content filtering pipeline using CosineStrategy to extract pricing features from a web page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
strategy = CosineStrategy(
    semantic_filter="pricing plans features",
    word_count_threshold=15,
    sim_threshold=0.5,
    top_k=3
)

async def extract_pricing_features(url: str):
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url=url,
            extraction_strategy=strategy
        )
        
        if result.success:
            content = json.loads(result.extracted_content)
            return {
                'pricing_features': content,
                'clusters': len(content),
                'similarity_scores': [item['score'] for item in content]
            }
```

----------------------------------------

TITLE: Multiple File Download Implementation in Crawl4AI
DESCRIPTION: Complete example demonstrating how to download multiple files using Crawl4AI with custom configuration and error handling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/file-downloading.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
import os
from pathlib import Path

async def download_multiple_files(url: str, download_path: str):
    config = BrowserConfig(accept_downloads=True, downloads_path=download_path)
    async with AsyncWebCrawler(config=config) as crawler:
        run_config = CrawlerRunConfig(
            js_code="""
                const downloadLinks = document.querySelectorAll('a[download]');
                for (const link of downloadLinks) {
                    link.click();
                    // Delay between clicks
                    await new Promise(r => setTimeout(r, 2000));  
                }
            """,
            wait_for=10  # Wait for all downloads to start
        )
        result = await crawler.arun(url=url, config=run_config)

        if result.downloaded_files:
            print("Downloaded files:")
            for file in result.downloaded_files:
                print(f"- {file}")
        else:
            print("No files downloaded.")

# Usage
download_path = os.path.join(Path.home(), ".crawl4ai", "downloads")
os.makedirs(download_path, exist_ok=True)

asyncio.run(download_multiple_files("https://www.python.org/downloads/windows/", download_path))
```

----------------------------------------

TITLE: Implementing Magic Mode Anti-Bot Detection
DESCRIPTION: Example of using Magic Mode to bypass anti-bot detection mechanisms on protected websites.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/releases_review/Crawl4AI_v0.3.72_Release_Announcement.ipynb#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
async def magic_mode_demo():
    async with AsyncWebCrawler() as crawler:  # Enables anti-bot detection bypass
        result = await crawler.arun(
            url="https://www.reuters.com/markets/us/global-markets-view-usa-pix-2024-08-29/",
            magic=True  # Enables magic mode
        )
        print(result.markdown)  # Shows the full content in Markdown format

# Run the demo
await magic_mode_demo()
```

----------------------------------------

TITLE: Configuring Proxy Support and Rotation
DESCRIPTION: Sets up proxy configuration for web crawling with authentication. Enables proxy rotation and verification for distributed crawling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    proxy_config={
        "server": "http://proxy:8080",
        "username": "user",
        "password": "pass"
    }
)
```

----------------------------------------

TITLE: Setting Up JSON CSS Extraction Strategy in Crawl4AI
DESCRIPTION: Demonstrates how to configure a JSON CSS extraction strategy for Crawl4AI, specifying selectors for different fields to be extracted from web pages.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_59

LANGUAGE: json
CODE:
```
{
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "extraction_strategy": {
                "type": "JsonCssExtractionStrategy",
                "params": {
                    "schema": {
                        "baseSelector": "article.post",
                        "fields": [
                            {"name": "title", "selector": "h1", "type": "text"},
                            {"name": "content", "selector": ".content", "type": "html"}
                        ]
                    }
                }
            }
        }
    }
}
```

----------------------------------------

TITLE: Browser Flow Configuration
DESCRIPTION: Configuration for controlling page navigation, timing, and wait conditions.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
run_config = CrawlerRunConfig(
    wait_for="css:.dynamic-content", # Wait for .dynamic-content
    delay_before_return_html=2.0,    # Wait 2s before capturing final HTML
    page_timeout=60000,             # Navigation & script timeout (ms)
)
```

----------------------------------------

TITLE: Configuring Content Filtering and Exclusions in Crawl4AI
DESCRIPTION: This snippet demonstrates the basic configuration options for filtering content, including word count thresholds, tag exclusions, link filtering, domain exclusions, and media filtering. These parameters help remove unwanted content from crawl results.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    # Content thresholds
    word_count_threshold=10,        # Minimum words per block

    # Tag exclusions
    excluded_tags=['form', 'header', 'footer', 'nav'],

    # Link filtering
    exclude_external_links=True,    
    exclude_social_media_links=True,
    # Block entire domains
    exclude_domains=["adtrackers.com", "spammynews.org"],    
    exclude_social_media_domains=["facebook.com", "twitter.com"],

    # Media filtering
    exclude_external_images=True
)
```

----------------------------------------

TITLE: Configuring Stealth Settings for Browser Automation in Python
DESCRIPTION: Defines stealth configuration settings to make automated browser behavior appear more human-like. Includes various Chrome-specific settings and navigator properties.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_110

LANGUAGE: python
CODE:
```
stealth_config = StealthConfig(
    webdriver=True,
    chrome_app=True,
    chrome_csi=True,
    chrome_load_times=True,
    chrome_runtime=True,
    navigator_languages=True,
    navigator_plugins=True,
    navigator_permissions=True,
    webgl_vendor=True,
    outerdimensions=True,
    navigator_hardware_concurrency=True,
    media_codecs=True,
)
```

----------------------------------------

TITLE: Implementing E-commerce Data Extraction with crawl4ai
DESCRIPTION: Complete example showing how to use the advanced e-commerce schema with crawl4ai to extract hierarchical product data. The code initializes a crawler, applies the schema using JsonCssExtractionStrategy, and processes the extracted JSON results.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_188

LANGUAGE: python
CODE:
```
import json
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

ecommerce_schema = {
    # ... the advanced schema from above ...
}

async def extract_ecommerce_data():
    strategy = JsonCssExtractionStrategy(ecommerce_schema, verbose=True)
    
    config = CrawlerRunConfig()
    
    async with AsyncWebCrawler(verbose=True) as crawler:
        result = await crawler.arun(
            url="https://gist.githubusercontent.com/githubusercontent/2d7b8ba3cd8ab6cf3c8da771ddb36878/raw/1ae2f90c6861ce7dd84cc50d3df9920dee5e1fd2/sample_ecommerce.html",
            extraction_strategy=strategy,
            config=config
        )

        if not result.success:
            print("Crawl failed:", result.error_message)
            return
        
        # Parse the JSON output
        data = json.loads(result.extracted_content)
        print(json.dumps(data, indent=2) if data else "No data found.")

asyncio.run(extract_ecommerce_data())
```

----------------------------------------

TITLE: URL Validation and Filtering for Web Crawling in Python
DESCRIPTION: Implements URL validation and filtering functionality for the crawler. This method checks if a URL has a valid format and applies the filter chain, with an exception for the starting URL which bypasses filtering.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_177

LANGUAGE: python
CODE:
```
async def can_process_url(self, url: str, depth: int) -> bool:
    """
    Validates the URL and applies the filter chain.
    For the start URL (depth 0) filtering is bypassed.
    """
    try:
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError("Missing scheme or netloc")
        if parsed.scheme not in ("http", "https"):
            raise ValueError("Invalid scheme")
        if "." not in parsed.netloc:
            raise ValueError("Invalid domain")
    except Exception as e:
        self.logger.warning(f"Invalid URL: {url}, error: {e}")
        return False

    if depth != 0 and not await self.filter_chain.apply(url):
        return False

    return True
```

----------------------------------------

TITLE: Using LLMConfig for Extraction and Filtering Tasks
DESCRIPTION: Shows how to use the new LLMConfig parameter for streamlining LLM integration across different Crawl4AI components. This approach simplifies configuration and enables reuse of LLM settings across extraction, filtering, and schema generation tasks.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_11

LANGUAGE: python
CODE:
```
from crawl4ai import LLMConfig
from crawl4ai.extraction_strategy import LLMExtractionStrategy
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

# Example of using LLMConfig with LLMExtractionStrategy
llm_config = LLMConfig(provider="openai/gpt-4o", api_token="YOUR_API_KEY")
strategy = LLMExtractionStrategy(llm_config=llm_config, schema=...)

# Example usage within a crawler
async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(
        url="https://example.com",
        config=CrawlerRunConfig(extraction_strategy=strategy)
    )
```

----------------------------------------

TITLE: URL Pattern Filtering in Web Crawler
DESCRIPTION: Implementation of basic URL pattern filtering using FilterChain to narrow down crawled pages based on URL patterns.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_37

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling.filters import FilterChain, URLPatternFilter

# Only follow URLs containing "blog" or "docs"
url_filter = URLPatternFilter(patterns=["*blog*", "*docs*"])

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=1,
        filter_chain=FilterChain([url_filter])
    )
)
```

----------------------------------------

TITLE: Building and Running Crawl4AI Locally with Docker Compose (Bash)
DESCRIPTION: Demonstrates building the Docker image locally using the `Dockerfile` in the project root and then running the container using `docker compose up --build -d`. This automatically uses the correct architecture.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_8

LANGUAGE: bash
CODE:
```
# Builds the image locally using Dockerfile and runs it
# Automatically uses the correct architecture for your machine
docker compose up --build -d
```

----------------------------------------

TITLE: Excluding Specific Domains with CrawlerRunConfig (Python)
DESCRIPTION: This snippet shows how to configure `CrawlerRunConfig` to exclude links pointing to a specific list of custom domains. The `exclude_domains` parameter accepts a list of domain strings (e.g., `["suspiciousads.com"]`) that should be filtered out during the crawl.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_5

LANGUAGE: python
CODE:
```
crawler_cfg = CrawlerRunConfig(
    exclude_domains=["suspiciousads.com"]
)
```

----------------------------------------

TITLE: Basic Session-Based Crawling Example in Python
DESCRIPTION: A simple implementation of session-based crawling that loads more content dynamically using JavaScript execution. This example demonstrates how to reuse the same session across multiple page loads, bypassing the cache for fresh content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/session-management.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai.async_configs import BrowserConfig, CrawlerRunConfig
from crawl4ai.cache_context import CacheMode

async def basic_session_crawl():
    async with AsyncWebCrawler() as crawler:
        session_id = "dynamic_content_session"
        url = "https://example.com/dynamic-content"

        for page in range(3):
            config = CrawlerRunConfig(
                url=url,
                session_id=session_id,
                js_code="document.querySelector('.load-more-button').click();" if page > 0 else None,
                css_selector=".content-item",
                cache_mode=CacheMode.BYPASS
            )
            
            result = await crawler.arun(config=config)
            print(f"Page {page + 1}: Found {result.extracted_content.count('.content-item')} items")

        await crawler.crawler_strategy.kill_session(session_id)

asyncio.run(basic_session_crawl())
```

----------------------------------------

TITLE: Running Pre-built Image with Docker Compose
DESCRIPTION: This command uses Docker Compose to pull and run a pre-built Crawl4AI image from Docker Hub in detached mode. It sets the `IMAGE` environment variable to specify the image tag. Docker Compose automatically selects the correct architecture.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/docker-deployment.md#_snippet_7

LANGUAGE: Bash
CODE:
```
# Pulls and runs the release candidate from Docker Hub
# Automatically selects the correct architecture
IMAGE=unclecode/crawl4ai:latest docker compose up -d
```

----------------------------------------

TITLE: Initializing LLMConfig in Python for Crawl4AI
DESCRIPTION: This snippet demonstrates how to create an LLMConfig instance for Crawl4AI. It shows how to specify the LLM provider and API token, with an example of using an environment variable for the API key.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
llm_config = LLMConfig(provider="openai/gpt-4o-mini", api_token=os.getenv("OPENAI_API_KEY"))
```

----------------------------------------

TITLE: Defining Blog Post Extraction Schema (Python)
DESCRIPTION: Provides a complete Crawl4AI schema definition in Python for extracting data from blog post cards. It includes `baseFields` for the URL and `fields` for title, date, summary, and author, using CSS selectors.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/no-llm-strategies.md#_snippet_10

LANGUAGE: Python
CODE:
```
schema = {
  "name": "Blog Posts",
  "baseSelector": "a.blog-post-card",
  "baseFields": [
    {"name": "post_url", "type": "attribute", "attribute": "href"}
  ],
  "fields": [
    {"name": "title", "selector": "h2.post-title", "type": "text", "default": "No Title"},
    {"name": "date", "selector": "time.post-date", "type": "text", "default": ""},
    {"name": "summary", "selector": "p.post-summary", "type": "text", "default": ""},
    {"name": "author", "selector": "span.post-author", "type": "text", "default": ""}
  ]
}
```

----------------------------------------

TITLE: Running Pre-built Crawl4AI Image with Docker Compose (Bash)
DESCRIPTION: Uses `docker compose up -d` to pull and run a specific pre-built Crawl4AI image from Docker Hub. The `IMAGE` environment variable specifies the image tag. Docker Compose handles architecture selection automatically.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_7

LANGUAGE: bash
CODE:
```
# Pulls and runs the release candidate from Docker Hub
# Automatically selects the correct architecture
IMAGE=unclecode/crawl4ai:0.6.0-rN # Use your favorite revision number docker compose up -d
```

----------------------------------------

TITLE: Cloning CrawlerRunConfig in Python
DESCRIPTION: Demonstrates how to create and modify crawler configurations using the clone() method. Shows how to create a base configuration and derive variations for different use cases like streaming and cache bypass.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
# Create a base configuration
base_config = CrawlerRunConfig(
    cache_mode=CacheMode.ENABLED,
    word_count_threshold=200
)

# Create variations using clone()
stream_config = base_config.clone(stream=True)
no_cache_config = base_config.clone(
    cache_mode=CacheMode.BYPASS,
    stream=True
)
```

----------------------------------------

TITLE: JsonCssExtractionStrategy Schema Structure (Python)
DESCRIPTION: Defines the required structure for the schema dictionary used by JsonCssExtractionStrategy. It includes a base selector and a list of field definitions, each specifying the field name, selector, type (text, attribute, html, regex), and optional properties like attribute name, regex pattern, transformation, and default value.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_5

LANGUAGE: python
CODE:
```
# Schema Structure
schema = {
    "name": str,              # Schema name
    "baseSelector": str,      # Base CSS selector
    "fields": [               # List of fields to extract
        {
            "name": str,      # Field name
            "selector": str,  # CSS selector
            "type": str,     # Field type: "text", "attribute", "html", "regex"
            "attribute": str, # For type="attribute"
            "pattern": str,  # For type="regex"
            "transform": str, # Optional: "lowercase", "uppercase", "strip"
            "default": Any    # Default value if extraction fails
        }
    ]
}
```

----------------------------------------

TITLE: Implementing Memory-Adaptive Dispatcher in Python
DESCRIPTION: Sets up an intelligent resource management system with memory monitoring and auto-throttling capabilities. Configures concurrent session management and real-time monitoring of crawler operations.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, DisplayMode
from crawl4ai.async_dispatcher import MemoryAdaptiveDispatcher, CrawlerMonitor

async def main():
    urls = ["https://example1.com", "https://example2.com"] * 50
    
    # Configure memory-aware dispatch
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=80.0,  # Auto-throttle at 80% memory
        check_interval=0.5,             # Check every 0.5 seconds
        max_session_permit=20,          # Max concurrent sessions
        monitor=CrawlerMonitor(         # Real-time monitoring
            display_mode=DisplayMode.DETAILED
        )
    )
    
    async with AsyncWebCrawler() as crawler:
        results = await dispatcher.run_urls(
            urls=urls,
            crawler=crawler,
            config=CrawlerRunConfig()
        )
```

----------------------------------------

TITLE: Implementing Fixed-Length Word Chunking in Python
DESCRIPTION: Segments text into chunks with a fixed number of words. The class accepts a configurable chunk size parameter and splits the text into segments of the specified word count.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_166

LANGUAGE: python
CODE:
```
class FixedLengthWordChunking:
    def __init__(self, chunk_size=100):
        self.chunk_size = chunk_size

    def chunk(self, text):
        words = text.split()
        return [' '.join(words[i:i + self.chunk_size]) for i in range(0, len(words), self.chunk_size)]

# Example Usage
text = "This is a long text with many words to be chunked into fixed sizes."
chunker = FixedLengthWordChunking(chunk_size=5)
print(chunker.chunk(text))
```

----------------------------------------

TITLE: Adjusting Viewport Dynamically to Match Page Content
DESCRIPTION: This code shows how to enable dynamic viewport adjustment that matches the page's content dimensions. The crawler calculates page width and height after loading and adjusts the viewport accordingly, ensuring all content is visible.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.4.1.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
await crawler.crawl(
    url="https://example.com",
    adjust_viewport_to_content=True  # Dynamically adjusts the viewport
)
```

----------------------------------------

TITLE: Accessing Link Data in Crawl4AI Results
DESCRIPTION: Example of how to access and process internal and external links from a crawl result, demonstrating how to extract link counts and examine link properties.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_89

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun("https://www.example.com")
    if result.success:
        internal_links = result.links.get("internal", [])
        external_links = result.links.get("external", [])
        print(f"Found {len(internal_links)} internal links.")
        print(f"Found {len(internal_links)} external links.")
        print(f"Found {len(result.media)} media items.")

        # Each link is typically a dictionary with fields like:
        # { "href": "...", "text": "...", "title": "...", "base_domain": "..." }
        if internal_links:
            print("Sample Internal Link:", internal_links[0])
    else:
        print("Crawl failed:", result.error_message)
```

----------------------------------------

TITLE: BrowserConfig Helper Methods Example in Python
DESCRIPTION: Example demonstrating how to use BrowserConfig's clone() method to create modified configurations. Shows creating a base configuration and then a specialized debug configuration with modified parameters.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
# Create a base browser config
base_browser = BrowserConfig(
    browser_type="chromium",
    headless=True,
    text_mode=True
)

# Create a visible browser config for debugging
debug_browser = base_browser.clone(
    headless=False,
    verbose=True
)
```

----------------------------------------

TITLE: Implementing Managed Browser Configuration in Crawl4AI
DESCRIPTION: Example of configuring and using a managed browser with persistent identity in Crawl4AI.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_138

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def main():
    # 1) Reference your persistent data directory
    browser_config = BrowserConfig(
        headless=True,             # 'True' for automated runs
        verbose=True,
        use_managed_browser=True,  # Enables persistent browser strategy
        browser_type="chromium",
        user_data_dir="/path/to/my-chrome-profile"
    )

    # 2) Standard crawl config
    crawl_config = CrawlerRunConfig(
        wait_for="css:.logged-in-content"
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url="https://example.com/private", config=crawl_config)
        if result.success:
            print("Successfully accessed private data with your identity!")
        else:
            print("Error:", result.error_message)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Performing a Simple Crawl via Direct API Call
DESCRIPTION: Illustrates how to make a direct HTTP POST request to the crawl4ai server's `/crawl` endpoint using Python's `requests` library. It constructs the JSON payload, including nested `BrowserConfig` and `CrawlerRunConfig` objects following the required `{"type": "ClassName", "params": {...}}` structure, sends the request, and prints the status code and response.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_30

LANGUAGE: python
CODE:
```
import requests

# Configuration objects converted to the required JSON structure
browser_config_payload = {
    "type": "BrowserConfig",
    "params": {"headless": True}
}
crawler_config_payload = {
    "type": "CrawlerRunConfig",
    "params": {"stream": False, "cache_mode": "bypass"} # Use string value of enum
}

crawl_payload = {
    "urls": ["https://httpbin.org/html"],
    "browser_config": browser_config_payload,
    "crawler_config": crawler_config_payload
}
response = requests.post(
    "http://localhost:11235/crawl", # Updated port
    # headers={"Authorization": f"Bearer {token}"},  # If JWT is enabled
    json=crawl_payload
)
print(f"Status Code: {response.status_code}")
if response.ok:
    print(response.json())
else:
    print(f"Error: {response.text}")

```

----------------------------------------

TITLE: Handling Load More Functionality with Crawl4AI
DESCRIPTION: Implements a two-step process to load initial content and then click a 'More' link to load additional content. Uses session management to maintain state between requests.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Step 1: Load initial Hacker News page
    config = CrawlerRunConfig(
        wait_for="css:.athing:nth-child(30)"  # Wait for 30 items
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            config=config
        )
        print("Initial items loaded.")

        # Step 2: Let's scroll and click the "More" link
        load_more_js = [
            "window.scrollTo(0, document.body.scrollHeight);",
            # The "More" link at page bottom
            "document.querySelector('a.morelink')?.click();"  
        ]
        
        next_page_conf = CrawlerRunConfig(
            js_code=load_more_js,
            wait_for="""js:() => {
                return document.querySelectorAll('.athing').length > 30;
            }""",
            # Mark that we do not re-navigate, but run JS in the same session:
            js_only=True,
            session_id="hn_session"
        )

        # Re-use the same crawler session
        result2 = await crawler.arun(
            url="https://news.ycombinator.com",  # same URL but continuing session
            config=next_page_conf
        )
        total_items = result2.cleaned_html.count("athing")
        print("Items after load-more:", total_items)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Basic Proxy Configuration in Python using BrowserConfig
DESCRIPTION: Demonstrates basic proxy setup using BrowserConfig class with both HTTP and SOCKS proxy examples.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_158

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import BrowserConfig

# Using proxy URL
browser_config = BrowserConfig(proxy="http://proxy.example.com:8080")
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")

# Using SOCKS proxy
browser_config = BrowserConfig(proxy="socks5://proxy.example.com:1080")
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")
```

----------------------------------------

TITLE: Saving Screenshot (Python)
DESCRIPTION: Illustrates how to check if a screenshot is available (as a Base64-encoded string) in the `screenshot` field and decode/save it to a file. This field is populated if `screenshot=True` was set in `CrawlerRunConfig`. Requires the `base64` module.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_16

LANGUAGE: python
CODE:
```
import base64
if result.screenshot:
    with open("page.png", "wb") as f:
        f.write(base64.b64decode(result.screenshot))
```

----------------------------------------

TITLE: Crawl with Profile and Verbose JSON Output
DESCRIPTION: Performs a crawl using a saved profile and outputs the results in verbose JSON format. Demonstrates combining profile usage with output and verbosity flags.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/codebase/cli.md#_snippet_14

LANGUAGE: Shell
CODE:
```
crwl https://site.com -p my-profile -o json -v
```

----------------------------------------

TITLE: Installing All Crawl4AI Extensions
DESCRIPTION: Command to install Crawl4AI with all available extensions and features.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_84

LANGUAGE: bash
CODE:
```
pip install crawl4ai[all]
crawl4ai-setup
```

----------------------------------------

TITLE: Implementing Robots.txt Compliance
DESCRIPTION: Enables robots.txt checking with SQLite caching support. Verifies crawling permissions and handles blocked access appropriately.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/v0.4.3b1.md#2025-04-23_snippet_6

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(check_robots_txt=True)
result = await crawler.arun(url, config=config)
if result.status_code == 403:
    print("Access blocked by robots.txt")
```

----------------------------------------

TITLE: Stopping Crawl4AI Service with Docker Compose (Bash)
DESCRIPTION: Provides the command `docker compose down` to stop and remove the containers, networks, and volumes defined in the `docker-compose.yml` file.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_10

LANGUAGE: bash
CODE:
```
# Stop the service
docker compose down
```

----------------------------------------

TITLE: Dictionary-based Storage State Implementation
DESCRIPTION: Example showing how to pass storage_state as a Python dictionary to AsyncWebCrawler
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/storage_state_tutorial.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    storage_dict = {
        "cookies": [
            {
                "name": "session",
                "value": "abcd1234",
                "domain": "example.com",
                "path": "/",
                "expires": 1675363572.037711,
                "httpOnly": False,
                "secure": False,
                "sameSite": "None"
            }
        ],
        "origins": [
            {
                "origin": "https://example.com",
                "localStorage": [
                    {"name": "token", "value": "my_auth_token"},
                    {"name": "refreshToken", "value": "my_refresh_token"}
                ]
            }
        ]
    }

    async with AsyncWebCrawler(
        headless=True,
        storage_state=storage_dict
    ) as crawler:
        result = await crawler.arun(url='https://example.com/protected')
        if result.success:
            print("Crawl succeeded with pre-loaded session data!")
            print("Page HTML length:", len(result.html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Implementing Semaphore-based Crawling in Python with Crawl4AI
DESCRIPTION: Demonstrates how to implement a web crawler with semaphore-based concurrency control using Crawl4AI. The code configures a crawler with a SemaphoreDispatcher that limits concurrency to 5 parallel requests and includes a rate limiter to prevent overwhelming servers.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_148

LANGUAGE: python
CODE:
```
async def crawl_with_semaphore(urls):
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS)
    
    dispatcher = SemaphoreDispatcher(
        semaphore_count=5,
        rate_limiter=RateLimiter(
            base_delay=(0.5, 1.0),
            max_delay=10.0
        ),
        monitor=CrawlerMonitor(
            max_visible_rows=15,
            display_mode=DisplayMode.DETAILED
        )
    )
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        results = await crawler.arun_many(
            urls, 
            config=run_config,
            dispatcher=dispatcher
        )
        return results
```

----------------------------------------

TITLE: Agentic Crawler Setup and Execution
DESCRIPTION: Implements an autonomous crawling system capable of understanding complex goals and executing multi-step operations. Features automatic planning, visual recognition, and error recovery.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/ROADMAP.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler
from crawl4ai.agents import CrawlerAgent

async with AsyncWebCrawler() as crawler:
    agent = CrawlerAgent(crawler)
    
    # Automatic planning and execution
    result = await agent.arun(
        goal="Find research papers about quantum computing published in 2023 with more than 50 citations",
        auto_retry=True
    )
    print("Generated Plan:", result.executed_steps)
    print("Extracted Data:", result.data)
    
    # Using custom steps with automatic execution
    result = await agent.arun(
        goal="Extract conference deadlines from ML conferences",
        custom_plan=[
            "Navigate to conference page",
            "Find important dates section",
            "Extract submission deadlines",
            "Verify dates are for 2024"
        ]
    )
    
    # Monitoring execution
    print("Step Completion:", result.step_status)
    print("Execution Time:", result.execution_time)
    print("Success Rate:", result.success_rate)
```

----------------------------------------

TITLE: Creating Content Relevance Filter for Page Scoring (Python)
DESCRIPTION: Instantiates a ContentRelevanceFilter with a semantic query and similarity threshold, then applies it to content analysis using a BM25-based metric. Incorporates this filter into a BFSDeepCrawlStrategy within a CrawlerRunConfig to include only topically relevant pages. Relies on imports from crawl4ai.deep_crawling.filters and proper setup of the Crawl4AI package.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_11

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling.filters import FilterChain, ContentRelevanceFilter\n\n# Create a content relevance filter\nrelevance_filter = ContentRelevanceFilter(\n    query=\"Web crawling and data extraction with Python\",\n    threshold=0.7  # Minimum similarity score (0.0 to 1.0)\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([relevance_filter])\n    )\n)\n
```

----------------------------------------

TITLE: Minimal Example of Using BrowserConfig in Python for Crawl4AI
DESCRIPTION: This example demonstrates how to create a BrowserConfig instance and use it with AsyncWebCrawler in Crawl4AI. It sets up a Firefox browser in visible mode with text-only crawling enabled.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/browser-crawler-config.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_conf = BrowserConfig(
    browser_type="firefox",
    headless=False,
    text_mode=True
)

async with AsyncWebCrawler(config=browser_conf) as crawler:
    result = await crawler.arun("https://example.com")
    print(result.markdown[:300])
```

----------------------------------------

TITLE: Using KeywordRelevanceScorer for Prioritized Crawling in Crawl4AI (Python)
DESCRIPTION: Demonstrates using `KeywordRelevanceScorer` to prioritize URLs during a deep crawl with `BestFirstCrawlingStrategy`. A scorer is created with relevant keywords and a weight (0.0 to 1.0). This scorer is assigned to the `url_scorer` parameter of the `BestFirstCrawlingStrategy`. The example configures the crawl to use this strategy, enables streaming (`stream=True`), and iterates through results, printing the relevance score and URL. Higher-scoring URLs are crawled first. Depends on `KeywordRelevanceScorer`, `BestFirstCrawlingStrategy`, `AsyncWebCrawler`, and `CrawlerRunConfig`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_8

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling.scorers import KeywordRelevanceScorer
from crawl4ai.deep_crawling import BestFirstCrawlingStrategy

# Create a keyword relevance scorer
keyword_scorer = KeywordRelevanceScorer(
    keywords=["crawl", "example", "async", "configuration"],
    weight=0.7  # Importance of this scorer (0.0 to 1.0)
)

config = CrawlerRunConfig(
    deep_crawl_strategy=BestFirstCrawlingStrategy(
        max_depth=2,
        url_scorer=keyword_scorer
    ),
    stream=True  # Recommended with BestFirstCrawling
)

# Results will come in order of relevance score
async with AsyncWebCrawler() as crawler:
    async for result in await crawler.arun("https://example.com", config=config):
        score = result.metadata.get("score", 0)
        print(f"Score: {score:.2f} | {result.url}")
```

----------------------------------------

TITLE: Setting Custom Headers in Crawl4AI
DESCRIPTION: Demonstrates two methods of setting custom headers in Crawl4AI: at the crawler strategy level and directly in the 'arun()' method. It includes examples of setting user agents and language preferences.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/advanced-features.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler

async def main():
    # Option 1: Set headers at the crawler strategy level
    crawler1 = AsyncWebCrawler(
        # The underlying strategy can accept headers in its constructor
        crawler_strategy=None  # We'll override below for clarity
    )
    crawler1.crawler_strategy.update_user_agent("MyCustomUA/1.0")
    crawler1.crawler_strategy.set_custom_headers({
        "Accept-Language": "fr-FR,fr;q=0.9"
    })
    result1 = await crawler1.arun("https://www.example.com")
    print("Example 1 result success:", result1.success)

    # Option 2: Pass headers directly to `arun()`
    crawler2 = AsyncWebCrawler()
    result2 = await crawler2.arun(
        url="https://www.example.com",
        headers={"Accept-Language": "es-ES,es;q=0.9"}
    )
    print("Example 2 result success:", result2.success)

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Using Crawl4AI Python SDK with Docker
DESCRIPTION: Python code example demonstrating how to use the Crawl4AI Docker client SDK for both streaming and non-streaming crawls.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_50

LANGUAGE: python
CODE:
```
from crawl4ai.docker_client import Crawl4aiDockerClient
from crawl4ai import BrowserConfig, CrawlerRunConfig

async def main():
    async with Crawl4aiDockerClient(base_url="http://localhost:8000", verbose=True) as client:
      # If JWT is enabled, you can authenticate like this: (more on this later)
        # await client.authenticate("test@example.com")
        
        # Non-streaming crawl
        results = await client.crawl(
            ["https://example.com", "https://python.org"],
            browser_config=BrowserConfig(headless=True),
            crawler_config=CrawlerRunConfig()
        )
        print(f"Non-streaming results: {results}")
        
        # Streaming crawl
        crawler_config = CrawlerRunConfig(stream=True)
        async for result in await client.crawl(
            ["https://example.com", "https://python.org"],
            browser_config=BrowserConfig(headless=True),
            crawler_config=crawler_config
        ):
            print(f"Streamed result: {result}")
        
        # Get schema
        schema = await client.get_schema()
        print(f"Schema: {schema}")

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring CosineStrategy for Product Review Analysis in Python
DESCRIPTION: Shows how to set up CosineStrategy for extracting multiple customer reviews and ratings from a product page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/extraction/clustring-strategies.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
strategy = CosineStrategy(
    semantic_filter="customer reviews and ratings",
    word_count_threshold=20,   # Reviews can be shorter
    top_k=10,                 # Get multiple reviews
    sim_threshold=0.4         # Allow variety in review content
)
```

----------------------------------------

TITLE: Generating Filtered Markdown with Crawl4AI in Python
DESCRIPTION: This snippet demonstrates how to customize Markdown generation using a content filter. It initializes a `DefaultMarkdownGenerator` with a `PruningContentFilter`, which removes less relevant content based on a threshold. This generator is then passed to `CrawlerRunConfig`. The example crawls 'https://news.ycombinator.com' and prints the lengths of both the raw Markdown (`result.markdown.raw_markdown`) and the filtered Markdown (`result.markdown.fit_markdown`) to show the effect of the filter.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/quickstart.md#_snippet_2

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig
from crawl4ai.content_filter_strategy import PruningContentFilter
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator

md_generator = DefaultMarkdownGenerator(
    content_filter=PruningContentFilter(threshold=0.4, threshold_type="fixed")
)

config = CrawlerRunConfig(
    cache_mode=CacheMode.BYPASS,
    markdown_generator=md_generator
)

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun("https://news.ycombinator.com", config=config)
    print("Raw Markdown length:", len(result.markdown.raw_markdown))
    print("Fit Markdown length:", len(result.markdown.fit_markdown))
```

----------------------------------------

TITLE: Creating `.llm.env` File for API Keys (Bash)
DESCRIPTION: Shows how to create a `.llm.env` file in the current working directory using the `cat` command with a Here Document (EOL). This file is used to store API keys for various LLM providers (OpenAI, Anthropic, etc.) which are then passed to the Docker container.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_1

LANGUAGE: bash
CODE:
```
# Create a .llm.env file with your API keys
cat > .llm.env << EOL
# OpenAI
OPENAI_API_KEY=sk-your-key

# Anthropic
ANTHROPIC_API_KEY=your-anthropic-key

# Other providers as needed
# DEEPSEEK_API_KEY=your-deepseek-key
# GROQ_API_KEY=your-groq-key
# TOGETHER_API_KEY=your-together-key
# MISTRAL_API_KEY=your-mistral-key
# GEMINI_API_TOKEN=your-gemini-token
EOL
```

----------------------------------------

TITLE: Capturing and Saving Screenshots with Crawl4AI
DESCRIPTION: This function demonstrates how to capture screenshots of webpages and save them to a file. It configures the crawler to take a screenshot during crawling, then decodes the base64 screenshot data and writes it to the specified output file.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_135

LANGUAGE: python
CODE:
```
async def capture_and_save_screenshot(url: str, output_path: str):
    browser_config = BrowserConfig(headless=True)
    crawler_config = CrawlerRunConfig(cache_mode=CacheMode.BYPASS, screenshot=True)

    async with AsyncWebCrawler(config=browser_config) as crawler:
        result = await crawler.arun(url=url, config=crawler_config)

        if result.success and result.screenshot:
            import base64

            screenshot_data = base64.b64decode(result.screenshot)
            with open(output_path, "wb") as f:
                f.write(screenshot_data)
            print(f"Screenshot saved successfully to {output_path}")
        else:
            print("Failed to capture screenshot")
```

----------------------------------------

TITLE: Excluding External Images in Crawl4AI
DESCRIPTION: Configuration to exclude external images from crawled content, keeping only images from the primary domain.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_87

LANGUAGE: python
CODE:
```
crawler_cfg = CrawlerRunConfig(
    exclude_external_images=True
)
```

----------------------------------------

TITLE: Rotating Proxy Implementation in Python
DESCRIPTION: Example implementation of dynamic proxy rotation using async functions and crawler configuration.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_160

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig, CrawlerRunConfig

async def get_next_proxy():
    # Your proxy rotation logic here
    return {"server": "http://next.proxy.com:8080"}

async def main():
    browser_config = BrowserConfig()
    run_config = CrawlerRunConfig()
    
    async with AsyncWebCrawler(config=browser_config) as crawler:
        # For each URL, create a new run config with different proxy
        for url in urls:
            proxy = await get_next_proxy()
            # Clone the config and update proxy - this creates a new browser context
            current_config = run_config.clone(proxy_config=proxy)
            result = await crawler.arun(url=url, config=current_config)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

----------------------------------------

TITLE: Processing Web Crawler Results to Extract Media and Links in Python
DESCRIPTION: Code that processes web crawler results from Wikipedia to extract images and links (both internal and external). The extracted data is printed and saved to JSON files for further analysis. The code demonstrates how to access and manipulate the media and links properties of a CrawlResult object.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_155

LANGUAGE: python
CODE:
```
result: List[CrawlResult] = await crawler.arun("https://en.wikipedia.org/wiki/Main_Page")

for i, result in enumerate(result):
    # Extract and save all images
    images = result.media.get("images", [])
    print(f"Found {len(images)} images")

    # Extract and save all links (internal and external)
    internal_links = result.links.get("internal", [])
    external_links = result.links.get("external", [])
    print(f"Found {len(internal_links)} internal links")
    print(f"Found {len(external_links)} external links")

    # Print some of the images and links
    for image in images[:3]:
        print(f"Image: {image['src']}")
    for link in internal_links[:3]:
        print(f"Internal link: {link['href']}")
    for link in external_links[:3]:
        print(f"External link: {link['href']}")

    # # Save everything to files
    with open(f"{__cur_dir__}/tmp/images.json", "w") as f:
        json.dump(images, f, indent=2)

    with open(f"{__cur_dir__}/tmp/links.json", "w") as f:
        json.dump(
            {"internal": internal_links, "external": external_links},
            f,
            indent=2,
        )
```

----------------------------------------

TITLE: Listing Downloaded Files (Python)
DESCRIPTION: Shows how to check for and iterate through the list of local file paths for items downloaded during the crawl. This field is populated if `accept_downloads=True` and `downloads_path` were configured in `BrowserConfig`.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_15

LANGUAGE: python
CODE:
```
if result.downloaded_files:
    for file_path in result.downloaded_files:
        print("Downloaded:", file_path)
```

----------------------------------------

TITLE: Implementing Streaming Mode Crawler in Python
DESCRIPTION: Demonstrates how to set up a streaming crawler using MemoryAdaptiveDispatcher for real-time result processing. Includes memory management and monitoring configuration.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/multi-url-crawling.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
async def crawl_streaming():
    browser_config = BrowserConfig(headless=True, verbose=False)
    run_config = CrawlerRunConfig(
        cache_mode=CacheMode.BYPASS,
        stream=True  # Enable streaming mode
    )
    
    dispatcher = MemoryAdaptiveDispatcher(
        memory_threshold_percent=70.0,
        check_interval=1.0,
        max_session_permit=10,
        monitor=CrawlerMonitor(
            display_mode=DisplayMode.DETAILED
        )
    )

    async with AsyncWebCrawler(config=browser_config) as crawler:
        # Process results as they become available
        async for result in await crawler.arun_many(
            urls=urls,
            config=run_config,
            dispatcher=dispatcher
        ):
            if result.success:
                # Process each result immediately
                await process_result(result)
            else:
                print(f"Failed to crawl {result.url}: {result.error_message}")
```

----------------------------------------

TITLE: Timing Control Configuration in Crawl4AI
DESCRIPTION: Demonstrates how to configure timing controls in Crawl4AI, including page timeout and delay before returning HTML. These parameters help manage the crawler's behavior with dynamic content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_5

LANGUAGE: python
CODE:
```
config = CrawlerRunConfig(
    page_timeout=60000,  # 60s limit
    delay_before_return_html=2.5
)
```

----------------------------------------

TITLE: Basic Web Crawling with Markdown Generation in Python
DESCRIPTION: This function demonstrates basic web crawling using AsyncWebCrawler. It crawls a single URL (Hacker News) and generates markdown content from the crawled page.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_147

LANGUAGE: python
CODE:
```
async def demo_basic_crawl():
    """Basic web crawling with markdown generation"""
    print("\n=== 1. Basic Web Crawling ===")
    async with AsyncWebCrawler(config = BrowserConfig(
        viewport_height=800,
        viewport_width=1200,
        headless=True,
        verbose=True,
    )) as crawler:
        results: List[CrawlResult] = await crawler.arun(
            url="https://news.ycombinator.com/"
        )

        for i, result in enumerate(results):
            print(f"Result {i + 1}:")
            print(f"Success: {result.success}")
            if result.success:
                print(f"Markdown length: {len(result.markdown.raw_markdown)} chars")
                print(f"First 100 chars: {result.markdown.raw_markdown[:100]}...")
            else:
                print("Failed to crawl the URL")
```

----------------------------------------

TITLE: Connecting to Browser and Setting Up Default Context in Playwright
DESCRIPTION: An asynchronous method that initializes the Playwright instance, connects to a browser (either managed or standalone), and sets up the default context. It handles different browser types and custom configuration options.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_122

LANGUAGE: python
CODE:
```
if self.playwright is not None:
    await self.close()
    
from playwright.async_api import async_playwright

self.playwright = await async_playwright().start()

if self.config.cdp_url or self.config.use_managed_browser:
    self.config.use_managed_browser = True
    cdp_url = await self.managed_browser.start() if not self.config.cdp_url else self.config.cdp_url
    self.browser = await self.playwright.chromium.connect_over_cdp(cdp_url)
    contexts = self.browser.contexts
    if contexts:
        self.default_context = contexts[0]
    else:
        self.default_context = await self.create_browser_context()
    await self.setup_context(self.default_context)
else:
    browser_args = self._build_browser_args()

    # Launch appropriate browser type
    if self.config.browser_type == "firefox":
        self.browser = await self.playwright.firefox.launch(**browser_args)
    elif self.config.browser_type == "webkit":
        self.browser = await self.playwright.webkit.launch(**browser_args)
    else:
        self.browser = await self.playwright.chromium.launch(**browser_args)

    self.default_context = self.browser
```

----------------------------------------

TITLE: Practical Example of Content Filtering in Crawl4AI
DESCRIPTION: This example demonstrates how to combine multiple content filtering techniques, including CSS selection, word count thresholds, tag exclusions, and link/domain filtering. The crawler is configured to bypass cache for demonstration purposes.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/content-selection.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode

async def main():
    config = CrawlerRunConfig(
        css_selector="main.content", 
        word_count_threshold=10,
        excluded_tags=["nav", "footer"],
        exclude_external_links=True,
        exclude_social_media_links=True,
        exclude_domains=["ads.com", "spammytrackers.net"],
        exclude_external_images=True,
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url="https://news.ycombinator.com", config=config)
        print("Cleaned HTML length:", len(result.cleaned_html))

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Configuring DFS Deep Crawl Strategy in Crawl4AI
DESCRIPTION: This snippet demonstrates the configuration of a Depth-First Search (DFS) deep crawling strategy. It explores branches fully before backtracking and allows customization of depth, domain boundaries, and page limits similar to BFS strategy.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_33

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling import DFSDeepCrawlStrategy

# Basic configuration
strategy = DFSDeepCrawlStrategy(
    max_depth=2,               # Crawl initial page + 2 levels deep
    include_external=False,    # Stay within the same domain
    max_pages=30,              # Maximum number of pages to crawl (optional)
    score_threshold=0.5,       # Minimum score for URLs to be crawled (optional)
)
```

----------------------------------------

TITLE: Multiple Filter Implementation in Web Crawler
DESCRIPTION: Advanced filtering implementation combining multiple filter types including URL patterns, domains, and content types.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_38

LANGUAGE: python
CODE:
```
from crawl4ai.deep_crawling.filters import (
    FilterChain,
    URLPatternFilter,
    DomainFilter,
    ContentTypeFilter
)

# Create a chain of filters
filter_chain = FilterChain([
    # Only follow URLs with specific patterns
    URLPatternFilter(patterns=["*guide*", "*tutorial*"]),
    
    # Only crawl specific domains
    DomainFilter(
        allowed_domains=["docs.example.com"],
        blocked_domains=["old.docs.example.com"]
    ),
    
    # Only include specific content types
    ContentTypeFilter(allowed_types=["text/html"])
])

config = CrawlerRunConfig(
    deep_crawl_strategy=BFSDeepCrawlStrategy(
        max_depth=2,
        filter_chain=filter_chain
    )
)
```

----------------------------------------

TITLE: Typical Initialization of AsyncWebCrawler in Python
DESCRIPTION: Example of initializing an AsyncWebCrawler instance with a BrowserConfig object specifying browser type, headless mode, and verbosity.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/async-webcrawler.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_cfg = BrowserConfig(
    browser_type="chromium",
    headless=True,
    verbose=True
)

crawler = AsyncWebCrawler(config=browser_cfg)
```

----------------------------------------

TITLE: Excluding External Images with CrawlerRunConfig (Python)
DESCRIPTION: This snippet demonstrates how to configure `CrawlerRunConfig` to exclude images originating from external domains. Setting `exclude_external_images=True` instructs the crawler to attempt discarding images whose source URLs do not belong to the primary domain being crawled, useful for ignoring third-party content like advertisements.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_0

LANGUAGE: python
CODE:
```
crawler_cfg = CrawlerRunConfig(
    exclude_external_images=True
)
```

----------------------------------------

TITLE: CrawlResult and MarkdownGenerationResult Model Definition in Python
DESCRIPTION: The core schema definition for the CrawlResult and MarkdownGenerationResult classes in Crawl4AI. This model captures all aspects of a crawl operation including HTML, screenshot, PDF, markdown output, links, media, and more.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/crawler-result.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
class MarkdownGenerationResult(BaseModel):
    raw_markdown: str
    markdown_with_citations: str
    references_markdown: str
    fit_markdown: Optional[str] = None
    fit_html: Optional[str] = None

class CrawlResult(BaseModel):
    url: str
    html: str
    success: bool
    cleaned_html: Optional[str] = None
    media: Dict[str, List[Dict]] = {}
    links: Dict[str, List[Dict]] = {}
    downloaded_files: Optional[List[str]] = None
    screenshot: Optional[str] = None
    pdf : Optional[bytes] = None
    mhtml: Optional[str] = None
    markdown: Optional[Union[str, MarkdownGenerationResult]] = None
    extracted_content: Optional[str] = None
    metadata: Optional[dict] = None
    error_message: Optional[str] = None
    session_id: Optional[str] = None
    response_headers: Optional[dict] = None
    status_code: Optional[int] = None
    ssl_certificate: Optional[SSLCertificate] = None
    class Config:
        arbitrary_types_allowed = True
```

----------------------------------------

TITLE: Selecting HTML Source for Markdown Generation
DESCRIPTION: This example shows how to control which HTML content is used as input for markdown generation using the content_source parameter. It demonstrates three options: raw HTML, cleaned HTML, and preprocessed HTML optimized for schema extraction.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_105

LANGUAGE: python
CODE:
```
from crawl4ai.markdown_generation_strategy import DefaultMarkdownGenerator
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    # Option 1: Use the raw HTML directly from the webpage (before any processing)
    raw_md_generator = DefaultMarkdownGenerator(
        content_source="raw_html",
        options={"ignore_links": True}
    )
    
    # Option 2: Use the cleaned HTML (after scraping strategy processing - default)
    cleaned_md_generator = DefaultMarkdownGenerator(
        content_source="cleaned_html",  # This is the default
        options={"ignore_links": True}
    )
    
    # Option 3: Use preprocessed HTML optimized for schema extraction
    fit_md_generator = DefaultMarkdownGenerator(
        content_source="fit_html",
        options={"ignore_links": True}
    )
    
    # Use one of the generators in your crawler config
    config = CrawlerRunConfig(
        markdown_generator=raw_md_generator  # Try each of the generators
    )
    
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun("https://example.com", config=config)
        if result.success:
            print("Markdown:\n", result.markdown.raw_markdown[:500])
        else:
            print("Crawl failed:", result.error_message)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
```

----------------------------------------

TITLE: Setting Up Deep Crawler Strategy in Crawl4AI
DESCRIPTION: Illustrates the configuration of a deep crawler strategy in Crawl4AI, including depth settings, filter chain, and URL scoring mechanisms.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_61

LANGUAGE: json
CODE:
```
{
  "crawler_config": {
    "type": "CrawlerRunConfig",
    "params": {
      "deep_crawl_strategy": {
        "type": "BFSDeepCrawlStrategy",
        "params": {
          "max_depth": 3,
          "filter_chain": {
            "type": "FilterChain",
            "params": {
              "filters": [
                {
                  "type": "ContentTypeFilter",
                  "params": {
                    "allowed_types": ["text/html", "application/xhtml+xml"]
                  }
                },
                {
                  "type": "DomainFilter",
                  "params": {
                    "allowed_domains": ["blog.*", "docs.*"],
                  }
                }
              ]
            }
          },
          "url_scorer": {
            "type": "CompositeScorer",
            "params": {
              "scorers": [
                {
                  "type": "KeywordRelevanceScorer",
                  "params": {
                    "keywords": ["tutorial", "guide", "documentation"],
                  }
                },
                {
                  "type": "PathDepthScorer",
                  "params": {
                    "weight": 0.5,
                    "optimal_depth": 3  
                  }
                }
              ]
            }
          }
        }
      }
    }
  }
}
```

----------------------------------------

TITLE: Configuring Network and Console Capture in Crawl4AI with Python
DESCRIPTION: This Python code snippet illustrates how to configure `CrawlerRunConfig` in the `crawl4ai` library to enable debugging features. Setting `capture_network=True` logs network traffic, `capture_console=True` captures browser console output, and `mhtml=True` saves MHTML snapshots of the crawled pages.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/README.md#_snippet_22

LANGUAGE: python
CODE:
```
crawler_config = CrawlerRunConfig(
    capture_network=True,
    capture_console=True,
    mhtml=True
)
```

----------------------------------------

TITLE: Using JsonCssExtractionStrategy with crawl4ai (Python)
DESCRIPTION: Shows how to use JsonCssExtractionStrategy to extract data based on CSS selectors defined in a JSON-like schema. The schema specifies a base selector for items and then defines fields to extract using relative selectors, types (text, attribute), and optional transformations.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/strategies.md#_snippet_12

LANGUAGE: python
CODE:
```
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

# Define schema
schema = {
    "name": "Product List",
    "baseSelector": ".product-card",
    "fields": [
        {
            "name": "title",
            "selector": "h2.title",
            "type": "text"
        },
        {
            "name": "price",
            "selector": ".price",
            "type": "text",
            "transform": "strip"
        },
        {
            "name": "image",
            "selector": "img",
            "type": "attribute",
            "attribute": "src"
        }
    ]
}

# Create and use strategy
strategy = JsonCssExtractionStrategy(schema)
result = await crawler.arun(
    url="https://example.com/products",
    extraction_strategy=strategy
)
```

----------------------------------------

TITLE: Configuring Basic Proxy Setup in Crawl4AI
DESCRIPTION: Demonstrates how to configure both HTTP and SOCKS proxies using BrowserConfig. Shows basic proxy URL configuration for AsyncWebCrawler initialization.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/proxy-security.md#2025-04-23_snippet_0

LANGUAGE: python
CODE:
```
from crawl4ai.async_configs import BrowserConfig

# Using proxy URL
browser_config = BrowserConfig(proxy="http://proxy.example.com:8080")
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")

# Using SOCKS proxy
browser_config = BrowserConfig(proxy="socks5://proxy.example.com:1080")
async with AsyncWebCrawler(config=browser_config) as crawler:
    result = await crawler.arun(url="https://example.com")
```

----------------------------------------

TITLE: Using Hooks for Custom Workflow in Crawl4AI
DESCRIPTION: This snippet shows how to use hooks in Crawl4AI for custom workflows. It demonstrates setting a 'before_goto' hook and provides examples of other available hooks for different stages of the crawling process.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/quickstart.ipynb#2025-04-23_snippet_7

LANGUAGE: python
CODE:
```
async def custom_hook_workflow():
    async with AsyncWebCrawler() as crawler:
        # Set a 'before_goto' hook to run custom code just before navigation
        crawler.crawler_strategy.set_hook("before_goto", lambda page: print("[Hook] Preparing to navigate..."))
        
        # Perform the crawl operation
        result = await crawler.arun(
            url="https://crawl4ai.com",
            bypass_cache=True
        )
        print(result.markdown.raw_markdown[:500])  # Display the first 500 characters

asyncio.run(custom_hook_workflow())
```

----------------------------------------

TITLE: Performing Simple Crawl with Crawl4AI REST API
DESCRIPTION: Demonstrates how to perform a simple crawl operation using the Crawl4AI REST API with Python requests library.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_62

LANGUAGE: python
CODE:
```
import requests

crawl_payload = {
    "urls": ["https://example.com"],
    "browser_config": {"headless": True},
    "crawler_config": {"stream": False}
}
response = requests.post(
    "http://localhost:8000/crawl",
    # headers={"Authorization": f"Bearer {token}"},  # If JWT is enabled, more on this later
    json=crawl_payload
)
print(response.json())  # Print the response for debugging
```

----------------------------------------

TITLE: CSS/XPath-based Extraction Configuration in YAML for Crawl4AI
DESCRIPTION: Defines extraction configuration for using CSS/XPath selectors to extract structured data from web pages.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_6

LANGUAGE: yaml
CODE:
```
# extract_css.yml
type: "json-css"
params:
  verbose: true
```

----------------------------------------

TITLE: Troubleshooting Builtin Browser in Crawl4AI via CLI
DESCRIPTION: This snippet provides CLI commands for troubleshooting issues with the builtin browser, including checking status, restarting, and stopping the browser when problems persist.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/examples/README_BUILTIN_BROWSER.md#2025-04-23_snippet_4

LANGUAGE: bash
CODE:
```
# Check the browser status:
crwl browser status

# Try restarting it:
crwl browser restart

# If problems persist, stop it and let Crawl4AI start a fresh one:
crwl browser stop
```

----------------------------------------

TITLE: Media Structure Example in Crawl4AI Results
DESCRIPTION: Sample data structure showing how different media types (images, videos, audio, tables) are organized in the CrawlResult object with their respective metadata.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_94

LANGUAGE: python
CODE:
```
result.media = {
  "images": [
    {
      "src": "https://cdn.prod.website-files.com/.../Group%2089.svg",
      "alt": "coding school for kids",
      "desc": "Trial Class Degrees degrees All Degrees AI Degree Technology ...",
      "score": 3,
      "type": "image",
      "group_id": 0,
      "format": None,
      "width": None,
      "height": None
    },
    # ...
  ],
  "videos": [
    # Similar structure but with video-specific fields
  ],
  "audio": [
    # Similar structure but with audio-specific fields
  ],
  "tables": [
    {
      "headers": ["Name", "Age", "Location"],
      "rows": [
        ["John Doe", "34", "New York"],
        ["Jane Smith", "28", "San Francisco"],
        ["Alex Johnson", "42", "Chicago"]
      ],
      "caption": "Employee Directory",
      "summary": "Directory of company employees"
    },
    # More tables if present
  ]
}
```

----------------------------------------

TITLE: Using arun_many() with Custom Dispatcher for Controlled Concurrency in Python
DESCRIPTION: This example demonstrates how to use arun_many() with a custom dispatcher (MemoryAdaptiveDispatcher) to control concurrency based on system memory usage and set maximum concurrent sessions.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/arun_many.md#2025-04-23_snippet_3

LANGUAGE: python
CODE:
```
dispatcher = MemoryAdaptiveDispatcher(
    memory_threshold_percent=70.0,
    max_session_permit=10
)
results = await crawler.arun_many(
    urls=["https://site1.com", "https://site2.com", "https://site3.com"],
    config=my_run_config,
    dispatcher=dispatcher
)
```

----------------------------------------

TITLE: Processing Crawler4ai Results
DESCRIPTION: Example demonstrating how to process and access the results obtained from a crawling operation, including how to iterate through pages and access their content.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/tutorials/coming_soon.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
# After crawling
for page in results.pages:
    print(f"URL: {page.url}")
    print(f"Title: {page.title}")
    print(f"Content Length: {len(page.text)}")
    
    # Access extracted links
    for link in page.links:
        print(f"  - {link}")
    
    # Access metadata
    for key, value in page.metadata.items():
        print(f"  {key}: {value}")
```

----------------------------------------

TITLE: CSS-Based Wait Conditions in Crawl4AI
DESCRIPTION: Shows how to wait for specific CSS elements to appear before proceeding with web scraping using Crawl4AI. This example waits for at least 30 items to load on Hacker News before continuing.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/page-interaction.md#2025-04-23_snippet_1

LANGUAGE: python
CODE:
```
import asyncio
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async def main():
    config = CrawlerRunConfig(
        # Wait for at least 30 items on Hacker News
        wait_for="css:.athing:nth-child(30)"  
    )
    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(
            url="https://news.ycombinator.com",
            config=config
        )
        print("We have at least 30 items loaded!")
        # Rough check
        print("Total items in HTML:", result.cleaned_html.count("athing"))  

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Installing Crawl4AI Basic Package in Python
DESCRIPTION: Commands for installing the basic Crawl4AI package using pip and setting up Playwright dependencies.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/basic/installation.md#2025-04-23_snippet_0

LANGUAGE: bash
CODE:
```
pip install crawl4ai
playwright install # Install Playwright dependencies
```

----------------------------------------

TITLE: Configuring Robots.txt Compliance in Crawl4AI
DESCRIPTION: Example of configuring crawler to respect robots.txt rules. It shows how to enable robots.txt checking and set a custom user agent for ethical web crawling practices.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/parameters.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
run_config = CrawlerRunConfig(
    check_robots_txt=True,  # Enable robots.txt compliance
    user_agent="MyBot/1.0"  # Identify your crawler
)
```

----------------------------------------

TITLE: Configuring JSON CSS Extraction Strategy via API
DESCRIPTION: Provides a JSON example demonstrating how to configure an extraction strategy directly via the API. It shows nesting a `JsonCssExtractionStrategy` within the `crawler_config`. Note the required `{"type": "ClassName", "params": {...}}` structure for complex objects and the `{"type": "dict", "value": {...}}` wrapper for the dictionary-based schema.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_29

LANGUAGE: json
CODE:
```
{
    "crawler_config": {
        "type": "CrawlerRunConfig",
        "params": {
            "extraction_strategy": {
                "type": "JsonCssExtractionStrategy",
                "params": {
                    "schema": {
                        "type": "dict",
                        "value": {
                           "baseSelector": "article.post",
                           "fields": [
                               {"name": "title", "selector": "h1", "type": "text"},
                               {"name": "content", "selector": ".content", "type": "html"}
                           ]
                         }
                    }
                }
            }
        }
    }
}
```

----------------------------------------

TITLE: Implementing AsyncHTTPCrawlerStrategy in Python
DESCRIPTION: This example shows how to use the new AsyncHTTPCrawlerStrategy, a lightweight and fast HTTP-only crawler. It demonstrates configuring the HTTP crawler and using it with AsyncWebCrawler for simple scraping tasks.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/blog/releases/0.5.0.md#2025-04-23_snippet_2

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, HTTPCrawlerConfig
from crawl4ai.async_crawler_strategy import AsyncHTTPCrawlerStrategy
import asyncio

# Use the HTTP crawler strategy
http_crawler_config = HTTPCrawlerConfig(
        method="GET",
        headers={"User-Agent": "MyCustomBot/1.0"},
        follow_redirects=True,
        verify_ssl=True
)

async def main():
    async with AsyncWebCrawler(crawler_strategy=AsyncHTTPCrawlerStrategy(browser_config =http_crawler_config)) as crawler:
        result = await crawler.arun("https://example.com")
        print(f"Status code: {result.status_code}")
        print(f"Content length: {len(result.html)}")

asyncio.run(main())
```

----------------------------------------

TITLE: Accessing Link Data (Python)
DESCRIPTION: Illustrates how to access and print information about internal links stored in the `links` field of a `CrawlResult`. The `links` field is a dictionary typically containing "internal" and "external" keys, each holding a list of link details.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_13

LANGUAGE: python
CODE:
```
for link in result.links["internal"]:
    print(f"Internal link to {link['href']} with text {link['text']}")
```

----------------------------------------

TITLE: Configuring SemaphoreDispatcher in Python for Concurrent Crawling
DESCRIPTION: This code example demonstrates how to set up a SemaphoreDispatcher for simple concurrency control with a fixed limit, including optional rate limiting and monitoring features.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_145

LANGUAGE: python
CODE:
```
from crawl4ai.async_dispatcher import SemaphoreDispatcher

dispatcher = SemaphoreDispatcher(
    max_session_permit=20,         # Maximum concurrent tasks
    rate_limiter=RateLimiter(      # Optional rate limiting
        base_delay=(0.5, 1.0),
        max_delay=10.0
    ),
    monitor=CrawlerMonitor(        # Optional monitoring
        max_visible_rows=15,
        display_mode=DisplayMode.DETAILED
    )
)
```

----------------------------------------

TITLE: Minimal AsyncWebCrawler Usage Example with BrowserConfig in Python
DESCRIPTION: Simple example showing how to initialize and use AsyncWebCrawler with a custom BrowserConfig. Demonstrates creating a Firefox browser in visible mode with text-only processing and basic page crawling.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, BrowserConfig

browser_conf = BrowserConfig(
    browser_type="firefox",
    headless=False,
    text_mode=True
)

async with AsyncWebCrawler(config=browser_conf) as crawler:
    result = await crawler.arun("https://example.com")
    print(result.markdown[:300])
```

----------------------------------------

TITLE: Advanced Crawl4AI Usage Example with JSON-CSS Extraction
DESCRIPTION: Shows how to crawl a webpage and extract content according to a JSON-CSS schema, outputting the results in JSON format.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/cli.md#2025-04-23_snippet_1

LANGUAGE: bash
CODE:
```
crwl "https://www.infoq.com/ai-ml-data-eng/" -e docs/examples/cli/extract_css.yml -s docs/examples/cli/css_schema.json -o json;
```

----------------------------------------

TITLE: Accessing Media Content from Crawl4AI Results
DESCRIPTION: This snippet demonstrates how to iterate through images found during crawling and print their URLs and alt text. The media dictionary contains categorized media elements including images, audio, and video.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_28

LANGUAGE: python
CODE:
```
images = result.media.get("images", [])
for img in images:
    print("Image URL:", img["src"], "Alt:", img.get("alt"))
```

----------------------------------------

TITLE: Comprehensive Crawl4AI Example with CSS Selection and Pattern-Based Extraction
DESCRIPTION: This comprehensive example demonstrates how to combine CSS selection, exclusion logic, and pattern-based extraction in Crawl4AI. It defines a function to extract main articles from a given URL with detailed configuration.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-doc-context.md#2025-04-23_snippet_19

LANGUAGE: python
CODE:
```
import asyncio
import json
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig, CacheMode
from crawl4ai.extraction_strategy import JsonCssExtractionStrategy

async def extract_main_articles(url: str):
    schema = {
        "name": "ArticleBlock",
        "baseSelector": "div.article-block",
        "fields": [
            {"name": "headline", "selector": "h2", "type": "text"},
            {"name": "summary", "selector": ".summary", "type": "text"},
            {
                "name": "metadata",
                "type": "nested",
                "fields": [
                    {"name": "author", "selector": ".author", "type": "text"},
                    {"name": "date", "selector": ".date", "type": "text"}
                ]
            }
        ]
    }

    config = CrawlerRunConfig(
        # Keep only #main-content
        css_selector="#main-content",
        
        # Filtering
        word_count_threshold=10,
        excluded_tags=["nav", "footer"],  
        exclude_external_links=True,
        exclude_domains=["somebadsite.com"],
        exclude_external_images=True,

        # Extraction
        extraction_strategy=JsonCssExtractionStrategy(schema),
        
        cache_mode=CacheMode.BYPASS
    )

    async with AsyncWebCrawler() as crawler:
        result = await crawler.arun(url=url, config=config)
        if not result.success:
            print(f"Error: {result.error_message}")
            return None
        return json.loads(result.extracted_content)

async def main():
    articles = await extract_main_articles("https://news.ycombinator.com/newest")
    if articles:
        print("Extracted Articles:", articles[:2])  # Show first 2

if __name__ == "__main__":
    asyncio.run(main())
```

----------------------------------------

TITLE: Implementing Advanced Filters in Python with AsyncWebCrawler
DESCRIPTION: This function demonstrates advanced filtering techniques for specialized crawling. It includes examples of SEO filters, text relevancy filtering, and combining advanced filters for more precise crawling results.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/c4ai-code-context.md#2025-04-23_snippet_209

LANGUAGE: python
CODE:
```
async def advanced_filters():
    """
    PART 4: Demonstrates advanced filtering techniques for specialized crawling.

    This function covers:
    - SEO filters
    - Text relevancy filtering
    - Combining advanced filters
    """
    print("\n===== ADVANCED FILTERS =====")

    async with AsyncWebCrawler() as crawler:
        # SEO FILTER EXAMPLE
        print("\n EXAMPLE 1: SEO FILTERS")
        print(
            "Quantitative SEO quality assessment filter based searching keywords in the head section"
        )

        seo_filter = SEOFilter(
            threshold=0.5, keywords=["dynamic", "interaction", "javascript"]
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1, filter_chain=FilterChain([seo_filter])
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )

        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        print(f"   Found {len(results)} pages with relevant keywords")
        for result in results:
            print(f"   {result.url}")

        # ADVANCED TEXT RELEVANCY FILTER
        print("\n EXAMPLE 2: ADVANCED TEXT RELEVANCY FILTER")

        # More sophisticated content relevance filter
        relevance_filter = ContentRelevanceFilter(
            query="Interact with the web using your authentic digital identity",
            threshold=0.7,
        )

        config = CrawlerRunConfig(
            deep_crawl_strategy=BFSDeepCrawlStrategy(
                max_depth=1, filter_chain=FilterChain([relevance_filter])
            ),
            scraping_strategy=LXMLWebScrapingStrategy(),
            verbose=True,
            cache_mode=CacheMode.BYPASS,
        )

        results = await crawler.arun(url="https://docs.crawl4ai.com", config=config)

        print(f"   Found {len(results)} pages")
        for result in results:
            relevance_score = result.metadata.get("relevance_score", 0)
            print(f"   Score: {relevance_score:.2f} | {result.url}")
```

----------------------------------------

TITLE: Creating SEO Metadata Filter with Crawl4AI (Python)
DESCRIPTION: Creates an SEOFilter instance configured with a minimum score threshold and a list of target keywords to evaluate web page metadata. Integrates this filter into a BFSDeepCrawlStrategy within a CrawlerRunConfig to limit crawling to pages meeting SEO criteria. Requires the Crawl4AI Python library with relevant filter modules.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/deep-crawling.md#_snippet_10

LANGUAGE: python
CODE:
```
# Create an SEO filter that looks for specific keywords in page metadata\nseo_filter = SEOFilter(\n    threshold=0.5,  # Minimum score (0.0 to 1.0)\n    keywords=[\"tutorial\", \"guide\", \"documentation\"]\n)\n\nconfig = CrawlerRunConfig(\n    deep_crawl_strategy=BFSDeepCrawlStrategy(\n        max_depth=1,\n        filter_chain=FilterChain([seo_filter])\n    )\n)\n
```

----------------------------------------

TITLE: Example Structure of result.media (Python)
DESCRIPTION: This snippet illustrates the potential structure of the `result.media` attribute in a `CrawlResult` object. It's a dictionary keyed by media type (e.g., `images`, `videos`, `audio`, `tables`). The example shows the detailed structure for image entries (with fields like `src`, `alt`, `desc`, `score`) and table entries (with `headers`, `rows`, `caption`).
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/core/link-media.md#_snippet_7

LANGUAGE: python
CODE:
```
result.media = {
  "images": [
    {
      "src": "https://cdn.prod.website-files.com/.../Group%2089.svg",
      "alt": "coding school for kids",
      "desc": "Trial Class Degrees degrees All Degrees AI Degree Technology ...",
      "score": 3,
      "type": "image",
      "group_id": 0,
      "format": None,
      "width": None,
      "height": None
    },
    # ...
  ],
  "videos": [
    # Similar structure but with video-specific fields
  ],
  "audio": [
    # Similar structure but with audio-specific fields
  ],
  "tables": [
    {
      "headers": ["Name", "Age", "Location"],
      "rows": [
        ["John Doe", "34", "New York"],
        ["Jane Smith", "28", "San Francisco"],
        ["Alex Johnson", "42", "Chicago"]
      ],
      "caption": "Employee Directory",
      "summary": "Directory of company employees"
    },
    # More tables if present
  ]
}
```

----------------------------------------

TITLE: Using Magic Mode in Crawl4AI (Python)
DESCRIPTION: Python code example demonstrating how to use Magic Mode in Crawl4AI for simplified automation without persistent profiles.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/advanced/identity-based-crawling.md#2025-04-23_snippet_4

LANGUAGE: python
CODE:
```
from crawl4ai import AsyncWebCrawler, CrawlerRunConfig

async with AsyncWebCrawler() as crawler:
    result = await crawler.arun(
        url="https://example.com",
        config=CrawlerRunConfig(
            magic=True,  # Simplifies a lot of interaction
            remove_overlay_elements=True,
            page_timeout=60000
        )
    )
```

----------------------------------------

TITLE: Requesting a Screenshot via API
DESCRIPTION: Shows the JSON payload structure for the `/screenshot` endpoint. It includes the mandatory `url`, an optional `screenshot_wait_for` delay (in seconds, default 2), and an optional `output_path` to save the PNG file.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/deploy/docker/README.md#_snippet_22

LANGUAGE: json
CODE:
```
{
  "url": "https://example.com",
  "screenshot_wait_for": 2,
  "output_path": "/path/to/save/screenshot.png"
}
```

----------------------------------------

TITLE: Analyzing Network Requests in Python
DESCRIPTION: This snippet shows how to iterate through the `network_requests` list within a `CrawlResult` object. It demonstrates filtering events by `event_type` (request, response, failed) and analyzing specific properties like URL or failure text. Requires a `CrawlResult` object with network capture enabled.
SOURCE: https://github.com/unclecode/crawl4ai/blob/main/docs/md_v2/api/crawl-result.md#_snippet_21

LANGUAGE: python
CODE:
```
if result.network_requests:
    # Count different types of events
    requests = [r for r in result.network_requests if r.get("event_type") == "request"]
    responses = [r for r in result.network_requests if r.get("event_type") == "response"]
    failures = [r for r in result.network_requests if r.get("event_type") == "request_failed"]
    
    print(f"Captured {len(requests)} requests, {len(responses)} responses, and {len(failures)} failures")
    
    # Analyze API calls
    api_calls = [r for r in requests if "api" in r.get("url", "")]
    
    # Identify failed resources
    for failure in failures:
        print(f"Failed to load: {failure.get('url')} - {failure.get('failure_text')}")
```